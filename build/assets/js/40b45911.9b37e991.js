"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[75049],{603905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>f});var r=a(667294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function n(e,t){if(null==e)return{};var a,r,s=function(e,t){if(null==e)return{};var a,r,s={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=r.createContext({}),c=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},h="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var a=e.components,s=e.mdxType,i=e.originalType,l=e.parentName,u=n(e,["components","mdxType","originalType","parentName"]),h=c(a),p=s,f=h["".concat(l,".").concat(p)]||h[p]||d[p]||i;return a?r.createElement(f,o(o({ref:t},u),{},{components:a})):r.createElement(f,o({ref:t},u))}));function f(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var i=a.length,o=new Array(i);o[0]=p;var n={};for(var l in t)hasOwnProperty.call(t,l)&&(n[l]=t[l]);n.originalType=e,n[h]="string"==typeof e?e:s,o[1]=n;for(var c=2;c<i;c++)o[c]=a[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}p.displayName="MDXCreateElement"},732834:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>n,toc:()=>c});var r=a(487462),s=(a(667294),a(603905));const i={},o="Airflow Architecture",n={unversionedId:"technologies/apache/airflow/airflow-architecture",id:"technologies/apache/airflow/airflow-architecture",title:"Airflow Architecture",description:"At its core, Airflow is simply a queuing system built on top of a metadata database. The database stores the state of queued tasks and a scheduler uses these states to prioritize how other tasks are added to the queue. This functionality is orchestrated by four primary components",source:"@site/docs/technologies/apache/airflow/airflow-architecture.md",sourceDirName:"technologies/apache/airflow",slug:"/technologies/apache/airflow/airflow-architecture",permalink:"/technologies/apache/airflow/airflow-architecture",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/technologies/apache/airflow/airflow-architecture.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Airflow",permalink:"/technologies/apache/airflow/"},next:{title:"Commands / Configs",permalink:"/technologies/apache/airflow/commands-configs"}},l={},c=[{value:"Basic Airflow Architecture",id:"basic-airflow-architecture",level:2},{value:"Schedular Operation",id:"schedular-operation",level:2},{value:"Single-node architecture",id:"single-node-architecture",level:2},{value:"Multi-node architecture",id:"multi-node-architecture",level:2},{value:"Multi-node architecture provides several benefits",id:"multi-node-architecture-provides-several-benefits",level:2},{value:"Others",id:"others",level:2},{value:"Clockwork: Distributed, Scalable Job Scheduler",id:"clockwork-distributed-scalable-job-scheduler",level:3}],u={toc:c},h="wrapper";function d(e){let{components:t,...i}=e;return(0,s.kt)(h,(0,r.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"airflow-architecture"},"Airflow Architecture"),(0,s.kt)("p",null,"At its core, Airflow is simply a queuing system built on top of a metadata database. The database stores the state of queued tasks and a scheduler uses these states to prioritize how other tasks are added to the queue. This functionality is orchestrated by four primary components"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Metadata Database:")," this database stores information regarding the state of tasks. Database updates are performed using an abstraction layer implemented in SQLAlchemy. This abstraction layer cleanly separates the function of the remaining components of Airflow from the database.")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Scheduler:")," The Scheduler is a process that uses DAG definitions in conjunction with the state of tasks in the metadata database to decide which tasks need to be executed, as well as their execution priority. The Scheduler is generally run as a service.")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Executor:")," The Executor is a message queuing process that is tightly bound to the Scheduler and determines the worker processes that actually execute each scheduled task. There are different types of Executors, each of which uses a specific class of worker processes to execute tasks. For example, theLocalExecutorexecutes tasks with parallel processes that run on the same machine as the Scheduler process. Other Executors, like the CeleryExecutor execute tasks using worker processes that exist on a separate cluster of worker machines."))),(0,s.kt)("p",null,"Inside ",(0,s.kt)("a",{parentName:"p",href:"https://airflow.apache.org/"},"Apache Airflow"),", tasks are carried out by anexecutor. The main types of executors are:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Sequential Executor:")," Each task is runlocally(on the same machine as the scheduler) in its own python subprocess. They are run sequentially which means that only one task can be executed at a time. It is the default executor."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Local Executor:")," It is the same as the sequential executor except that multiple tasks canrun in parallel. It needs a metadata database (where DAGs and tasks status are stored) that supports parallelism like MySQL. Setting such a database requires some extra work since the default configuration uses SQLite."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Celery Executor:")," The workload is distributed on multiple celery workers which can run on different machines.It is the executor you should use for availability and scalability.")),(0,s.kt)("ol",{start:4},(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("strong",{parentName:"li"},"Workers:")," These are the processes that actually execute the logic of tasks, and are determined by the Executor being used.")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:a(189174).Z,width:"1378",height:"1053"})),(0,s.kt)("p",null,"Airflow's General Architecture.Airflow's operation is built atop a Metadata Database which stores the state of tasks and workflows (i.e. DAGs). The Scheduler and Executor send tasks to a queue for Worker processes to perform. The Webserver runs (often-times running on the same machine as the Scheduler) and communicates with the database to render task state and Task Execution Logs in the Web UI. Each colored box indicates that each component can exist in isolation from the other components, depending on the type of deployment configuration."),(0,s.kt)("h2",{id:"basic-airflow-architecture"},"Basic Airflow Architecture"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:a(914626).Z,width:"1114",height:"580"})),(0,s.kt)("p",null,"There are a few components to note:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Metadata Database:")," Airflow uses a SQL database to store metadata about the data pipelines being run. In the diagram above, this is represented as Postgres which is extremely popular with Airflow. Alternate databases supported with Airflow include MySQL."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Web ServerandScheduler:")," The Airflow web server and Scheduler are separate processes run (in this case) on the local machine and interact with the database mentioned above."),(0,s.kt)("li",{parentName:"ul"},"TheExecutoris shown separately above, since it is commonly discussed within Airflow and in the documentation, but in reality it is NOT a separate process, but run within the Scheduler."),(0,s.kt)("li",{parentName:"ul"},"TheWorker(s)are separate processes which also interact with the other components of the Airflow architecture and the metadata repository."),(0,s.kt)("li",{parentName:"ul"},"airflow.cfgis the Airflow configuration file which is accessed by the Web Server, Scheduler, and Workers."),(0,s.kt)("li",{parentName:"ul"},"DAGsrefers to the DAG files containing Python code, representing the data pipelines to be run by Airflow. The location of these files is specified in the Airflow configuration file, but they need to be accessible by the Web Server, Scheduler, and Workers.")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/concepts.html"},"https://airflow.apache.org/docs/apache-airflow/stable/concepts.html")),(0,s.kt)("h2",{id:"schedular-operation"},"Schedular Operation"),(0,s.kt)("p",null,"Step 0. Load available DAG definitions from disk (fill DagBag)"),(0,s.kt)("p",null,"While the scheduler is running:"),(0,s.kt)("p",null,"Step 1. The scheduler uses the DAG definitions to identify and/or initialize any DagRuns in the metadata db."),(0,s.kt)("p",null,'Step 2. The scheduler checks the states of the TaskInstances associated with active DagRuns, resolves any dependencies amongst TaskInstances, identifies TaskInstances that need to be executed, and adds them to a worker queue, updating the status of newly-queued TaskInstances to "queued" in the database.'),(0,s.kt)("p",null,'Step 3. Each available worker pulls a TaskInstance from the queue and starts executing it, updating the database record for the TaskInstance from "queued" to "running".'),(0,s.kt)("p",null,'Step 4. Once a TaskInstance is finished running, the associated worker reports back to the queue and updates the status for the TaskInstance in the database (e.g. "finished", "failed", etc.)'),(0,s.kt)("p",null,'Step 5. The scheduler updates the states of all active DagRuns ("running", "failed", "finished") according to the states of all completed associated TaskInstances.'),(0,s.kt)("p",null,"Step 6. Repeat Steps 1-5"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://medium.com/@dustinstansbury/how-quizlet-uses-apache-airflow-in-practice-a903cbb5626d"},"https://medium.com/@dustinstansbury/how-quizlet-uses-apache-airflow-in-practice-a903cbb5626d")),(0,s.kt)("h2",{id:"single-node-architecture"},"Single-node architecture"),(0,s.kt)("p",null,"In a single-node architecture all components are on the same node. To use a single node architecture, Airflow has to be configured with the LocalExecutor mode."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:a(918307).Z,width:"784",height:"658"})),(0,s.kt)("p",null,"The single-node architecture is widely used by the users in case they have a moderate amount of DAGs. In this mode, the worker pulls tasks to run from an IPC (Inter Process Communication) queue. This mode doesn't any need external dependencies. It scales up well until all resources on the server are used. This solution works pretty well. However, to scale out to multiple servers, the Celery executor mode has to be used. Celery executor uses Celery (and a message-queuing server) to distribute the load on a pool of workers."),(0,s.kt)("h2",{id:"multi-node-architecture"},"Multi-node architecture"),(0,s.kt)("p",null,"In a multi node architecture daemons are spread in different machines. We decided to colocate the webserver and the scheduler. To use this architecture, Airflow has to be configure with the Celery Executor mode."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:a(285735).Z,width:"1102",height:"678"})),(0,s.kt)("p",null,"In this mode, a Celery backend has to be set (Redis in our case). Celery is an asynchronous queue based on distributed message passing. Airflow uses it to execute several tasks concurrently on several workers server using multiprocessing. This mode allows to scale up the Airflow cluster really easily by adding new workers."),(0,s.kt)("h2",{id:"multi-node-architecture-provides-several-benefits"},"Multi-node architecture provides several benefits"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Higher availability:")," if one of the worker nodes goes down, the cluster will still be up and DAGs will still be running."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Dedicated workers for specific tasks :")," we have a workflow where some of our DAGs are CPU intensive. As we have several workers we can dedicate some of them to these kinds of DAGs."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scaling horizontally:")," Indeed since workers don't need to register with any central authority to start processing tasks, we can scale our cluster by easily adding new workers. Nodes can be turned on and off without any downtime on the cluster.")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://drivy.engineering/airflow-architecture"},"https://drivy.engineering/airflow-architecture")),(0,s.kt)("h2",{id:"others"},"Others"),(0,s.kt)("h3",{id:"clockwork-distributed-scalable-job-scheduler"},"Clockwork: Distributed, Scalable Job Scheduler"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://cynic.dev/posts/clockwork-scalable-job-scheduler"},"https://cynic.dev/posts/clockwork-scalable-job-scheduler")))}d.isMDXComponent=!0},189174:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/Technologies-Apache-Airflow-Architecture-image1-8049422a3b1e8ef765ad4cd346c4f640.jpg"},914626:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/Technologies-Apache-Airflow-Architecture-image2-dfd6d941a378fcefd65e3acd59c5ab0d.jpg"},918307:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/Technologies-Apache-Airflow-Architecture-image3-e338519039c5e02b5a0844df8721903c.jpg"},285735:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/Technologies-Apache-Airflow-Architecture-image4-8dec60510cc8a9a062ac0a2723a7233b.jpg"}}]);