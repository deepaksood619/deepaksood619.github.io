"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[21253],{603905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>g});var n=a(667294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=n.createContext({}),s=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=s(e.components);return n.createElement(c.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=s(a),d=r,g=m["".concat(c,".").concat(d)]||m[d]||u[d]||i;return a?n.createElement(g,o(o({ref:t},p),{},{components:a})):n.createElement(g,o({ref:t},p))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l[m]="string"==typeof e?e:r,o[1]=l;for(var s=2;s<i;s++)o[s]=a[s];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},543412:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var n=a(487462),r=(a(667294),a(603905));const i={},o="Q-Learning Algorithms",l={unversionedId:"ai/move-37/q-learning-algorithms",id:"ai/move-37/q-learning-algorithms",title:"Q-Learning Algorithms",description:"Q-Learning algorithms are a family of Reinforcement Learning algorithms.",source:"@site/docs/ai/move-37/q-learning-algorithms.md",sourceDirName:"ai/move-37",slug:"/ai/move-37/q-learning-algorithms",permalink:"/ai/move-37/q-learning-algorithms",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/ai/move-37/q-learning-algorithms.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Others",permalink:"/ai/move-37/others"},next:{title:"Quizzes",permalink:"/ai/move-37/quizzes"}},c={},s=[{value:"Policy Gradient Method - Attempts to learn functions which directly map an observation to an action",id:"policy-gradient-method---attempts-to-learn-functions-which-directly-map-an-observation-to-an-action",level:2},{value:"Q-Learning - Attempts to learn the value of being in a given state, and taking a specific action there",id:"q-learning---attempts-to-learn-the-value-of-being-in-a-given-state-and-taking-a-specific-action-there",level:2},{value:"Policy - Policy is a simple lookup table: state -&gt; best action",id:"policy---policy-is-a-simple-lookup-table-state---best-action",level:2},{value:"Reward - the reward from our immediate action, plus all discounted future rewards from applying the current policy (Denoted by capital G)",id:"reward---the-reward-from-our-immediate-action-plus-all-discounted-future-rewards-from-applying-the-current-policy-denoted-by-capital-g",level:2}],p={toc:s},m="wrapper";function u(e){let{components:t,...i}=e;return(0,r.kt)(m,(0,n.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"q-learning-algorithms"},"Q-Learning Algorithms"),(0,r.kt)("p",null,"Q-Learning algorithms are a family of Reinforcement Learning algorithms."),(0,r.kt)("p",null,"Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."),(0,r.kt)("h2",{id:"policy-gradient-method---attempts-to-learn-functions-which-directly-map-an-observation-to-an-action"},"Policy Gradient Method - Attempts to learn functions which directly map an observation to an action"),(0,r.kt)("h2",{id:"q-learning---attempts-to-learn-the-value-of-being-in-a-given-state-and-taking-a-specific-action-there"},"Q-Learning - Attempts to learn the value of being in a given state, and taking a specific action there"),(0,r.kt)("p",null,"Q - Quality"),(0,r.kt)("p",null,"Q - Long term discounted reward we expect from taking action a in state s"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:a(589395).Z,width:"1101",height:"415"})),(0,r.kt)("p",null,"The policy for state s is to choose the actual bias Q value."),(0,r.kt)("h2",{id:"policy---policy-is-a-simple-lookup-table-state---best-action"},"Policy - Policy is a simple lookup table: state -> best action"),(0,r.kt)("h2",{id:"reward---the-reward-from-our-immediate-action-plus-all-discounted-future-rewards-from-applying-the-current-policy-denoted-by-capital-g"},"Reward - the reward from our immediate action, plus all discounted future rewards from applying the current policy (Denoted by capital G)"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:a(517224).Z,width:"1100",height:"580"})))}u.isMDXComponent=!0},589395:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Q-Learning-Algorithms-image1-3abc6595fe95f21757cb6e30ce56a139.jpg"},517224:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Q-Learning-Algorithms-image2-af1354d144e7d6ef5661eb3ffb9d7200.jpg"}}]);