"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[72555],{603905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>k});var n=a(667294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=n.createContext({}),s=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=s(e.components);return n.createElement(p.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=s(a),d=r,k=c["".concat(p,".").concat(d)]||c[d]||m[d]||i;return a?n.createElement(k,o(o({ref:t},u),{},{components:a})):n.createElement(k,o({ref:t},u))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l[c]="string"==typeof e?e:r,o[1]=l;for(var s=2;s<i;s++)o[s]=a[s];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},287437:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var n=a(487462),r=(a(667294),a(603905));const i={},o="Course - Launching into ML",l={unversionedId:"ai/courses/course-launching-into-ml",id:"ai/courses/course-launching-into-ml",title:"Course - Launching into ML",description:"Objectives",source:"@site/docs/ai/courses/course-launching-into-ml.md",sourceDirName:"ai/courses",slug:"/ai/courses/course-launching-into-ml",permalink:"/ai/courses/course-launching-into-ml",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/ai/courses/course-launching-into-ml.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Course - Intro to TensorFlow",permalink:"/ai/courses/course-intro-to-tensorflow"},next:{title:"Exponential Smoothing",permalink:"/ai/courses/course-time-series-analysis/exponential-smoothing"}},p={},s=[{value:"Objectives",id:"objectives",level:2},{value:"Prediction Problem",id:"prediction-problem",level:2},{value:"Practical ML",id:"practical-ml",level:2},{value:"Optimization",id:"optimization",level:2},{value:"Generalization and Sampling",id:"generalization-and-sampling",level:2}],u={toc:s},c="wrapper";function m(e){let{components:t,...a}=e;return(0,r.kt)(c,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"course---launching-into-ml"},"Course - Launching into ML"),(0,r.kt)("h2",{id:"objectives"},"Objectives"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Identify why deep learning is currently popular")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Optimize and evaluate models using loss functions and performance metrics")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Mitigate common problems that arise in Machine Learning"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Lack of generalization"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create repeatable training, evaluation and test datasets")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Optimization - Set up a supervised learning problem and find solution using gradient descent")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Performance metrics and how to choose between different models")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Intuitive understanding of neural networks")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Tensor flow playground")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Generalization and sampling"))),(0,r.kt)("h2",{id:"prediction-problem"},"Prediction Problem"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Supervised Learning"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Classification problem")))),(0,r.kt)("p",null,"Classification models usually use cross entropy as their loss function (the penalty for cross entropy is almost linear and the predicted probability is close to actual label, but as it gets further away it becomes exponential, when it gets close to predicting the opposite class of the label)"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Regression problem")),(0,r.kt)("p",null,"Regression models usually use mean squared error as their loss function (Quadratic penalty for mean squared error, so it is essentially trying to minimize the euclidean distance between the actual label and the predicted label)"),(0,r.kt)("p",null,"Description Problem"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Unsupervised learning")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.coursera.org/learn/launching-machine-learning"},"https://www.coursera.org/learn/launching-machine-learning")),(0,r.kt)("h2",{id:"practical-ml"},"Practical ML"),(0,r.kt)("p",null,"In this module, we will introduce some of the main types of machine learning and review the history of ML leading up to the state of the art so that you can accelerate your growth as an ML practitioner."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://www.coursera.org/learn/launching-machine-learning/lecture/j4Rbd/introduction"},"Video:Introduction"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Supervised Learning")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Regression and Classification")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Short History of ML: Linear Regression")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Short History of ML: Perceptron")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Short History of ML: Neural Networks")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Short History of ML: Decision Trees")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Short History of ML: Kernel Methods")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Short History of ML: Random Forests")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Short History of ML: Modern Neural Networks")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Discussion Prompt:Modern Neural Networks"))),(0,r.kt)("h2",{id:"optimization"},"Optimization"),(0,r.kt)("p",null,"In this module we will walk you through how to optimize your ML models."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://www.coursera.org/learn/launching-machine-learning/lecture/ebCZS/introduction"},"Video:Introduction"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Defining ML Models")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Introducing the Natality Dataset")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Introducing Loss Functions")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Gradient Descent")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Troubleshooting a Loss Curve")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:ML Model Pitfalls")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Lab: Introducing the TensorFlow Playground")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Lab: TensorFlow Playground - Advanced")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Lab: Practicing with Neural Networks")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Loss Curve Troubleshooting")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Video:Performance Metrics"))),(0,r.kt)("h2",{id:"generalization-and-sampling"},"Generalization and Sampling"),(0,r.kt)("p",null,"Now it's time to answer a rather weird question: when is the most accurate ML model not the right one to pick? As we hinted at in the last module on Optimization -- simply because a model has a loss metric of 0 for your training dataset does not mean it will perform well on new data in the real world."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudPlatform/training-data-analyst"},"https://github.com/GoogleCloudPlatform/training-data-analyst")))}m.isMDXComponent=!0}}]);