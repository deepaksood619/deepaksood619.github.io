"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[32414],{603905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>g});var r=n(667294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),c=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=c(n),p=a,g=d["".concat(s,".").concat(p)]||d[p]||m[p]||o;return n?r.createElement(g,i(i({ref:t},u),{},{components:n})):r.createElement(g,i({ref:t},u))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:a,i[1]=l;for(var c=2;c<o;c++)i[c]=n[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}p.displayName="MDXCreateElement"},687413:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var r=n(487462),a=(n(667294),n(603905));const o={},i="Learning Vector Quantization (LVQ)",l={unversionedId:"ai/ml-algorithms/learning-vector-quantization-lvq",id:"ai/ml-algorithms/learning-vector-quantization-lvq",title:"Learning Vector Quantization (LVQ)",description:"Hi, adownside of K-Nearest Neighbors is that you need to hang on to your entire training dataset.",source:"@site/docs/ai/ml-algorithms/learning-vector-quantization-lvq.md",sourceDirName:"ai/ml-algorithms",slug:"/ai/ml-algorithms/learning-vector-quantization-lvq",permalink:"/ai/ml-algorithms/learning-vector-quantization-lvq",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/ai/ml-algorithms/learning-vector-quantization-lvq.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"K-Nearest Neighbor (KNN)",permalink:"/ai/ml-algorithms/k-nearest-neighbor-knn"},next:{title:"Linear Discriminant Analysis (LDA)",permalink:"/ai/ml-algorithms/linear-discriminant-analysis-lda"}},s={},c=[],u={toc:c},d="wrapper";function m(e){let{components:t,...n}=e;return(0,a.kt)(d,(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"learning-vector-quantization-lvq"},"Learning Vector Quantization (LVQ)"),(0,a.kt)("p",null,"Hi, adownside of K-Nearest Neighbors is that you need to hang on to your entire training dataset."),(0,a.kt)("p",null,"The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like."),(0,a.kt)("p",null,"The representation for LVQ is a collection of codebook vectors. These are selected randomly in the beginning and adapted to best summarize the training dataset over a number of iterations of the learning algorithm."),(0,a.kt)("p",null,"After learned, the codebook vectors can be used to make predictions just like K-Nearest Neighbors. The most similar neighbor (best matching codebook vector) is found by calculating the distance between each codebook vector and the new data instance. The class value or (real value in the case of regression) for the best matching unit is then returned as the prediction."),(0,a.kt)("p",null,"Best results are achieved if you rescale your data to have the same range, such as between 0 and 1."),(0,a.kt)("p",null,"If you discover that KNNgives good results on your dataset try using LVQ to reduce the memory requirements of storing the entire training dataset."))}m.isMDXComponent=!0}}]);