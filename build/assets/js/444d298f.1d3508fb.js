"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[50349],{603905:(e,a,t)=>{t.d(a,{Zo:()=>k,kt:()=>f});var n=t(667294);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){o(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,o=function(e,a){if(null==e)return{};var t,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(o[t]=e[t]);return o}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=n.createContext({}),p=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},k=function(e){var a=p(e.components);return n.createElement(l.Provider,{value:a},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},d=n.forwardRef((function(e,a){var t=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,k=s(e,["components","mdxType","originalType","parentName"]),c=p(t),d=o,f=c["".concat(l,".").concat(d)]||c[d]||m[d]||r;return t?n.createElement(f,i(i({ref:a},k),{},{components:t})):n.createElement(f,i({ref:a},k))}));function f(e,a){var t=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var r=t.length,i=new Array(r);i[0]=d;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[c]="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=t[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"},880016:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var n=t(487462),o=(t(667294),t(603905));const r={},i="kafkacat",s={unversionedId:"technologies/kafka/kafkacat",id:"technologies/kafka/kafkacat",title:"kafkacat",description:"kafkacat is a generic non-JVM producer and consumer for Apache Kafka >=0.8, think of it as a netcat for Kafka.",source:"@site/docs/technologies/kafka/kafkacat.md",sourceDirName:"technologies/kafka",slug:"/technologies/kafka/kafkacat",permalink:"/technologies/kafka/kafkacat",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/technologies/kafka/kafkacat.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Kafka Topic/Replication",permalink:"/technologies/kafka/kafka-topic-replication"},next:{title:"Monitoring",permalink:"/technologies/kafka/monitoring"}},l={},p=[{value:"Installing",id:"installing",level:4},{value:"Commands",id:"commands",level:2},{value:"Consumers",id:"consumers",level:3},{value:"Metadata listing",id:"metadata-listing",level:2},{value:"Producers",id:"producers",level:2},{value:"Kafkacat documentation",id:"kafkacat-documentation",level:2},{value:"Example Commands",id:"example-commands",level:2},{value:"metadata listing",id:"metadata-listing-1",level:3},{value:"consumer - get data from bank_data",id:"consumer---get-data-from-bank_data",level:3},{value:"get size of the packets",id:"get-size-of-the-packets",level:3},{value:"producer",id:"producer",level:3}],k={toc:p},c="wrapper";function m(e){let{components:a,...t}=e;return(0,o.kt)(c,(0,n.Z)({},k,t,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"kafkacat"},"kafkacat"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/edenhill/kafkacat"},"https://github.com/edenhill/kafkacat")),(0,o.kt)("p",null,"kafkacat is a generic non-JVM producer and consumer for Apache Kafka >=0.8, think of it as a netcat for Kafka."),(0,o.kt)("p",null,"In producer mode kafkacat reads messages from stdin, delimited with a configurable delimiter (-D, defaults to newline), and produces them to the provided Kafka cluster (-b), topic (-t) and partition (-p)."),(0,o.kt)("p",null,"In consumer mode kafkacat reads messages from a topic and partition and prints them to stdout using the configured message delimiter."),(0,o.kt)("p",null,"There's also support for the Kafka >=0.9 high-level balanced consumer, use the ",(0,o.kt)("inlineCode",{parentName:"p"},"-G <group>")," switch and provide a list of topics to join the group."),(0,o.kt)("p",null,"kafkacat also features a Metadata list (-L) mode to display the current state of the Kafka cluster and its topics and partitions."),(0,o.kt)("p",null,"Supports Avro message deserialization using the Confluent Schema-Registry, and generic primitive deserializers (see examples below)."),(0,o.kt)("p",null,"kafkacat is fast and lightweight; statically linked it is no more than 150Kb."),(0,o.kt)("h4",{id:"installing"},"Installing"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"brew install kafkacat\napt-get install kafkacat\n")),(0,o.kt)("h2",{id:"commands"},"Commands"),(0,o.kt)("h3",{id:"consumers"},"Consumers"),(0,o.kt)("p",null,"High-level balanced KafkaConsumer: subscribe to topic1 and topic2 (requires broker >=0.9.0 and librdkafka version >=0.9.1)"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b localhost:9091 -G mygroup topic1 topic2")),(0,o.kt)("p",null,"Read messages from Kafka 'syslog' topic, print to stdout"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -t druid_uncompressed -c 10")),(0,o.kt)("p",null,"Read the last 2000 messages from 'syslog' topic, then exit"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kafkacat -C -b mybroker -t syslog -p 0 -o -2000 -e\n\nkafkacat -C -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -t druid_compressed -p 0 -o -2000 -e\n")),(0,o.kt)("p",null,"Consume from all partitions from 'syslog' topic"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -C -b mybroker -t syslog")),(0,o.kt)("p",null,"Output consumed messages in JSON envelope:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -t syslog -J")),(0,o.kt)("p",null,"Decode Avro key (-s key=avro), value (-s value=avro) or both (-s avro) to JSON using schema from the Schema-Registry:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -t ledger -s avro -r <http://schema-registry-url:8080>")),(0,o.kt)("p",null,'Decode Avro message value and extract Avro record\'s "age" field:'),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -t ledger -s value=avro -r <http://schema-registry-url:8080> | jq .payload.age")),(0,o.kt)("p",null,"Decode key as 32-bit signed integer and value as 16-bit signed integer followed by an unsigned byte followed by string:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -t mytopic -s key='i$' -s value='hB s'")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"Hint: see./kafkacat -h for all available deserializer options.")),(0,o.kt)("p",null,"Output consumed messages according to format string:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -t syslog -f 'Topic %t [%p], offset: %o, key: %k, payload: %S bytes: %sn'")),(0,o.kt)("p",null,"Read the last 100 messages from topic 'syslog' with librdkafka configuration parameter 'broker.version.fallback' set to '0.8.2.1' :"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -C -b mybroker -X broker.version.fallback=0.8.2.1 -t syslog -p 0 -o -100 -e")),(0,o.kt)("p",null,"Print headers in consumer:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -C -t mytopic -f 'Headers: %h: Message value: %sn'")),(0,o.kt)("p",null,"Enable the idempotent producer, providing exactly-once and strict-orderingproducerguarantees:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -X enable.idempotence=true -P -t mytopic ....")),(0,o.kt)("h2",{id:"metadata-listing"},"Metadata listing"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -L -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092")),(0,o.kt)("p",null,"Metadata for all topics (from broker 1: mybroker:9092/1):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'3 brokers:\nbroker 1 at mybroker:9092\nbroker 2 at mybrokertoo:9092\nbroker 3 at thirdbroker:9092\n16 topics:\ntopic "syslog" with 3 partitions:\npartition 0, leader 3, replicas: 1,2,3, isrs: 1,2,3\npartition 1, leader 1, replicas: 1,2,3, isrs: 1,2,3\npartition 2, leader 1, replicas: 1,2, isrs: 1,2\ntopic "rdkafkatest1_auto_49f744a4327b1b1e" with 2 partitions:\npartition 0, leader 3, replicas: 3, isrs: 3\npartition 1, leader 1, replicas: 1, isrs: 1\ntopic "rdkafkatest1_auto_e02f58f2c581cba" with 2 partitions:\npartition 0, leader 3, replicas: 3, isrs: 3\npartition 1, leader 1, replicas: 1, isrs: 1\n')),(0,o.kt)("p",null,"JSON metadata listing"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -L -J")),(0,o.kt)("p",null,"Pretty-printed JSON metadata listing"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -b mybroker -L -J | jq .")),(0,o.kt)("p",null,"Query offset(s) by timestamp(s)"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kafkacat -b **kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092** -Q -t druid_telemetry_data_Samhi:0:1569048234230\n\nkafkacat -b **kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092** -Q -t druid_telemetry_data_Samhi:0:**1568989500000**\n")),(0,o.kt)("p",null,"Consume messages between two timestamps"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# Working\nkafkacat -b **kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092** -C -t druid_telemetry_data_Samhi -o s@1574063938000 -o e@1574063940000 **-**f 'nKey (%K bytes): %ktnValue (%S bytes): %snTimestamp: %TtPartition: %ptOffset: %on--n'\n\n# Redirect logs to different topic\nkafkacat -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -C -t **smap_samhi**-o s@**1568989590000**-o e@**1568989620000 | kafkacat -b**kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -P -t samhi_logs\n")),(0,o.kt)("h2",{id:"producers"},"Producers"),(0,o.kt)("p",null,"Read messages from stdin, produce to 'syslog' topic with snappy compression"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"tail -f /var/log/syslog | kafkacat -b mybroker -t syslog -z snappy")),(0,o.kt)("p",null,"Produce messages from file (one file is one message)"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat -P -b mybroker -t filedrop -p 0 myfile1.bin /etc/motd thirdfile.tgz")),(0,o.kt)("p",null,'Produce a tombstone (a "delete" for compacted topics) for key "abc" by providing an empty message value which-Zinterpretes as NULL:'),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},'echo "abc:" | kafkacat -b mybroker -t mytopic -Z -K:')),(0,o.kt)("p",null,"Produce with headers:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},'echo "hello there" | kafkacat -b mybroker -H "header1=header value" -H "nullheader" -H "emptyheader=" -H "header1=duplicateIsOk"')),(0,o.kt)("h2",{id:"kafkacat-documentation"},"Kafkacat documentation"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"Usage: kafkacat <options> [file1 file2 .. | topic1 topic2 ..]]")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kafkacat - Apache Kafka producer and consumer tool")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/edenhill/kafkacat"},"https://github.com/edenhill/kafkacat")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'-C | -P | -L | -Q  Mode: Consume, Produce, Metadata List, Query mode\n   -G <group-id>      Mode: High-level KafkaConsumer (Kafka >=0.9 balanced consumer groups)\n                      Expects a list of topics to subscribe to\n   -t <topic>         Topic to consume from, produce to, or list\n   -p <partition>     Partition\n   -b <brokers,..>    Bootstrap broker(s) (host [:port])\n   -D <delim>         Message delimiter character:\n                      a-z.. | \\r | \\n | \\t | \\xNN\n                      Default: \\n\n   -E                 Do not exit on non fatal error\n   -K <delim>         Key delimiter (same format as -D)\n   -c <cnt>           Limit message count\n   -F <config-file>   Read configuration properties from file,\n                      file format is "property=value".\n                      The KAFKACAT_CONFIG=path environment can also be used, but -F takes preceedence.\n                      The default configuration file is $HOME/.config/kafkacat.conf\n   -X list            List available librdkafka configuration properties\n   -X prop=val        Set librdkafka configuration property.\n                      Properties prefixed with "topic." are\n                      applied as topic properties.\n   -X dump            Dump configuration and exit.\n   -d <dbg1,...>      Enable librdkafka debugging:\n                      all,generic,broker,topic,metadata,feature,queue,msg,protocol,cgrp,security,fetch,interceptor,plugin,consumer,admin,eos\n   -q                 Be quiet (verbosity set to 0)\n   -v                 Increase verbosity\n   -V                 Print version\n   -h                 Print usage help\n\n Producer options:\n   -z snappy|gzip|lz4 Message compression. Default: none\n   -p -1              Use random partitioner\n   -D <delim>         Delimiter to split input into messages\n   -K <delim>         Delimiter to split input key and message\n   -k <str>           Use a fixed key for all messages.\n                      If combined with -K, per-message keys\n                      takes precendence.\n   -H <header=value>  Add Message Headers (may be specified multiple times)\n   -l                 Send messages from a file separated by\n                      delimiter, as with stdin.\n                      (only one file allowed)\n   -T                 Output sent messages to stdout, acting like tee.\n   -c <cnt>           Exit after producing this number of messages\n   -Z                 Send empty messages as NULL messages\n   file1 file2..      Read messages from files.\n                      With -l, only one file permitted.\n                      Otherwise, the entire file contents will\n                      be sent as one single message.\n\n Consumer options:\n   -o <offset>        Offset to start consuming from:\n                      beginning | end | stored |\n                      <value>  (absolute offset) |\n                      -<value> (relative offset from end)\n                      s@<value> (timestamp in ms to start at)\n                      e@<value> (timestamp in ms to stop at (not included))\n   -e                 Exit successfully when last message received\n   -f <fmt..>         Output formatting string, see below.\n                      Takes precedence over -D and -K.\n   -J                 Output with JSON envelope\n   -s key=<serdes>    Deserialize non-NULL keys using <serdes>.\n   -s value=<serdes>  Deserialize non-NULL values using <serdes>.\n   -s <serdes>        Deserialize non-NULL keys and values using <serdes>.\n                      Available deserializers (<serdes>):\n                        <pack-str> - A combination of:\n                                     <: little-endian,\n                                     >: big-endian (recommended),\n                                     b: signed 8-bit integer\n                                     B: unsigned 8-bit integer\n                                     h: signed 16-bit integer\n                                     H: unsigned 16-bit integer\n                                     i: signed 32-bit integer\n                                     I: unsigned 32-bit integer\n                                     q: signed 64-bit integer\n                                     Q: unsigned 64-bit integer\n                                     c: ASCII character\n                                     s: remaining data is string\n                                     $: match end-of-input (no more bytes remaining or a parse error is raised).\n                                        Not including this token skips any\n                                        remaining data after the pack-str is\n                                        exhausted.\n   -D <delim>         Delimiter to separate messages on output\n   -K <delim>         Print message keys prefixing the message\n                      with specified delimiter.\n   -O                 Print message offset using -K delimiter\n   -c <cnt>           Exit after consuming this number of messages\n   -Z                 Print NULL values and keys as "NULL"instead of empty.\n                      For JSON (-J) the nullstr is always null.\n   -u                 Unbuffered output\n\n Metadata options (-L):\n   -t <topic>         Topic to query (optional)\n\n Query options (-Q):\n   -t <t>:<p>:<ts>    Get offset for topic <t>,\n                      partition <p>, timestamp <ts>.\n                      Timestamp is the number of milliseconds\n                      since epoch UTC.\n                      Requires broker >= 0.10.0.0 and librdkafka >= 0.9.3.\n                      Multiple -t .. are allowed but a partition\n                      must only occur once.\n\n Format string tokens:\n   %s                 Message payload\n   %S                 Message payload length (or -1 for NULL)\n   %R                 Message payload length (or -1 for NULL) serialized\n                      as a binary big endian 32-bit signed integer\n   %k                 Message key\n   %K                 Message key length (or -1 for NULL)\n   %T                 Message timestamp (milliseconds since epoch UTC)\n   %h                 Message headers (n=v CSV)\n   %t                 Topic\n   %p                 Partition\n   %o                 Message offset\n   \\n \\r \\t           Newlines, tab\n   \\xXX \\xNNN         Any ASCII character\n  Example:\n   -f \'Topic %t [%p] at offset %o: key %k: %s\\n\'\n\n JSON message envelope (on one line) when consuming with -J:\n  { "topic": str, "partition": int, "offset": int,\n    "tstype": "create|logappend|unknown", "ts": int, // timestamp in milliseconds since epoch\n    "headers": { "<name>": str, .. }, // optional\n    "key": str|json, "payload": str|json,\n    "key_error": str, "payload_error": str } //optional\n  (note: key_error and payload_error are only included if deserialization failed)\n\n Consumer mode (writes messages to stdout):\n    kafkacat -b <broker> -t <topic> -p <partition>\n  or:\n   kafkacat -C -b ...\n\n High-level KafkaConsumer mode:\n   kafkacat -b <broker> -G <group-id> topic1 top2 ^aregex\\d+\n\n Producer mode (reads messages from stdin):\n   ... | kafkacat -b <broker> -t <topic> -p <partition>\n  or:\n   kafkacat -P -b ...\n\n Metadata listing:\n   kafkacat -L -b <broker> [-t <topic>]\n\n Query offset by timestamp:\n  kafkacat -Q -b broker -t <topic>:<partition>:<timestamp>\n')),(0,o.kt)("h2",{id:"example-commands"},"Example Commands"),(0,o.kt)("h3",{id:"metadata-listing-1"},"metadata listing"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kafkacat -L -b kafka0.example.com:9094,kafka1.example.com,kafka2.example.com\n\nkafkacat -L -b localhost:9094\n")),(0,o.kt)("h3",{id:"consumer---get-data-from-bank_data"},"consumer - get data from bank_data"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kafkacat -C -b my-cluster-kafka-brokers.kafka:9092 -t bank_data -p 0 -o -2000 -e\n\nkafkacat -C -b kafka0.example.com:9094,kafka1.example.com,kafka2.example.com -t test\n\nkafkacat -C -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test_bank_data -o -2000 -f 'nKey (%K bytes): %ktnValue (%S bytes): %snTimestamp: %TtPartition: %ptOffset: %on--n'\n")),(0,o.kt)("h3",{id:"get-size-of-the-packets"},"get size of the packets"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kafkacat -C -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test_bank_data -o -2000 -f 'nValue (%S bytes) t Timestamp: %TtPartition: %ptOffset: %o'\n\nkafkacat -C -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test_bank_data -o -2000 -f 'n%S,%T,%p,%o'\n")),(0,o.kt)("h3",{id:"producer"},"producer"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'echo "hello" | kafkacat -P -b my-cluster-kafka-brokers.kafka:9092 -t test\n\nwhile true; do echo $(($(date +%s%N)/1000000)) | kafkacat -P -b my-cluster-kafka-brokers.kafka:9092 -t test; sleep 2; echo $(($(date +%s%N)/1000000)); done\n\nkafkacat -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test -c 10\n')))}m.isMDXComponent=!0}}]);