"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[66733],{603905:(e,t,i)=>{i.d(t,{Zo:()=>m,kt:()=>u});var n=i(667294);function a(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function o(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function r(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?o(Object(i),!0).forEach((function(t){a(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):o(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function l(e,t){if(null==e)return{};var i,n,a=function(e,t){if(null==e)return{};var i,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||(a[i]=e[i]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(a[i]=e[i])}return a}var s=n.createContext({}),p=function(e){var t=n.useContext(s),i=t;return e&&(i="function"==typeof e?e(t):r(r({},t),e)),i},m=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var i=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),d=p(i),h=a,u=d["".concat(s,".").concat(h)]||d[h]||c[h]||o;return i?n.createElement(u,r(r({ref:t},m),{},{components:i})):n.createElement(u,r({ref:t},m))}));function u(e,t){var i=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=i.length,r=new Array(o);r[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:a,r[1]=l;for(var p=2;p<o;p++)r[p]=i[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,i)}h.displayName="MDXCreateElement"},738587:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var n=i(487462),a=(i(667294),i(603905));const o={},r="3. Monte Carlo Methods",l={unversionedId:"ai/move-37/3-monte-carlo-methods",id:"ai/move-37/3-monte-carlo-methods",title:"3. Monte Carlo Methods",description:"Internet of Things Optimization",source:"@site/docs/ai/move-37/3-monte-carlo-methods.md",sourceDirName:"ai/move-37",slug:"/ai/move-37/3-monte-carlo-methods",permalink:"/ai/move-37/3-monte-carlo-methods",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/ai/move-37/3-monte-carlo-methods.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"2. Dynamic Programming",permalink:"/ai/move-37/2-dynamic-programming"},next:{title:"4. Model Free Learning",permalink:"/ai/move-37/4-model-free-learning"}},s={},p=[{value:"Internet of Things Optimization",id:"internet-of-things-optimization",level:2},{value:"Exploration vs Exploitation",id:"exploration-vs-exploitation",level:2},{value:"MC Control and MC Prediction",id:"mc-control-and-mc-prediction",level:2},{value:"Model Free Learning",id:"model-free-learning",level:3},{value:"MC Method - Monte Carlo Method for model free learning",id:"mc-method---monte-carlo-method-for-model-free-learning",level:2},{value:"First-visit MC:average returns only for first time s is visited in an episode",id:"first-visit-mcaverage-returns-only-for-first-time-s-is-visited-in-an-episode",level:2},{value:"Every-Visit MC:average returns for every time s is visited in an episode",id:"every-visit-mcaverage-returns-for-every-time-s-is-visited-in-an-episode",level:2},{value:"Epsilon-greedy policy",id:"epsilon-greedy-policy",level:2},{value:"GLIE Monte Carlo Method (Greedy in the Limit of Infinite Exploration)",id:"glie-monte-carlo-method-greedy-in-the-limit-of-infinite-exploration",level:2},{value:"Temporal Difference Learning",id:"temporal-difference-learning",level:2},{value:"Q-Learning for Trading",id:"q-learning-for-trading",level:2}],m={toc:p},d="wrapper";function c(e){let{components:t,...o}=e;return(0,a.kt)(d,(0,n.Z)({},m,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"3-monte-carlo-methods"},"3. Monte Carlo Methods"),(0,a.kt)("h2",{id:"internet-of-things-optimization"},"Internet of Things Optimization"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Monte Carlo vs Dynamic Programming",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"No need for a complete Markov Decision Process"),(0,a.kt)("li",{parentName:"ul"},"Computationally more efficient"),(0,a.kt)("li",{parentName:"ul"},"Can be used with stochastic simulations"))),(0,a.kt)("li",{parentName:"ul"},"In model-free reinforcement learning, as opposed to model based, we don't know the reward function and the transition function beforehand we have to learn them though experience."),(0,a.kt)("li",{parentName:"ul"},"A model-free learning technique called monte carlo uses repeated random sampling to obtain numerical results"),(0,a.kt)("li",{parentName:"ul"},"In first visite monte carlo the state value function is defined as the average of the returns following the agents first visit to S in a set of episodes")),(0,a.kt)("h2",{id:"exploration-vs-exploitation"},"Exploration vs Exploitation"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Example - In the context of a restaurant, we can order the same meal that we have already eaten, so ",(0,a.kt)("strong",{parentName:"li"},"exploit")," our previous knowledge to get already tested meal, vs we can ",(0,a.kt)("strong",{parentName:"li"},"explore")," new items in the hope of eating more tastier meal."),(0,a.kt)("li",{parentName:"ul"},"We can do this in RL by adjusting \u03f5 (epsilon), to balance between exploration vs exploitation to get the maximum reward possible."),(0,a.kt)("li",{parentName:"ul"},"Exploration vs Exploitation tradeoff"),(0,a.kt)("li",{parentName:"ul"},"Epsilon Greedy")),(0,a.kt)("p",null,"epsilonis the fraction of times we sample a lever randomly and1- epsilonis the fraction of times we choose optimally."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Thompson Sampling")),(0,a.kt)("p",null,"The basic idea is toassume a simple prior distributionon the underlying parameters of the reward distribution of every lever, and at every time step, play a lever according to itsposterior probabilityof being the best arm."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Multi-armed bandit problem"))),(0,a.kt)("p",null,"In ",(0,a.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Probability_theory"},"probability theory"),", the",(0,a.kt)("strong",{parentName:"p"},"multi-armed bandit problem"),"(sometimes called the",(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("em",{parentName:"strong"},"K"),"-or",(0,a.kt)("em",{parentName:"strong"},"N"),"-armed bandit problem"),") is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice."),(0,a.kt)("p",null,"The name comes from imagining a ",(0,a.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Gambler"},"gambler")," at a row of ",(0,a.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Slot_machines"},"slot machines"),'(sometimes known as "one-armed bandits"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.The multi-armed bandit problem also falls into the broad category of ',(0,a.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Stochastic_scheduling"},"stochastic scheduling"),"."),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Multi-armed_bandit"},"https://en.wikipedia.org/wiki/Multi-armed_bandit")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"We must strike a balance between explore/exploit")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Epsilon Greedy - epsilon (a hyperparameter) is the probability that our agent will choose a random action instead of following policy")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Algorithm"),(0,a.kt)("ol",{parentName:"li"},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"generate a random number p, between 0 and 1")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"if p < (1-\u03b5) take the action dictated by policy")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"Otherwise take a random action")))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"First visit optimization"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"What happens if we visit the same state more than once"),(0,a.kt)("li",{parentName:"ul"},"It's been proven the subsequent visits doesn't change the answer"),(0,a.kt)("li",{parentName:"ul"},"All we need is the first visit"),(0,a.kt)("li",{parentName:"ul"},"We throw rest of the data away"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Monte Carlo Q Learning Algorithm"))),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"image",src:i(749711).Z,width:"1101",height:"690"})),(0,a.kt)("h2",{id:"mc-control-and-mc-prediction"},"MC Control and MC Prediction"),(0,a.kt)("p",null,"There are two types of tasks in reinforcement learning -- Prediction and Control."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Prediction"))),(0,a.kt)("p",null,"A task that can predict expected total reward from any given state assuming the function\u03c0(a|s)is given. Prediction calculates the value functionV\u03c0"),(0,a.kt)("p",null,"e.g.:Policyevaluation (Estimate)."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Control"))),(0,a.kt)("p",null,"A task that can findpolicy\u03c0(a|s)that maximizes the expected total reward from any given state. Simply if given a policy\u03c0control finds an optimal policy\u03c0*."),(0,a.kt)("p",null,"e.g.:Policy Improvement (Optimize)."),(0,a.kt)("p",null,"Policy Iteration is a combination of prediction and control to find optimal policy."),(0,a.kt)("p",null,"There are two types of policy learning methods -"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"On policy learning"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"This methodlearns on the job, it evaluates or improves the policy that used to make the decisions."),(0,a.kt)("li",{parentName:"ul"},"We must act based on our current policy"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Off policy learning"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"This method evaluates one policy while following another policy. The earlier is called target policy which may be deterministic and thelatterbehaviorpolicy is stochastic."),(0,a.kt)("li",{parentName:"ul"},"Any action is okay")))),(0,a.kt)("h3",{id:"model-free-learning"},"Model Free Learning"),(0,a.kt)("p",null,"Learn a problem when not all the components are available. In model free learning we just focus on calculating the value functions directly from the interactions with the environment. Our aim here is to figureout Vfor unknown MDP assuming that we have a policy."),(0,a.kt)("h2",{id:"mc-method---monte-carlo-method-for-model-free-learning"},"MC Method - Monte Carlo Method for model free learning"),(0,a.kt)("p",null,"There are two different types of MC."),(0,a.kt)("h2",{id:"first-visit-mcaverage-returns-only-for-first-time-s-is-visited-in-an-episode"},"First-visit MC:average returns only for first time s is visited in an episode"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Incremental Mean")),(0,a.kt)("h2",{id:"every-visit-mcaverage-returns-for-every-time-s-is-visited-in-an-episode"},"Every-Visit MC:average returns for every time s is visited in an episode"),(0,a.kt)("p",null,"Both converge asymptotically."),(0,a.kt)("p",null,"Monte Carlo methods for control task (Policy iteration is the base of control task)"),(0,a.kt)("p",null,"Inmodel-free RL, we need to interact with the environment to find out the best strategy so we need to explore the entire the state space while figuring out best actions."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Exploration:is about finding more information about the environment. In other words exploring a lot of states and actions in the environment."),(0,a.kt)("li",{parentName:"ul"},"Exploitation:is about exploiting the known information to maximize the reward.")),(0,a.kt)("p",null,"Example for exploration vs exploitation (in context of Roomba, floor cleaning robot) - When the state is in charged up mode It needs to cover maximum area in a grid world for cleaning which falls under exploration. When the state of the machine changes to low batteryitneeds to find the charging dock as soon as possible to avoid getting stuck here the robot needs to exploit rather than explore to maximize the reward.So due to the exploration problem, we cannot expect Roomba to act greedily in MC to improve policy instead we use the epsilon-greedy policy."),(0,a.kt)("h2",{id:"epsilon-greedy-policy"},"Epsilon-greedy policy"),(0,a.kt)("p",null,"Thebest-knownaction based on our experience is selected with (1-epsilon) probability and the rest of the time i.e. with epsilon probability any action is selected randomly. Initially, epsilon is 1 so we can explore more but as we do many iterations we slowly decrease the epsilon to 0( whichis exploitation \u2192 choosing the best-known action) this get us to have the value of epsilonbetween 0 and 1."),(0,a.kt)("h2",{id:"glie-monte-carlo-method-greedy-in-the-limit-of-infinite-exploration"},"GLIE Monte Carlo Method (Greedy in the Limit of Infinite Exploration)"),(0,a.kt)("p",null,"GLIE Monte-Carlo is an on-policy learning method that learns from complete episodes. For each state-action pair, we keep track of how many times the pair has been visited with a simple counter function."),(0,a.kt)("p",null,"N(St,At) \u2190 N(St,At) + 1"),(0,a.kt)("p",null,"For each episode, we can update our estimated value function using an incremental mean."),(0,a.kt)("p",null,"Q(St,At) \u2190 Q(St,At) + (1 / N(St,At)) (Gt--Q(St,At))"),(0,a.kt)("p",null,"Here,Gteither represents the return from timetwhen the agent first visited the state-action pair, or the sum of returns from each timetthat the agent visited the state-action pair, depending on whether you are using first-visit or every-visit Monte Carlo."),(0,a.kt)("p",null,"We'll adopt a\u03f5-greedy policy with\u03f5=1/kwherekrepresents the number of episodes our agent has learned from."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"image",src:i(886596).Z,width:"1124",height:"694"})),(0,a.kt)("h2",{id:"temporal-difference-learning"},"Temporal Difference Learning"),(0,a.kt)("p",null,"The temporal difference approach approximates the value of a state-action pair by comparing estimates at two points in time (thus the name, temporal difference). The intuition behind this method is that we can formulate a better estimate for the value of a state-action pair after having observed some of the reward that our agent accumulates after having visited a state and performing a given action."),(0,a.kt)("h2",{id:"q-learning-for-trading"},"Q-Learning for Trading"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"A partially observable markov decision process is one where we don't know what the true state looks like, but we can observe a part of it."),(0,a.kt)("li",{parentName:"ul"},"A Q table is one where the states and rows and actions are columns, it helps us find the best action to take for each state"),(0,a.kt)("li",{parentName:"ul"},"Q learning is the process of learning what this Q table is directly, without needing to learn either the transition probability or reward function")))}c.isMDXComponent=!0},749711:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/3.-Monte-Carlo-Methods-image1-9f4854e47609afe72d39aae5e7cde4d9.jpg"},886596:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/3.-Monte-Carlo-Methods-image2-f27b74b414feea491e83ba466b73c5bf.jpg"}}]);