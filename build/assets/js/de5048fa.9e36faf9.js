"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[2242],{603905:(e,a,t)=>{t.d(a,{Zo:()=>l,kt:()=>u});var i=t(667294);function s(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function n(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);a&&(i=i.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?n(Object(t),!0).forEach((function(a){s(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):n(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,i,s=function(e,a){if(null==e)return{};var t,i,s={},n=Object.keys(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||(s[t]=e[t]);return s}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var h=i.createContext({}),d=function(e){var a=i.useContext(h),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},l=function(e){var a=d(e.components);return i.createElement(h.Provider,{value:a},e.children)},c="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return i.createElement(i.Fragment,{},a)}},p=i.forwardRef((function(e,a){var t=e.components,s=e.mdxType,n=e.originalType,h=e.parentName,l=o(e,["components","mdxType","originalType","parentName"]),c=d(t),p=s,u=c["".concat(h,".").concat(p)]||c[p]||g[p]||n;return t?i.createElement(u,r(r({ref:a},l),{},{components:t})):i.createElement(u,r({ref:a},l))}));function u(e,a){var t=arguments,s=a&&a.mdxType;if("string"==typeof e||s){var n=t.length,r=new Array(n);r[0]=p;var o={};for(var h in a)hasOwnProperty.call(a,h)&&(o[h]=a[h]);o.originalType=e,o[c]="string"==typeof e?e:s,r[1]=o;for(var d=2;d<n;d++)r[d]=t[d];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}p.displayName="MDXCreateElement"},164324:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>h,contentTitle:()=>r,default:()=>g,frontMatter:()=>n,metadata:()=>o,toc:()=>d});var i=t(487462),s=(t(667294),t(603905));const n={},r="Partitioning / Sharding",o={unversionedId:"databases/sql-databases/partitioning-sharding",id:"databases/sql-databases/partitioning-sharding",title:"Partitioning / Sharding",description:"Partitioning/Sharding Data",source:"@site/docs/databases/sql-databases/partitioning-sharding.md",sourceDirName:"databases/sql-databases",slug:"/databases/sql-databases/partitioning-sharding",permalink:"/databases/sql-databases/partitioning-sharding",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/databases/sql-databases/partitioning-sharding.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Normalization",permalink:"/databases/sql-databases/normalization"},next:{title:"Postgres",permalink:"/databases/sql-databases/postgres/"}},h={},d=[{value:"Partitioning/Sharding Data",id:"partitioningsharding-data",level:2},{value:"Partitioning vs Sharding",id:"partitioning-vs-sharding",level:2},{value:"Logical vs Physical Shard",id:"logical-vs-physical-shard",level:2},{value:"Benefits of Sharding",id:"benefits-of-sharding",level:2},{value:"Drawbacks of Sharding",id:"drawbacks-of-sharding",level:2},{value:"Sharding Architectures",id:"sharding-architectures",level:2},{value:"Algorithmic vs Dynamic Sharding",id:"algorithmic-vs-dynamic-sharding",level:3},{value:"Entity Groups",id:"entity-groups",level:2},{value:"Key/Hash based sharding",id:"keyhash-based-sharding",level:3},{value:"Range based sharding",id:"range-based-sharding",level:3},{value:"Hash-Range combination sharding",id:"hash-range-combination-sharding",level:3},{value:"Directory based sharding",id:"directory-based-sharding",level:3},{value:"Others",id:"others",level:3},{value:"Should I Shard?",id:"should-i-shard",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Partition Types",id:"partition-types",level:2}],l={toc:d},c="wrapper";function g(e){let{components:a,...n}=e;return(0,s.kt)(c,(0,i.Z)({},l,n,{components:a,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"partitioning--sharding"},"Partitioning / Sharding"),(0,s.kt)("h2",{id:"partitioningsharding-data"},"Partitioning/Sharding Data"),(0,s.kt)("p",null,"We cannot store 1 Trillion entries in a database, so we can shard / split / divide databases into parts where one part is responsible for that amount of data."),(0,s.kt)("h2",{id:"partitioning-vs-sharding"},"Partitioning vs Sharding"),(0,s.kt)("p",null,'Shard is also commonly used to mean "shared nothing" partitioning. But it\'s also possible to have a "shared nothing" architecture without partitioning'),(0,s.kt)("p",null,"A partition is a physically separate file that comprises a subset of rows of a logical file, which occupies the same CPU+memory+storage node as its peer partitions.\nA shard is a physical compute node comprised of CPU+memory+storage. A shard's schema (and integrity constraints) may be replicated across as many shards as needed. Shards may contain unpartioned and partitioned tables.\nWhen using shards and partitions together we effectively have two keys which we can use to chunk out the data. How we decide to choose those keys depends on the query biases of the main applications reading and writing data.\nFor example, Facebook could shard its data by User Key (so you might live on one MySQL node, and I might live on another MySQL node). But within those nodes, they could also partition data based on Create Date of the timeline item (e.g. items posted could be broken down by month).\nThis sharding and partitioning scheme would make sense for Facebook because of Facebook's natural query biases during operations. People normally look at the stuff that pertains to them (which can live on the same shard - even if some of that data is replicated from other user shards), and they will then dig into stuff that is recent (which will live within a small physical file, holding the current month's partition). So the partition key could be based on User ID + Month ID (since presumably within the shard multiple users would still exist). Keep in mind that both shard keys and partition keys can be composite keys based on multiple columns.\nAnother thing to keep in mind is that within a shard, the RDBMS can protect data integrity. So Facebook could configure foreign key constraints to other local tables (e.g. the local MySQL instance can guarantee that a photo has to belongs to a valid Facebook User). If Facebook were to instead shard based on Date instead of User ID, then they would not be able to provide these integrity guarantees for user items, like photos.\nSo it is important to recognize that sharding and partitioning keys are not necessarily interchangeable.\nFor data warehouse design, integrity constraints are of little concerns (since consistency should be maintained in the operational database). So in that case the only thing to consider is the analyst's query bias. For example, Facebook's data warehouse may decide to shard on Advertiser ID and partition on Advertiser ID + date.\n",(0,s.kt)("a",{parentName:"p",href:"https://www.quora.com/Whats-the-difference-between-sharding-DB-tables-and-partitioning-them"},"https://www.quora.com/Whats-the-difference-between-sharding-DB-tables-and-partitioning-them"),"\nSharding is a database architecture pattern related to",(0,s.kt)("strong",{parentName:"p"},"horizontal partitioning - the practice of separating one table's rows into multiple different tables, known as partitions.")," Each partition has the same schema and columns, but also entirely different rows. Likewise, the data held in each is unique and independent of the data held in other partitions.\nIt can be helpful to think of horizontal partitioning in terms of how it relates tovertical partitioning. ",(0,s.kt)("strong",{parentName:"p"},"In a vertically-partitioned table, entire columns are separated out and put into new, distinct tables.")," The data held within one vertical partition is independent from the data in all the others, and each holds both distinct rows and columns. The following diagram illustrates how a table could be partitioned both horizontally and vertically:"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:t(898060).Z,width:"1100",height:"926"})),(0,s.kt)("h2",{id:"logical-vs-physical-shard"},"Logical vs Physical Shard"),(0,s.kt)("p",null,"Sharding involves breaking up one's data into two or more smaller chunks, calledlogical shards. The logical shards are then distributed across separate database nodes, referred to asphysical shards, which can hold multiple logical shards. Despite this, the data held within all the shards collectively represent an entire logical dataset."),(0,s.kt)("p",null,"Shard or Partition Key is a portion of primary key which determines how data should be distributed. A partition key allows you to retrieve and modify data efficiently by routing operations to the correct database. Entries with the same partition key are stored in the same node. A ",(0,s.kt)("strong",{parentName:"p"},"logical shard"),"is a collection of data sharing the same partition key. A database node, sometimes referred as a ",(0,s.kt)("strong",{parentName:"p"},"physical shard"),", contains multiple logical shards"),(0,s.kt)("p",null,"Database shards exemplify a ",(0,s.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Shared-nothing_architecture"},"shared-nothing architecture"),". This means that the shards are autonomous; they don't share any of the same data or computing resources. In some cases, though, it may make sense to replicate certain tables into each shard to serve as reference tables. For example, let's say there's a database for an application that depends on fixed conversion rates for weight measurements. By replicating a table containing the necessary conversion rate data into each shard, it would help to ensure that all of the data required for queries is held in every shard."),(0,s.kt)("p",null,"Oftentimes, sharding is implemented at the application level, meaning that the application includes code that defines which shard to transmit reads and writes to. However, some database management systems have sharding capabilities built in, allowing you to implement sharding directly at the database level."),(0,s.kt)("h2",{id:"benefits-of-sharding"},"Benefits of Sharding"),(0,s.kt)("p",null,"The main appeal of sharding a database is that it can help to facilitatehorizontal scaling, also known asscaling out. Horizontal scaling is the practice of adding more machines to an existing stack in order to spread out the load and allow for more traffic and faster processing. This is often contrasted withvertical scaling, otherwise known asscaling up, which involves upgrading the hardware of an existing server, usually by adding more RAM or CPU.\nIt's relatively simple to have a relational database running on a single machine and scale it up as necessary by upgrading its computing resources. Ultimately, though, any non-distributed database will be limited in terms of storage and compute power, so having the freedom to scale horizontally makes your setup far more flexible.\nAnother reason why some might choose a sharded database architecture is to speed up query response times. When you submit a query on a database that hasn't been sharded, it may have to search every row in the table you're querying before it can find the result set you're looking for. For an application with a large, monolithic database, queries can become prohibitively slow. By sharding one table into multiple, though, queries have to go over fewer rows and their result sets are returned much more quickly.\nSharding can also help to make an application more reliable by mitigating the impact of outages. If your application or website relies on an unsharded database, an outage has the potential to make the entire application unavailable. With a sharded database, though, an outage is likely to affect only a single shard. Even though this might make some parts of the application or website unavailable to some users, the overall impact would still be less than if the entire database crashed."),(0,s.kt)("h2",{id:"drawbacks-of-sharding"},"Drawbacks of Sharding"),(0,s.kt)("p",null,"While sharding a database can make scaling easier and improve performance, it can also impose certain limitations. Here, we'll discuss some of these and why they might be reasons to avoid sharding altogether.\nThe first difficulty that people encounter with sharding is the sheer complexity of properly implementing a sharded database architecture. If done incorrectly, there's a significant risk that the sharding process can lead to lost data or corrupted tables. Even when done correctly, though, sharding is likely to have a major impact on your team's workflows. Rather than accessing and managing one's data from a single entry point, users must manage data across multiple shard locations, which could potentially be disruptive to some teams.\nOne problem that users sometimes encounter after having sharded a database is that the shards eventually become unbalanced. By way of example, let's say you have a database with two separate shards, one for customers whose last names begin with letters A through M and another for those whose names begin with the letters N through Z. However, your application serves an inordinate amount of people whose last names start with the letter G. Accordingly, the A-M shard gradually accrues more data than the N-Z one, causing the application to slow down and stall out for a significant portion of your users. The A-M shard has become what is known as adatabase hotspot. In this case, any benefits of sharding the database are canceled out by the slowdowns and crashes. The database would likely need to be repaired and resharded to allow for a more even data distribution.\nAnother major drawback is that once a database has been sharded, it can be very difficult to return it to its unsharded architecture. Any backups of the database made before it was sharded won't include data written since the partitioning. Consequently, rebuilding the original unsharded architecture would require merging the new partitioned data with the old backups or, alternatively, transforming the partitioned DB back into a single DB, both of which would be costly and time consuming endeavors.\nA final disadvantage to consider is that sharding isn't natively supported by every database engine. For instance, PostgreSQL does not include automatic sharding as a feature, although it is possible to manually shard a PostgreSQL database. There are a number of Postgres forks that do include automatic sharding, but these often trail behind the latest PostgreSQL release and lack certain other features. Some specialized database technologies - like MySQL Cluster or certain database-as-a-service products like MongoDB Atlas - do include auto-sharding as a feature, but vanilla versions of these database management systems do not. Because of this, sharding often requires a \"roll your own\" approach. This means that documentation for sharding or tips for troubleshooting problems are often difficult to find."),(0,s.kt)("h2",{id:"sharding-architectures"},"Sharding Architectures"),(0,s.kt)("h3",{id:"algorithmic-vs-dynamic-sharding"},"Algorithmic vs Dynamic Sharding"),(0,s.kt)("p",null,"In algorithmic sharding, the client can determine a given partition's database without any help. In dynamic sharding, a separate locator service tracks the partitions amongst the nodes.\n",(0,s.kt)("img",{alt:"image",src:t(676585).Z,width:"1101",height:"364"})),(0,s.kt)("p",null,"An algorithmically sharded database, with a simple sharding function\n",(0,s.kt)("img",{alt:"image",src:t(780015).Z,width:"1556",height:"547"})),(0,s.kt)("p",null,"A dynamic sharding scheme using range based partitioning."),(0,s.kt)("h2",{id:"entity-groups"},"Entity Groups"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:t(196891).Z,width:"1556",height:"489"})),(0,s.kt)("p",null,"Entity Groups partitions all related tables together\nThe concept of entity groups is very simple. Store related entities in the same partition to provide additional capabilities within a single partition. Specifically:"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"Queries within a single physical shard are efficient."),(0,s.kt)("li",{parentName:"ol"},"Stronger consistency semantics can be achieved within a shard.\n",(0,s.kt)("a",{parentName:"li",href:"https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6"},"https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6"))),(0,s.kt)("h3",{id:"keyhash-based-sharding"},"Key/Hash based sharding"),(0,s.kt)("p",null,"Key based sharding, also known ashash based sharding, involves using a value taken from newly written data - such as a customer's ID number, a client application's IP address, a ZIP code, etc. --- and plugging it into ahash functionto determine which shard the data should go to. A hash function is a function that takes as input a piece of data (for example, a customer email) and outputs a discrete value, known as ahash value. In the case of sharding, the hash value is a shard ID used to determine which shard the incoming data will be stored on. Altogether, the process looks like this:"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:t(309168).Z,width:"1099",height:"797"})),(0,s.kt)("p",null,"To ensure that entries are placed in the correct shards and in a consistent manner, the values entered into the hash function should all come from the same column. This column is known as ashard key. In simple terms, shard keys are similar to ",(0,s.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Primary_key"},"primary keys")," in that both are columns which are used to establish a unique identifier for individual rows. Broadly speaking, a shard key should be static, meaning it shouldn't contain values that might change over time. Otherwise, it would increase the amount of work that goes into update operations, and could slow down performance.\nWhile key based sharding is a fairly common sharding architecture, it can make things tricky when trying to dynamically add or remove additional servers to a database. As you add servers, each one will need a corresponding hash value and many of your existing entries, if not all of them, will need to be remapped to their new, correct hash value and then migrated to the appropriate server. As you begin rebalancing the data, neither the new nor the old hashing functions will be valid. Consequently, your server won't be able to write any new data during the migration and your application could be subject to downtime.\nThe main appeal of this strategy is that it can be used to evenly distribute data so as to prevent hotspots. Also, because it distributes data algorithmically, there's no need to maintain a map of where all the data is located, as is necessary with other strategies like range or directory based sharding.\nHash-based sharding processes keys using a hash function and then uses the results to get the sharding ID, as shown in Figure 3 (source:",(0,s.kt)("a",{parentName:"p",href:"https://docs.mongodb.com/manual/core/hashed-sharding/"},"MongoDB uses hash-based sharding to partition data"),").\nContrary to range-based sharding, where all keys can be put in order, hash-based sharding has the advantage that keys are distributed almost randomly, so the distribution is even. As a result, it is more friendly to systems with heavy write workloads and read workloads that are almost all random. This is because the write pressure can be evenly distributed in the cluster. But apparently, operations likerange scanare almost impossible."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:t(869201).Z,width:"1000",height:"319"})),(0,s.kt)("p",null,"Some typical examples of hash-based sharding are ",(0,s.kt)("a",{parentName:"p",href:"https://docs.datastax.com/en/archived/cassandra/2.1/cassandra/architecture/architectureDataDistributeHashing_c.html"},"Cassandra Consistent hashing"),", presharding of Redis Cluster and ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/CodisLabs/codis"},"Codis"),", and ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/twitter/twemproxy/blob/master/README#features"},"Twemproxy consistent hashing"),"."),(0,s.kt)("h3",{id:"range-based-sharding"},"Range based sharding"),(0,s.kt)("p",null,"Range based sharding involves sharding data based on ranges of a given value. To illustrate, let's say you have a database that stores information about all the products within a retailer's catalog. You could create a few different shards and divvy up each products' information based on which price range they fall into, like this:"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:t(899531).Z,width:"1101",height:"694"})),(0,s.kt)("p",null,"The main benefit of range based sharding is that it's relatively simple to implement. Every shard holds a different set of data but they all have an identical schema as one another, as well as the original database. The application code just reads which range the data falls into and writes it to the corresponding shard.\nOn the other hand, range based sharding doesn't protect data from being unevenly distributed, leading to the aforementioned database hotspots. Looking at the example diagram, even if each shard holds an equal amount of data the odds are that specific products will receive more attention than others. Their respective shards will, in turn, receive a disproportionate number of reads.\nRange-based sharding assumes that all keys in the database system can be put in order, and it takes a continuous section of keys as a sharding unit.\nIt's very common to sort keys in order. HBase keys are sorted in byte order, while MySQL keys are sorted in auto-increment ID order. For some storage engines, the order is natural. In the case of both log-structured merge-tree (LSM-Tree) and B-Tree, keys are naturally in order."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:t(801201).Z,width:"1000",height:"317"}),"\nIn Figure 2 (source:",(0,s.kt)("a",{parentName:"p",href:"https://docs.mongodb.com/manual/core/ranged-sharding/"},"MongoDB uses range-based sharding to partition data"),"), the key space is divided into (minKey, maxKey). Each sharding unit (chunk) is a section of continuous keys. The advantage of range-based sharding is that the adjacent data has a high probability of being together (such as the data with a common prefix), which can well support operations likerange scan. For example, HBase Region is a typical range-based sharding strategy.\nHowever, range-based sharding is not friendly to sequential writes with heavy workloads. For example, in the time series type of write load, the write hotspot is always in the last Region. This occurs because the log key is generally related to the timestamp, and the time is monotonically increasing. But relational databases often need to executetable scan(orindex scan), and the common choice is range-based sharding."),(0,s.kt)("h3",{id:"hash-range-combination-sharding"},"Hash-Range combination sharding"),(0,s.kt)("p",null,"Note that hash-based and range-based sharding strategies are not isolated. Instead, you can flexibly combine them. For example, you can establish a multi-level sharding strategy, which uses hash in the uppermost layer, while in each hash-based sharding unit, data is stored in order."),(0,s.kt)("h3",{id:"directory-based-sharding"},"Directory based sharding"),(0,s.kt)("p",null,"To implementdirectory based sharding, one must create and maintain alookup tablethat uses a shard key to keep track of which shard holds which data. In a nutshell, a lookup table is a table that holds a static set of information about where specific data can be found. The following diagram shows a simplistic example of directory based sharding:"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"image",src:t(48979).Z,width:"1100",height:"944"})),(0,s.kt)("p",null,"Here, theDelivery Zonecolumn is defined as a shard key. Data from the shard key is written to the lookup table along with whatever shard each respective row should be written to. This is similar to range based sharding, but instead of determining which range the shard key's data falls into, each key is tied to its own specific shard. Directory based sharding is a good choice over range based sharding in cases where the shard key has a low cardinality and it doesn't make sense for a shard to store a range of keys. Note that it's also distinct from key based sharding in that it doesn't process the shard key through a hash function; it just checks the key against a lookup table to see where the data needs to be written.\nThe main appeal of directory based sharding is its flexibility. Range based sharding architectures limit you to specifying ranges of values, while key based ones limit you to using a fixed hash function which, as mentioned previously, can be exceedingly difficult to change later on. Directory based sharding, on the other hand, allows you to use whatever system or algorithm you want to assign data entries to shards, and it's relatively easy to dynamically add shards using this approach.\nWhile directory based sharding is the most flexible of the sharding methods discussed here, the need to connect to the lookup table before every query or write can have a detrimental impact on an application's performance. Furthermore, the lookup table can become a single point of failure: if it becomes corrupted or otherwise fails, it can impact one's ability to write new data or access their existing data."),(0,s.kt)("h3",{id:"others"},"Others"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},"Initial Implementation in Cassandra -- Linear Hash Sharding")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},"DynamoDB and Cassandra -- Consistent Hash Sharding")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},"Google Spanner and HBase -- Range Sharding\n",(0,s.kt)("a",{parentName:"p",href:"https://blog.yugabyte.com/four-data-sharding-strategies-we-analyzed-in-building-a-distributed-sql-database"},"https://blog.yugabyte.com/four-data-sharding-strategies-we-analyzed-in-building-a-distributed-sql-database")))),(0,s.kt)("h2",{id:"should-i-shard"},"Should I Shard?"),(0,s.kt)("p",null,"Whether or not one should implement a sharded database architecture is almost always a matter of debate. Some see sharding as an inevitable outcome for databases that reach a certain size, while others see it as a headache that should be avoided unless it's absolutely necessary, due to the operational complexity that sharding adds.\nBecause of this added complexity, sharding is usually only performed when dealing with very large amounts of data. Here are some common scenarios where it may be beneficial to shard a database:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"The amount of application data grows to exceed the storage capacity of a single database node."),(0,s.kt)("li",{parentName:"ul"},"The volume of writes or reads to the database surpasses what a single node or its read replicas can handle, resulting in slowed response times or timeouts."),(0,s.kt)("li",{parentName:"ul"},"The network bandwidth required by the application outpaces the bandwidth available to a single database node and any read replicas, resulting in slowed response times or timeouts.\nBefore sharding, you should exhaust all other options for optimizing your database. Some optimizations you might want to consider include:"),(0,s.kt)("li",{parentName:"ul"},"Setting up a remote database. If you're working with a monolithic application in which all of its components reside on the same server, you can improve your database's performance by moving it over to its own machine. This doesn't add as much complexity as sharding since the database's tables remain intact. However, it still allows you to vertically scale your database apart from the rest of your infrastructure."),(0,s.kt)("li",{parentName:"ul"},"Implementing ",(0,s.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Database_caching"},"caching"),". If your application's read performance is what's causing you trouble, caching is one strategy that can help to improve it. Caching involves temporarily storing data that has already been requested in memory, allowing you to access it much more quickly later on."),(0,s.kt)("li",{parentName:"ul"},"Creating one or more read replicas. Another strategy that can help to improve read performance, this involves copying the data from one database server (theprimary server) over to one or moresecondary servers. Following this, every new write goes to the primary before being copied over to the secondaries, while reads are made exclusively to the secondary servers. Distributing reads and writes like this keeps any one machine from taking on too much of the load, helping to prevent slowdowns and crashes. Note that creating read replicas involves more computing resources and thus costs more money, which could be a significant constraint for some."),(0,s.kt)("li",{parentName:"ul"},"Upgrading to a larger server. In most cases, scaling up one's database server to a machine with more resources requires less effort than sharding. As with creating read replicas, an upgraded server with more resources will likely cost more money. Accordingly, you should only go through with resizing if it truly ends up being your best option.\n",(0,s.kt)("a",{parentName:"li",href:"https://www.digitalocean.com/community/tutorials/understanding-database-sharding"},"https://www.digitalocean.com/community/tutorials/understanding-database-sharding"))),(0,s.kt)("p",null,"High Scalability - ",(0,s.kt)("a",{parentName:"p",href:"http://highscalability.com/unorthodox-approach-database-design-coming-shard"},"http://highscalability.com/unorthodox-approach-database-design-coming-shard")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://docs.oracle.com/cd/B19306_01/server.102/b14220/partconc.htm"},"https://docs.oracle.com/cd/B19306_01/server.102/b14220/partconc.htm"),"\n",(0,s.kt)("a",{parentName:"p",href:"https://dev.mysql.com/doc/refman/5.7/en/partitioning-overview.html"},"https://dev.mysql.com/doc/refman/5.7/en/partitioning-overview.html"),"\n",(0,s.kt)("a",{parentName:"p",href:"https://www.vertabelo.com/blog/everything-you-need-to-know-about-mysql-partitions"},"https://www.vertabelo.com/blog/everything-you-need-to-know-about-mysql-partitions")),(0,s.kt)("p",null,'MySQL partitioning is about altering -- ideally, optimizing -- the way the database engine physically stores data. It allows you to distribute portions of table data (a.k.a. partitions) across the file system based on a set of user-defined rules (a.k.a. the "partitioning function"). In this way, if the queries you perform access only a fraction of table data and the partitioning function is properly set, there will be less to scan and queries will be faster.\nIt is important to note that partitioning makes the most sense when dealing with large data sets. If you have fewer than a million rows or only thousands of records, partitioning will not make a difference.'),(0,s.kt)("h2",{id:"advantages"},"Advantages"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Storage:"),"It is possible to store more data in one table than can be held on a single disk or file system partition."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Deletion:"),"Dropping a useless partition is almost instantaneous, but a classicalDELETEquery run in a very large table could take minutes."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Partition Pruning:"),"This is the ability to exclude non-matching partitions and their data from a search; it makes querying faster. Also, MySQL 5.7 supports explicit partition selection in queries, which greatly increases the search speed. (Obviously, this only works if you know in advance which partitions you want to use.) This also applies forDELETE,INSERT,REPLACE, andUPDATEstatements as well asLOAD DATAandLOAD XML.")),(0,s.kt)("h2",{id:"partition-types"},"Partition Types"),(0,s.kt)("p",null,"Four partition types available:RANGE,LIST,HASHandKEY"))}g.isMDXComponent=!0},898060:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image1-0b0992ed079738d98ca6307a69c8e6f6.jpg"},676585:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image2-b329721fa62602f72727570d5920d3ef.jpg"},780015:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image3-d8115a976a64cad3746f94d19bcd4b3d.jpg"},196891:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image4-0d48e6a4d120f85a549814153d732a6f.jpg"},309168:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image5-023f5466739ca9b2463bfbeefce2b0ed.jpg"},869201:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image6-d70440cd0d110032302b2f369281ec4a.jpg"},899531:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image7-83f3ac9a45a95adbbea202d8938f05cd.jpg"},801201:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image8-c9e03e43ec02ca240ecc508f08ddd6f9.jpg"},48979:(e,a,t)=>{t.d(a,{Z:()=>i});const i=t.p+"assets/images/Partitioning-Sharding-image9-e4bf0e8e0c4ed1dd04742b6a77b61967.jpg"}}]);