"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[3696],{603905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>m});var o=a(667294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},r=Object.keys(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var c=o.createContext({}),l=function(e){var t=o.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=l(e.components);return o.createElement(c.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=l(a),h=n,m=d["".concat(c,".").concat(h)]||d[h]||u[h]||r;return a?o.createElement(m,i(i({ref:t},p),{},{components:a})):o.createElement(m,i({ref:t},p))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,i=new Array(r);i[0]=h;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[d]="string"==typeof e?e:n,i[1]=s;for(var l=2;l<r;l++)i[l]=a[l];return o.createElement.apply(null,i)}return o.createElement.apply(null,a)}h.displayName="MDXCreateElement"},689664:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var o=a(487462),n=(a(667294),a(603905));const r={},i="Intro",s={unversionedId:"technologies/apache/apache-hadoop/intro",id:"technologies/apache/apache-hadoop/intro",title:"Intro",description:"Apache Hadoop is a processing framework that exclusively provides batch processing. Hadoop was the first big data framework to gain significant traction in the open-source community. Based on several papers and presentations by Google about how they were dealing with tremendous amounts of data at the time, Hadoop reimplemented the algorithms and component stack to make large scale batch processing more accessible.",source:"@site/docs/technologies/apache/apache-hadoop/intro.md",sourceDirName:"technologies/apache/apache-hadoop",slug:"/technologies/apache/apache-hadoop/intro",permalink:"/technologies/apache/apache-hadoop/intro",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/technologies/apache/apache-hadoop/intro.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"HDFS",permalink:"/technologies/apache/apache-hadoop/hdfs"},next:{title:"MapReduce Examples",permalink:"/technologies/apache/apache-hadoop/mapreduce-examples"}},c={},l=[{value:"Batch Processing Model",id:"batch-processing-model",level:2},{value:"Advantages and Limitations",id:"advantages-and-limitations",level:2},{value:"Summary",id:"summary",level:2}],p={toc:l},d="wrapper";function u(e){let{components:t,...a}=e;return(0,n.kt)(d,(0,o.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"intro"},"Intro"),(0,n.kt)("p",null,"Apache Hadoop is a processing framework that exclusively provides batch processing. Hadoop was the first big data framework to gain significant traction in the open-source community. Based on several papers and presentations by Google about how they were dealing with tremendous amounts of data at the time, Hadoop reimplemented the algorithms and component stack to make large scale batch processing more accessible."),(0,n.kt)("p",null,"Modern versions of Hadoop are composed of several components or layers, that work together to process batch data:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"HDFS: HDFS is the distributed filesystem layer that coordinates storage and replication across the cluster nodes. HDFS ensures that data remains available in spite of inevitable host failures. It is used as the source of data, to store intermediate processing results, and to persist the final calculated results."),(0,n.kt)("li",{parentName:"ul"},"YARN: YARN, which stands for Yet Another Resource Negotiator, is the cluster coordinating component of the Hadoop stack. It is responsible for coordinating and managing the underlying resources and scheduling jobs to be run. YARN makes it possible to run much more diverse workloads on a Hadoop cluster than was possible in earlier iterations by acting as an interface to the cluster resources."),(0,n.kt)("li",{parentName:"ul"},"MapReduce: MapReduce is Hadoop's native batch processing engine.")),(0,n.kt)("h2",{id:"batch-processing-model"},"Batch Processing Model"),(0,n.kt)("p",null,"The processing functionality of Hadoop comes from the MapReduce engine. MapReduce's processing technique follows the map, shuffle, reduce algorithm using key-value pairs. The basic procedure involves:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Reading the dataset from the HDFS filesystem"),(0,n.kt)("li",{parentName:"ul"},"Dividing the dataset into chunks and distributed among the available nodes"),(0,n.kt)("li",{parentName:"ul"},"Applying the computation on each node to the subset of data (the intermediate results are written back to HDFS)"),(0,n.kt)("li",{parentName:"ul"},"Redistributing the intermediate results to group by key"),(0,n.kt)("li",{parentName:"ul"},'"Reducing" the value of each key by summarizing and combining the results calculated by the individual nodes'),(0,n.kt)("li",{parentName:"ul"},"Write the calculated final results back to HDFS")),(0,n.kt)("h2",{id:"advantages-and-limitations"},"Advantages and Limitations"),(0,n.kt)("p",null,"Because this methodology heavily leverages permanent storage, reading and writing multiple times per task, it tends to be fairly slow. On the other hand, since disk space is typically one of the most abundant server resources, it means that MapReduce can handle enormous datasets. This also means that Hadoop's MapReduce can typically run on less expensive hardware than some alternatives since it does not attempt to store everything in memory. MapReduce has incredible scalability potential and has been used in production on tens of thousands of nodes."),(0,n.kt)("p",null,"As a target for development, MapReduce is known for having a rather steep learning curve. Other additions to the Hadoop ecosystem can reduce the impact of this to varying degrees, but it can still be a factor in quickly implementing an idea on a Hadoop cluster."),(0,n.kt)("p",null,"Hadoop has an extensive ecosystem, with the Hadoop cluster itself frequently used as a building block for other software. Many other processing frameworks and engines have Hadoop integrations to utilize HDFS and the YARN resource manager."),(0,n.kt)("h2",{id:"summary"},"Summary"),(0,n.kt)("p",null,"Apache Hadoop and its MapReduce processing engine offer a well-tested batch processing model that is best suited for handling very large data sets where time is not a significant factor. The low cost of components necessary for a well-functioning Hadoop cluster makes this processing inexpensive and effective for many use cases. Compatibility and integration with other frameworks and engines mean that Hadoop can often serve as the foundation for multiple processing workloads using diverse technology."))}u.isMDXComponent=!0}}]);