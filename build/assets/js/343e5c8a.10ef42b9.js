"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[21765],{603905:(e,t,i)=>{i.d(t,{Zo:()=>l,kt:()=>u});var a=i(667294);function n(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function r(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,a)}return i}function o(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?r(Object(i),!0).forEach((function(t){n(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function s(e,t){if(null==e)return{};var i,a,n=function(e,t){if(null==e)return{};var i,a,n={},r=Object.keys(e);for(a=0;a<r.length;a++)i=r[a],t.indexOf(i)>=0||(n[i]=e[i]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)i=r[a],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(n[i]=e[i])}return n}var p=a.createContext({}),c=function(e){var t=a.useContext(p),i=t;return e&&(i="function"==typeof e?e(t):o(o({},t),e)),i},l=function(e){var t=c(e.components);return a.createElement(p.Provider,{value:t},e.children)},h="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var i=e.components,n=e.mdxType,r=e.originalType,p=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),h=c(i),d=n,u=h["".concat(p,".").concat(d)]||h[d]||m[d]||r;return i?a.createElement(u,o(o({ref:t},l),{},{components:i})):a.createElement(u,o({ref:t},l))}));function u(e,t){var i=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=i.length,o=new Array(r);o[0]=d;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[h]="string"==typeof e?e:n,o[1]=s;for(var c=2;c<r;c++)o[c]=i[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,i)}d.displayName="MDXCreateElement"},68843:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=i(487462),n=(i(667294),i(603905));const r={},o="Computer Vision / CV",s={unversionedId:"ai/computer-vision-cv/intro",id:"ai/computer-vision-cv/intro",title:"Computer Vision / CV",description:"13.1. Image Augmentation",source:"@site/docs/ai/computer-vision-cv/intro.md",sourceDirName:"ai/computer-vision-cv",slug:"/ai/computer-vision-cv/intro",permalink:"/ai/computer-vision-cv/intro",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/ai/computer-vision-cv/intro.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Image Formats",permalink:"/ai/computer-vision-cv/image-formats"},next:{title:"MNIST For ML Beginners | TensorFlow",permalink:"/ai/computer-vision-cv/mnist-for-ml-beginners-tensorflow"}},p={},c=[{value:"Face Detection Concepts",id:"face-detection-concepts",level:2},{value:"Cases",id:"cases",level:3},{value:"Image Gradient",id:"image-gradient",level:2},{value:"Hough Transform",id:"hough-transform",level:2},{value:"Canny Edge Detection",id:"canny-edge-detection",level:2},{value:"Peak Signal to Noise Ratio (PSNR)",id:"peak-signal-to-noise-ratio-psnr",level:2},{value:"References",id:"references",level:2}],l={toc:c},h="wrapper";function m(e){let{components:t,...r}=e;return(0,n.kt)(h,(0,a.Z)({},l,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"computer-vision--cv"},"Computer Vision / CV"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/image-augmentation.html"},"13.1. Image Augmentation")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/fine-tuning.html"},"13.2. Fine-Tuning")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/bounding-box.html"},"13.3. Object Detection and Bounding Boxes")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/anchor.html"},"13.4. Anchor Boxes")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/multiscale-object-detection.html"},"13.5. Multiscale Object Detection")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/object-detection-dataset.html"},"13.6. The Object Detection Dataset")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/ssd.html"},"13.7. Single Shot Multibox Detection")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/rcnn.html"},"13.8. Region-based CNNs (R-CNNs)")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html"},"13.9. Semantic Segmentation and the Dataset")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/transposed-conv.html"},"13.10. Transposed Convolution")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/fcn.html"},"13.11. Fully Convolutional Networks (FCN)")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/neural-style.html"},"13.12. Neural Style Transfer")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/kaggle-cifar10.html"},"13.13. Image Classification (CIFAR-10) on Kaggle")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/kaggle-dog.html"},"13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://d2l.ai/chapter_computer-vision/index.html"},"https://d2l.ai/chapter_computer-vision/index.html")),(0,n.kt)("p",null,"Computer vision involves analyzing patterns in visual images and reconstructing the real-world objects that produced them. The process is often broken up into two phases:",(0,n.kt)("strong",{parentName:"p"},"feature detectionandpattern recognition.")," Feature detection involves selecting important features of the image; pattern recognition involves discovering patterns in the features."),(0,n.kt)("p",null,"Content-Based Image Retrieval (CBIR) is the process of building image search engines"),(0,n.kt)("p",null,"ANPR - Automatic Number Plate Recognition"),(0,n.kt)("h2",{id:"face-detection-concepts"},"Face Detection Concepts"),(0,n.kt)("p",null,"Face detection locates human faces in visual media such as digital images or video. When a face is detected it has an associated position, size, and orientation; and it can be searched for landmarks such as the eyes and nose."),(0,n.kt)("p",null,"Here are some of the terms that we use regarding the face detection feature of ML Kit:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Face tracking extends face detection to video sequences. Any face that appears in a video for any length of time can be tracked from frame to frame. This means a face detected in consecutive video frames can be identified as being the same person. Note that this isn't a form offace recognition; face tracking only makes inferences based on the position and motion of the faces in a video sequence."),(0,n.kt)("li",{parentName:"ul"},"A landmark is a point of interest within a face. The left eye, right eye, and base of the nose are all examples of landmarks. ML Kit provides the ability to find landmarks on a detected face."),(0,n.kt)("li",{parentName:"ul"},"Acontouris a set of points that follow the shape of a facial feature. ML Kit provides the ability to find the contours of a face."),(0,n.kt)("li",{parentName:"ul"},"Classification determines whether a certain facial characteristic is present. For example, a face can be classified by whether its eyes are open or closed, or if the face is smiling or not.")),(0,n.kt)("h3",{id:"cases"},"Cases"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Perfect Light"),(0,n.kt)("li",{parentName:"ul"},"Dull light"),(0,n.kt)("li",{parentName:"ul"},"BW"),(0,n.kt)("li",{parentName:"ul"},"Half face"),(0,n.kt)("li",{parentName:"ul"},"Atoneside"),(0,n.kt)("li",{parentName:"ul"},"Side face"),(0,n.kt)("li",{parentName:"ul"},"Multiple face dull")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://developers.google.com/ml-kit/vision/face-detection/face-detection-concepts"},"https://developers.google.com/ml-kit/vision/face-detection/face-detection-concepts")),(0,n.kt)("h2",{id:"image-gradient"},"Image Gradient"),(0,n.kt)("p",null,"Animage gradientis a directional change in the intensity or color in an image. The gradient of the image is one of the fundamental building blocks in ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Image_processing"},"image processing"),". For example, the ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Canny_edge_detector"},"Canny edge detector")," uses image gradient for ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Edge_detection"},"edge detection"),". In ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Graphics_software"},"graphics software")," for ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Digital_image_editing"},"digital image editing"),", the term gradient or ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Color_gradient"},"color gradient")," is also used for a gradual blend of ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Color"},"color")," which can be considered as an even ",(0,n.kt)("a",{parentName:"p",href:"https://en.wiktionary.org/wiki/gradation"},"gradation")," from low to high values, as used from white to black in the images to the right. Another name for this iscolor progression."),(0,n.kt)("p",null,"Mathematically, the ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Gradient"},"gradient")," of a two-variable function (here the image intensity function) at each image point is a 2D ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Vector_(geometric)"},"vector")," with the components given by the ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Derivative"},"derivatives")," in the horizontal and vertical directions. At each image point, the gradient vector points in the direction of largest possible intensity increase, and the length of the gradient vector corresponds to the rate of change in that direction."),(0,n.kt)("p",null,"Since the intensity function of a digital image is only known at discrete points, derivatives of this function cannot be defined unless we assume that there is an underlying ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Continuous_function"},"continuous")," intensity function which has been sampled at the image points. With some additional assumptions, the derivative of the continuous intensity function can be computed as a function on the sampled intensity function, i.e., the digital image. Approximations of these derivative functions can be defined at varying degrees of accuracy. The most common way to approximate the image gradient is to ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Convolution"},"convolve")," an image with a kernel, such as the ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Sobel_operator"},"Sobel operator")," or ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Prewitt_operator"},"Prewitt operator"),"."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:i(880819).Z,width:"400",height:"135"})),(0,n.kt)("p",null,"On the left, an intensity image of a cat. In the center, a gradient image in the x direction measuring horizontal change in intensity. On the right, a gradient image in the y direction measuring vertical change in intensity. Gray pixels have a small gradient; black or white pixels have a large gradient."),(0,n.kt)("p",null,"See Also -"),(0,n.kt)("p",null,"Mathematics > Calculus > Gradient"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Image_gradient"},"https://en.wikipedia.org/wiki/Image_gradient")),(0,n.kt)("h2",{id:"hough-transform"},"Hough Transform"),(0,n.kt)("p",null,"The ",(0,n.kt)("strong",{parentName:"p"},"Hough transform")," is a ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Feature_extraction"},"feature extraction")," technique used in ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Image_analysis"},"image analysis"),", ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Computer_vision"},"computer vision"),", and ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Digital_image_processing"},"digital image processing"),".The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure. This voting procedure is carried out in a ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Parameter_space"},"parameter space"),", from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Hough_transform"},"https://en.wikipedia.org/wiki/Hough_transform")),(0,n.kt)("h2",{id:"canny-edge-detection"},"Canny Edge Detection"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html"},"https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html")),(0,n.kt)("h2",{id:"peak-signal-to-noise-ratio-psnr"},"Peak Signal to Noise Ratio (PSNR)"),(0,n.kt)("p",null,"Peak signal-to-noise ratio(PSNR) is an engineering term for the ratio between the maximum possible power of a ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Signal_(information_theory)"},"signal")," and the power of corrupting ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Noise"},"noise")," that affects the fidelity of its representation. Because many signals have a very wide ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Dynamic_range"},"dynamic range"),", PSNR is usually expressed as a ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Logarithm"},"logarithmic")," quantity using the ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Decibel"},"decibel")," scale."),(0,n.kt)("p",null,"PSNR is commonly used to quantify reconstruction quality for images and video subject to ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Lossy_compression"},"lossy compression"),"."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"},"https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio")),(0,n.kt)("h2",{id:"references"},"References"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"../../computer-science/courses/self-driving-nanodegree"},"Self Driving Nanodegree")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab"},"https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://www.freecodecamp.org/news/advanced-computer-vision-with-python"},"https://www.freecodecamp.org/news/advanced-computer-vision-with-python")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://maxhalford.github.io/blog/comic-book-panel-segmentation/"},"Comic book panel segmentation \u2022 Max Halford")))}m.isMDXComponent=!0},880819:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/Computer-Vision-CV-image1-92ab8934d6a8d156ec6225c30fc3d5e8.jpg"}}]);