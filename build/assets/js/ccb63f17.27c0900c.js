"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[34624],{603905:(e,n,t)=>{t.d(n,{Zo:()=>f,kt:()=>m});var o=t(667294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=o.createContext({}),c=function(e){var n=o.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},f=function(e){var n=c(e.components);return o.createElement(l.Provider,{value:n},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},k=o.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,f=s(e,["components","mdxType","originalType","parentName"]),u=c(t),k=r,m=u["".concat(l,".").concat(k)]||u[k]||p[k]||a;return t?o.createElement(m,i(i({ref:n},f),{},{components:t})):o.createElement(m,i({ref:n},f))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,i=new Array(a);i[0]=k;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<a;c++)i[c]=t[c];return o.createElement.apply(null,i)}return o.createElement.apply(null,t)}k.displayName="MDXCreateElement"},603605:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var o=t(487462),r=(t(667294),t(603905));const a={},i="confluent-kafka",s={unversionedId:"technologies/kafka/confluent-kafka",id:"technologies/kafka/confluent-kafka",title:"confluent-kafka",description:"Confluent kafka-python",source:"@site/docs/technologies/kafka/confluent-kafka.md",sourceDirName:"technologies/kafka",slug:"/technologies/kafka/confluent-kafka",permalink:"/technologies/kafka/confluent-kafka",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/technologies/kafka/confluent-kafka.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Kafka",permalink:"/technologies/kafka/"},next:{title:"Installing Kafka",permalink:"/technologies/kafka/installing-kafka"}},l={},c=[{value:"Confluent kafka-python",id:"confluent-kafka-python",level:2},{value:"Consumer",id:"consumer",level:2},{value:"Producer",id:"producer",level:2},{value:"Resources",id:"resources",level:2}],f={toc:c},u="wrapper";function p(e){let{components:n,...t}=e;return(0,r.kt)(u,(0,o.Z)({},f,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"confluent-kafka"},"confluent-kafka"),(0,r.kt)("h2",{id:"confluent-kafka-python"},"Confluent kafka-python"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'pip install confluent-kafka\npip install "confluent-kafka [avro]"\n')),(0,r.kt)("h2",{id:"consumer"},"Consumer"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from confluent_kafka import Consumer, KafkaError\n\nconsumer_config = {\n    'bootstrap.servers': 'my-cluster-kafka-brokers.kafka:9092',\n    'partition.assignment.strategy': 'roundrobin',\n    'group.id': 'test_bank_data_consumer',\n    'auto.offset.reset': 'earliest',  # earliest/latest\n    'enable.auto.commit': 'false',\n    # for limiting the amount of messages pre-fetched by librdkafka\n    'queued.max.messages.kbytes': '32000',\n    'fetch.message.max.bytes': '15728640',\n}\n\nc = Consumer(consumer_config)\n\n# callbacks\ndef print_on_assign(consumer, partitions):\n    logging.info(f'Assignment: {partitions}')\n\n    for partition in partitions:\n        logging.info(f'watermark: {c.get_watermark_offsets(partition=partition)}')\n\n    logging.info(f'committed offsets for all partitions: {c.committed(partitions=partitions)}')\n\n    logging.info(f'position: {c.position(partitions=partitions)}')\n\ndef print_on_revoke(consumer, partitions):\n    logging.info(f'Revoke Assignment: {partitions}')\n\nc.subscribe(['bank_data'], on_assign=print_on_assign, on_revoke=print_on_revoke)\n\ntimeout_seconds = 1\n\nwhile True:\n    msg = c.poll(1.0)\n\n    # initial error handling\n    if msg is None:\n        continue\n\n    if msg.error():\n        if msg.error().code() == KafkaError._PARTITION_EOF:\n            continue\n        else:\n            logging.error(f'druid consumer error: {msg.error()}')\n            break\n\n    logging.debug(f'{msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n\n    try:\n        # get value from message and convert bytes\n        final_data = msg.value()\n        final_data = json.loads(final_data.decode('utf-8'))\n        c.commit()\n\n    except Exception as e:\n        try:\n            logging.error(f'data/msg: {msg.value()}')\n        except Exception:\n            logging.exception(f'cannot print data')\n        logging.exception(\n            f'global exception occurred, Will not attempt for another {timeout_seconds} seconds.')\n    else:\n        continue\n\n    # exponential back-off if exception occurred\n    time.sleep(timeout_seconds)\n    timeout_seconds *= 2\n")),(0,r.kt)("h2",{id:"producer"},"Producer"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from confluent_kafka import Producer\n\np = Producer({\n    'bootstrap.servers': 'my-cluster-kafka-brokers.kafka:9092',\n    'queue.buffering.max.messages': '1000000',\n    'queue.buffering.max.kbytes': '1048576',\n    'message.max.bytes': '15728640',\n    'delivery.timeout.ms': '10000',\n    'request.timeout.ms': '5000'\n})\n\ndef delivery_report(err, msg):\n    \"\"\" Called once for each message produced to indicate delivery result.\n        Triggered by poll() or flush(). \"\"\"\n    if err is not None:\n        # raise error and handle using exception\n        logging.exception(f'kafka deliver_report error: {err}')\n    else:\n        logging.debug(f'Message delivered topic: {msg.topic()} partition: {msg.partition()} offset: {msg.offset()}')\n\np.produce('bank_data', json.dumps(payload), callback=delivery_report)\np.flush()\n")),(0,r.kt)("h2",{id:"resources"},"Resources"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/confluentinc/confluent-kafka-python"},"https://github.com/confluentinc/confluent-kafka-python")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.confluent.io/current/clients/confluent-kafka-python"},"https://docs.confluent.io/current/clients/confluent-kafka-python")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION"},"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/3-libraries-you-should-know-to-master-apache-kafka-in-python-c95fdf8700f2"},"https://towardsdatascience.com/3-libraries-you-should-know-to-master-apache-kafka-in-python-c95fdf8700f2")))}p.isMDXComponent=!0}}]);