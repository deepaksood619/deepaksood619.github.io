"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[94535],{603905:(e,t,a)=>{a.d(t,{Zo:()=>h,kt:()=>u});var r=a(667294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=r.createContext({}),c=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},h=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),p=c(a),m=n,u=p["".concat(l,".").concat(m)]||p[m]||d[m]||i;return a?r.createElement(u,o(o({ref:t},h),{},{components:a})):r.createElement(u,o({ref:t},h))}));function u(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:n,o[1]=s;for(var c=2;c<i;c++)o[c]=a[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},93033:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=a(487462),n=(a(667294),a(603905));const i={},o="Concurrency / Threading",s={unversionedId:"computer-science/operating-system/concurrency-threading",id:"computer-science/operating-system/concurrency-threading",title:"Concurrency / Threading",description:"Concurrency",source:"@site/docs/computer-science/operating-system/concurrency-threading.md",sourceDirName:"computer-science/operating-system",slug:"/computer-science/operating-system/concurrency-threading",permalink:"/computer-science/operating-system/concurrency-threading",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/computer-science/operating-system/concurrency-threading.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Concurrency Problems",permalink:"/computer-science/operating-system/concurrency-problems"},next:{title:"Coroutines",permalink:"/computer-science/operating-system/coroutines"}},l={},c=[{value:"Concurrency",id:"concurrency",level:2},{value:"Process",id:"process",level:2},{value:"Threads",id:"threads",level:2},{value:"Advantages of threads",id:"advantages-of-threads",level:2},{value:"Threads are implemented in the following two ways",id:"threads-are-implemented-in-the-following-two-ways",level:2},{value:"User Level Threads",id:"user-level-threads",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Kernel Level Threads",id:"kernel-level-threads",level:2},{value:"Advantages",id:"advantages-1",level:2},{value:"Disadvantages",id:"disadvantages-1",level:2},{value:"Threads vs Processes",id:"threads-vs-processes",level:2},{value:"Are Threads Lighter than Processes",id:"are-threads-lighter-than-processes",level:2},{value:"What can be improved in Threads?",id:"what-can-be-improved-in-threads",level:2},{value:"Threads vs Async",id:"threads-vs-async",level:2},{value:"Threads",id:"threads-1",level:2},{value:"Async",id:"async",level:2},{value:"Comparison",id:"comparison",level:2},{value:"Considerations",id:"considerations",level:2},{value:"Amdahl&#39;s Law",id:"amdahls-law",level:2},{value:"Amdahl&#39;s law is often used in parallel computing to predict the theoretical speedup when using multiple processors. For example, if a program needs 20 hours using a single processor core, and a particular part of the program which takes one hour to execute cannot be parallelized, while the remaining 19 hours (p = 0.95) of execution time can be parallelized, then regardless of how many processors are devoted to a parallelized execution of this program, the minimum execution time cannot be less than that critical one hour. Hence, the theoretical speedup is limited to at most 20 times(1/(1\u2212p)=20). For this reason parallel computing is relevant only for a low number of processors and very parallelizable programs",id:"amdahls-law-is-often-used-in-parallel-computing-to-predict-the-theoretical-speedup-when-using-multiple-processors-for-example-if-a-program-needs-20-hours-using-a-single-processor-core-and-a-particular-part-of-the-program-which-takes-one-hour-to-execute-cannot-be-parallelized-while-the-remaining-19-hours-p--095-of-execution-time-can-be-parallelized-then-regardless-of-how-many-processors-are-devoted-to-a-parallelized-execution-of-this-program-the-minimum-execution-time-cannot-be-less-than-that-critical-one-hour-hence-the-theoretical-speedup-is-limited-to-at-most-20-times11p20-for-this-reason-parallel-computing-is-relevant-only-for-a-low-number-of-processors-and-very-parallelizable-programs",level:2},{value:"Process Control Block",id:"process-control-block",level:2},{value:"Synchronization",id:"synchronization",level:2},{value:"Need for synchronization",id:"need-for-synchronization",level:2},{value:"Implementation of Synchronization",id:"implementation-of-synchronization",level:2},{value:"Hyper - Threading Technology (HTT)",id:"hyper---threading-technology-htt",level:2},{value:"Compare and Swap (CAS)",id:"compare-and-swap-cas",level:2},{value:"See also",id:"see-also",level:2},{value:"References",id:"references",level:2}],h={toc:c},p="wrapper";function d(e){let{components:t,...i}=e;return(0,n.kt)(p,(0,r.Z)({},h,i,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"concurrency--threading"},"Concurrency / Threading"),(0,n.kt)("h2",{id:"concurrency"},"Concurrency"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Composition of independently executing functions/processes"),(0,n.kt)("li",{parentName:"ul"},"Parallelism is the simultaneous execution of multiple things, possible related and possibly not."),(0,n.kt)("li",{parentName:"ul"},"Concurrency is about dealing with a lot of things at once while parallelism is about doing a lot of things at once"),(0,n.kt)("li",{parentName:"ul"},"Concurrent - Mouse, keyboard, display and disk drivers"),(0,n.kt)("li",{parentName:"ul"},"Parallel - Vector dot product"),(0,n.kt)("li",{parentName:"ul"},"Adding some design can increase the speed of program execution (Analogy is of grofers working together to load books into the incinerator)")),(0,n.kt)("h2",{id:"process"},"Process"),(0,n.kt)("p",null,"In general, most processes can be described as either",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"https://en.wikipedia.org/wiki/I/O-bound"},"I/O-bound")," or ",(0,n.kt)("a",{parentName:"strong",href:"https://en.wikipedia.org/wiki/CPU-bound"},"CPU-bound")),". An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations. A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations. It is important that a long-term scheduler selects a good process mix of I/O-bound and CPU-bound processes. If all processes are I/O-bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do. On the other hand, if all processes are CPU-bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. The system with the best performance will thus have a combination of CPU-bound and I/O-bound processes. In modern operating systems, this is used to make sure that real-time processes get enough CPU time to finish their tasks."),(0,n.kt)("h2",{id:"threads"},"Threads"),(0,n.kt)("p",null,"A thread is just a sequence of instructions that can be executed independently by a processor. Threads are lighter than the process and so you can spawn a lot of them.\nA real life application would be a web server.\nA webserver typically is designed to handle multiple requests at the same time. And these requests normally don't depend on each other.\nSo a Thread can be created (or taken from a Thread pool) and requests can be delegated, to achieve concurrency.\nModern processors can executed multiple threads at once (multi-threading) and also switch between threads to achieve parallelism.A thread is a flow of execution through the process code. It has its own program counter that keeps track of which instruction to execute next. It also has system registers which hold its current working variables, and a stack which contains the execution history.\nA thread shares with its peer threads various information like code segment, data segment, and open files. When one thread alters a code segment memory item, all other threads see that.\nA thread is also called a",(0,n.kt)("strong",{parentName:"p"},"lightweight process"),". Threads provide a way to improve application performance through parallelism. Threads represent a software approach to improving the performance of operating systems by reducing the overhead. A thread is equivalent to a classical process.\nEach thread belongs to exactly one process, and no thread can exist outside a process. Each thread represents a separate flow of control. Threads have been successfully used in implementing network servers and web servers. They also provide a suitable foundation for parallel execution of applications on shared memory multiprocessors.\n",(0,n.kt)("img",{alt:"image",src:a(405290).Z,width:"704",height:"406"})),(0,n.kt)("h2",{id:"advantages-of-threads"},"Advantages of threads"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"They minimize the context switching time."),(0,n.kt)("li",{parentName:"ul"},"Using them provides concurrency within a process."),(0,n.kt)("li",{parentName:"ul"},"They provide efficient communication."),(0,n.kt)("li",{parentName:"ul"},"It is more economical to create and context switch threads."),(0,n.kt)("li",{parentName:"ul"},"Threads allow utilization of multiprocessor architectures to a greater scale and efficiency.")),(0,n.kt)("h2",{id:"threads-are-implemented-in-the-following-two-ways"},"Threads are implemented in the following two ways"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"User Level Threads:User-managed threads."),(0,n.kt)("li",{parentName:"ul"},"Kernel Level Threads:Operating System-managed threads acting on a kernel, an operating system core.")),(0,n.kt)("h2",{id:"user-level-threads"},"User Level Threads"),(0,n.kt)("p",null,"In this case, the thread management kernel is not aware of the existence of threads. The thread library contains code for creating and destroying threads, for passing messages and data between threads, for scheduling thread execution, and for saving and restoring thread contexts. The application starts with a single thread."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:a(272321).Z,width:"462",height:"274"})),(0,n.kt)("h2",{id:"advantages"},"Advantages"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Thread switching does not require Kernel mode privileges."),(0,n.kt)("li",{parentName:"ul"},"User level thread can run on any operating system."),(0,n.kt)("li",{parentName:"ul"},"Scheduling can be application-specific in the user level thread."),(0,n.kt)("li",{parentName:"ul"},"User level threads are fast to create and manage.")),(0,n.kt)("h2",{id:"disadvantages"},"Disadvantages"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"In a typical operating system, most system calls are blocking."),(0,n.kt)("li",{parentName:"ul"},"Multithreaded application cannot take advantage of multiprocessing.")),(0,n.kt)("h2",{id:"kernel-level-threads"},"Kernel Level Threads"),(0,n.kt)("p",null,"In this case, thread management is done by the Kernel. There is no thread management code in the application area. Kernel threads are supported directly by the operating system. Any application can be programmed to be multithreaded. All of the threads within an application are supported within a single process.\nThe Kernel maintains context information for the process as a whole and for individuals threads within the process. Scheduling by the Kernel is done on a thread basis. The Kernel performs thread creation, scheduling, and management in Kernel space. Kernel threads are generally slower to create and manage than the user threads."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:a(583055).Z,width:"461",height:"273"})),(0,n.kt)("h2",{id:"advantages-1"},"Advantages"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The Kernel can simultaneously schedule multiple threads from the same process on multiple processes."),(0,n.kt)("li",{parentName:"ul"},"If one thread in a process is blocked, the Kernel can schedule another thread of the same process."),(0,n.kt)("li",{parentName:"ul"},"Kernel routines themselves can be multithreaded.")),(0,n.kt)("h2",{id:"disadvantages-1"},"Disadvantages"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Kernel threads are generally slower to create and manage than the user threads."),(0,n.kt)("li",{parentName:"ul"},"Transfer of control from one thread to another within the same process requires a mode switch to the Kernel.")),(0,n.kt)("h2",{id:"threads-vs-processes"},"Threads vs Processes"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Threads ",(0,n.kt)("strong",{parentName:"li"},"uses shared state"),", so there is an ease of communication between multiple threads, but the disadvantage of shared state is that there can be race condition, multiple threads can race with each other to get access to resources."),(0,n.kt)("li",{parentName:"ul"},"Processes ",(0,n.kt)("strong",{parentName:"li"},"doesn't have shared state"),", they are fully independent from each other. The weakness of processes is lack of communication (hence the need for IPC and object pickling and other overhead)- Threads are more lightweight and have lower overhead compared to processes. Spawning processes is a bit slower than spawning threads.\n| ",(0,n.kt)("strong",{parentName:"li"},"Bottleneck")," | ",(0,n.kt)("strong",{parentName:"li"},"Example"),"                        | ",(0,n.kt)("strong",{parentName:"li"},"Optimize with")," |\n|----------------|------------------------------------|-------------------|\n| IO             | Network connection, file operation | Multithreading    |\n| CPU            | Complex math problem, search       | Multiprocessing   |\n",(0,n.kt)("a",{parentName:"li",href:"https://zacs.site/blog/linear-python.html"},"https://zacs.site/blog/linear-python.html"))),(0,n.kt)("h2",{id:"are-threads-lighter-than-processes"},"Are Threads Lighter than Processes"),(0,n.kt)("p",null,"Yes and No.\nIn concept,"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Threads share memory and don't need to create a new virtual memory space when they are created and thus don't require a MMU (memory management unit) context switch")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Communication between threads is simpler as they have a shared memory while processes requires various modes of IPC (Inter-Process Communications) like semaphores, message queues, pipes etc.\nThat being said, this doesn't always guarantee a better performance than processes in this multi-core processor world.\ne.g. Linux doesn't distinguish between threads and processes and both are called tasks. Each task can have a minimum to maximum level of sharing when cloned.\nWhen you callfork(), a new task is created with no shared file descriptors, PIDs and memory space. When you callpthread_create(), a new task is created with all of the above shared.\nAlso, synchronising data in ",(0,n.kt)("a",{parentName:"p",href:"https://users.cs.cf.ac.uk/Dave.Marshall/C/node27.html"},"shared memory")," as well as in ",(0,n.kt)("a",{parentName:"p",href:"https://www.quora.com/What-is-the-L1-L2-and-L3-cache-of-a-microprocessor-and-how-does-it-affect-the-performance-of-it-For-example-I-have-a-laptop-with-an-Intel-4700MQ-microprocessor-with-a-6MB-L3-cache-What-does-this-value-indicate"},"L1 cache")," of tasks running on multiple cores takes a bigger toll than running different processes on isolated memory.\nLinux developers have tried to minimise the cost between task switch and have succeeded at it. Creating a new task is still a bigger overhead than a new thread but switching is not."))),(0,n.kt)("h2",{id:"what-can-be-improved-in-threads"},"What can be improved in Threads?"),(0,n.kt)("p",null,"There are three things which make threads slow:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Threads consume a lot of memory due to their large stack size (\u2265 1MB). So creating 1000s of thread means you already need 1GB of memory.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Threads need to restore a lot of registers some of which include AVX( Advanced vector extension), SSE (Streaming SIMD Ext.), Floating Point registers, Program Counter (PC), Stack Pointer (SP) which hurts the application performance.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Threads setup and teardown requires call to OS for resources (such as memory) which is slow."))),(0,n.kt)("h2",{id:"threads-vs-async"},"Threads vs Async"),(0,n.kt)("h2",{id:"threads-1"},"Threads"),(0,n.kt)("p",null,"Threads switch preemptively. This is convenient because you don't need to add explicit code to cause a task switch."),(0,n.kt)("p",null,"The cost of this convenience is that you have to assume a switch can happen at any time. Accordingly, critical sections have to be guarded with locks. Dinning Philosophers Problem."),(0,n.kt)("p",null,"The limit on threads is total CPU power minus the cost of task switches and synchronization overhead."),(0,n.kt)("h2",{id:"async"},"Async"),(0,n.kt)("p",null,'Async switches cooperatively, so you do need to add explicit code "yield" or "await" to cause a task switch.'),(0,n.kt)("p",null,"Now you control when task switches occur, so locks and other synchronization are no longer needed."),(0,n.kt)("p",null,"Also, the cost task switches is very low. Calling a pure Python function has more overhead than restarting a generator or awaitable."),(0,n.kt)("p",null,"This means that async is very cheap."),(0,n.kt)("p",null,"In return, you'll need a non-blocking version of just about everything you do. Accordingly, the async world has a huge ecosystem of support tools. This increases the learning curve."),(0,n.kt)("h2",{id:"comparison"},"Comparison"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Async maximizes CPU utilization because it has less overhead than threads."),(0,n.kt)("li",{parentName:"ul"},"Threading typically works with existing code and tools as long as locks are added around critical sections."),(0,n.kt)("li",{parentName:"ul"},"For complex systems, async ismucheasier to get right than threads with locks."),(0,n.kt)("li",{parentName:"ul"},"Threads require very little tooling (locks and queues)."),(0,n.kt)("li",{parentName:"ul"},"Async needs a great deal of tooling (futures, event loops, and non-blocking versions of just about everything)."),(0,n.kt)("li",{parentName:"ul"},"In a threaded system the decision to suspend one thread and execute another is largely outside of the programmer's control. Rather, it is under the control of the operating system, and the programmer must assume that a thread may be suspended and replaced with another at almost any time. In contrast, under the asynchronous model a task will continue to run until it explicitly relinquishes control to other tasks.- The problem with locks is that it just a flag, and it should be checked to access the resources. If you don't check then there would be problems.")),(0,n.kt)("h2",{id:"considerations"},"Considerations"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Threading"),(0,n.kt)("li",{parentName:"ul"},"Multiprocessing"),(0,n.kt)("li",{parentName:"ul"},"Async")),(0,n.kt)("h2",{id:"amdahls-law"},"Amdahl's Law"),(0,n.kt)("h2",{id:"amdahls-law-is-often-used-in-parallel-computing-to-predict-the-theoretical-speedup-when-using-multiple-processors-for-example-if-a-program-needs-20-hours-using-a-single-processor-core-and-a-particular-part-of-the-program-which-takes-one-hour-to-execute-cannot-be-parallelized-while-the-remaining-19-hours-p--095-of-execution-time-can-be-parallelized-then-regardless-of-how-many-processors-are-devoted-to-a-parallelized-execution-of-this-program-the-minimum-execution-time-cannot-be-less-than-that-critical-one-hour-hence-the-theoretical-speedup-is-limited-to-at-most-20-times11p20-for-this-reason-parallel-computing-is-relevant-only-for-a-low-number-of-processors-and-very-parallelizable-programs"},"Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors. For example, if a program needs 20 hours using a single processor core, and a particular part of the program which takes one hour to execute cannot be parallelized, while the remaining 19 hours (p = 0.95) of execution time can be parallelized, then regardless of how many processors are devoted to a parallelized execution of this program, the minimum execution time cannot be less than that critical one hour. Hence, the theoretical speedup is limited to at most 20 times(1/(1\u2212p)=20). For this reason parallel computing is relevant only for a low number of processors and very parallelizable programs"),(0,n.kt)("h2",{id:"process-control-block"},"Process Control Block"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Process State")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Process Number")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Program Counter")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Registers")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Memory Limits")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"List of open Files")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Signal Mask")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"CPU Scheduling info"))),(0,n.kt)("h2",{id:"synchronization"},"Synchronization"),(0,n.kt)("p",null,"Synchronizationrefers to one of two distinct but related concepts"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Process Synchronization"))),(0,n.kt)("p",null,"Process synchronizationrefers to the idea that multiple processes are to join up or ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Handshaking"},"handshake")," at a certain point, in order to reach an agreement or commit to a certain sequence of action."),(0,n.kt)("ol",{start:2},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Data Synchronization"))),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Data_synchronization"},"Data synchronization")," refers to the idea of keeping multiple copies of a dataset in coherence with one another, or to maintain ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Data_integrity"},"data integrity"),"\nProcess synchronization primitives are commonly used to implement data synchronization."),(0,n.kt)("h2",{id:"need-for-synchronization"},"Need for synchronization"),(0,n.kt)("p",null,"The need for synchronization does not arise merely in multi-processor systems but for any kind of concurrent processes; even in single processor systems. Mentioned below are some of the main needs for synchronization:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Fork-join_model"},"Forks and Joins"),":When a job arrives at a fork point, it is split into N sub-jobs which are then serviced by n tasks. After being serviced, each sub-job waits until all other sub-jobs are done processing. Then, they are joined again and leave the system. Thus, in parallel programming, we require synchronization as all the parallel processes wait for several other processes to occur."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem"},"Producer-Consumer:")," In a producer-consumer relationship, the consumer process is dependent on the producer process till the necessary data has been produced."),(0,n.kt)("li",{parentName:"ul"},"Exclusive use resources:When multiple processes are dependent on a resource and they need to access it at the same time the operating system needs to ensure that only one processor accesses it at a given point in time.This reduces concurrency.")),(0,n.kt)("h2",{id:"implementation-of-synchronization"},"Implementation of Synchronization"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Spinlock"))),(0,n.kt)("p",null,"Another effective way of implementing synchronization is by using spinlocks. Before accessing any shared resource or piece of code, every processor checks a flag. If the flag is reset, then the processor sets the flag and continues executing the thread. But, if the flag is set (locked), the threads would keep spinning in a loop and keep checking if the flag is set or not. But, spinlocks are effective only if the flag is reset for lower cycles otherwise it can lead to performance issues as it wastes many processor cycles waiting.- ",(0,n.kt)("strong",{parentName:"p"},"Barriers")),(0,n.kt)("p",null,"Barriers are simple to implement and provide good responsiveness. They are based on the concept of implementing wait cycles to provide synchronization. Consider three threads running simultaneously, starting from barrier 1. After time t, thread1 reaches barrier 2 but it still has to wait for thread 2 and 3 to reach barrier2 as it does not have the correct data. Once all the threads reach barrier 2 they all start again. After time t, thread 1 reaches barrier3 but it will have to wait for threads 2 and 3 and the correct data again.\nThus, in barrier synchronization of multiple threads there will always be a few threads that will end up waiting for other threads as in the above example thread 1 keeps waiting for thread 2 and 3. This results in severe degradation of the process performance.\nIn ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Parallel_computing"},"parallel computing"),", abarrieris a type of ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Synchronization_(computer_science)"},"synchronization")," method. A barrier for a group of threads or processes in the source code means any thread/process must stop at this point and cannot proceed until all other threads/processes reach this barrier.\nMany collective routines and directive-based parallel languages impose implicit barriers. For example, a paralleldoloop in ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Fortran"},"Fortran")," with ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/OpenMP"},"OpenMP")," will not be allowed to continue on any thread until the last iteration is completed. This is in case the program relies on the result of the loop immediately after its completion. In ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Message_passing"},"message passing"),", any global communication (such as reduction or scatter) may imply a barrier.\n",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Barrier_(computer_science)"},"https://en.wikipedia.org/wiki/Barrier_(computer_science)"),"- ",(0,n.kt)("strong",{parentName:"p"},"Semaphores")),(0,n.kt)("p",null,"Semaphores are signalling mechanisms which can allow one or more threads/processors to access a section. A Semaphore has a flag which has a certain fixed value associated with it and each time a thread wishes to access the section, it decrements the flag. Similarly, when the thread leaves the section, the flag is incremented. If the flag is zero, the thread cannot access the section and gets blocked if it chooses to wait."),(0,n.kt)("p",null,"Some semaphores would allow only one thread or process in the code section. Such Semaphores are called binary semaphore and are very similar to Mutex. Here, if the value of semaphore is 1, the thread is allowed to access and if the value is 0, the access is denied.\n",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Synchronization_(computer_science)"},"https://en.wikipedia.org/wiki/Synchronization_(computer_science)")),(0,n.kt)("h2",{id:"hyper---threading-technology-htt"},"Hyper - Threading Technology (HTT)"),(0,n.kt)("p",null,"Hyper-threading(officially calledHyper-Threading TechnologyorHT Technologyand abbreviated asHTTorHT) is ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Intel"},"Intel's"),(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Proprietary_hardware"},"proprietary"),(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Simultaneous_multithreading"},"simultaneous multithreading"),"(SMT) implementation used to improve ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Parallel_computation"},"parallelization")," of computations (doing multiple tasks at once) performed on ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/X86"},"x86")," microprocessors. It first appeared in February 2002 on ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Xeon"},"Xeon")," server ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Central_processing_unit"},"processors")," and in November 2002 on ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Pentium_4"},"Pentium4")," desktop CPUs.Later, Intel included this technology in ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Itanium"},"Itanium"),", ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Intel_Atom"},"Atom"),", and ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Intel_Core"},"Core 'i' Series")," CPUs, among others.\nFor each ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Processor_core"},"processor core")," that is physically present, the ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Operating_system"},"operating system")," addresses two virtual (logical) cores and shares the workload between them when possible. The main function of hyper-threading is to increase the number of independent instructions in the pipeline; it takes advantage of ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Superscalar_processor"},"superscalar")," architecture, in which multiple instructions operate on separate data ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Parallel_computing"},"in parallel"),". With HTT, one physical core appears as two processors to the operating system, allowing ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Concurrent_computing"},"concurrent")," scheduling of two processes per core. In addition, two or more processes can use the same resources: If resources for one process are not available, then another process can continue if its resources are available.\nIn addition to requiring simultaneous multithreading (SMT) support in the operating system, hyper-threading can be properly utilized only with an operating system specifically optimized for it.Furthermore, Intel recommends HTT to be disabled when using operating systems unaware of this hardware feature.\n",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Hyper-threading"},"https://en.wikipedia.org/wiki/Hyper-threading"),"\nMultithreading, concurrency, locks, synchronization"),(0,n.kt)("h2",{id:"compare-and-swap-cas"},"Compare and Swap (CAS)"),(0,n.kt)("p",null,"Compare and Swap is an atomic structure used in multithreading to achieve synchronization. It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Boolean_logic"},"boolean")," response (this variant is often called",(0,n.kt)("strong",{parentName:"p"},"compare-and-set"),"), or by returning the value read from the memory location (",(0,n.kt)("em",{parentName:"p"},"not"),"the value written to it).\nAtomic instruction that compares contents of a memory location M to a given value V"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"If values are equal, installs new given value V' in M"),(0,n.kt)("li",{parentName:"ul"},"Otherwise operation fails\n__sync_bool_compare_and_swap(&M, 20, 30)")),(0,n.kt)("p",null,"__sync_bool_compare_and_swap(Address, Compare Value, New Value)\n",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Compare-and-swap"},"https://en.wikipedia.org/wiki/Compare-and-swap")),(0,n.kt)("h2",{id:"see-also"},"See also"),(0,n.kt)("p",null,"Python > Advanced > Concurrency"),(0,n.kt)("h2",{id:"references"},"References"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://schneems.com/2017/10/23/wtf-is-a-thread/"},"https://schneems.com/2017/10/23/wtf-is-a-thread/#"),"\nDijkstra's Guarded Commands - ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Guarded_Command_Language"},"https://en.wikipedia.org/wiki/Guarded_Command_Language"),"\nCommunicating sequential processes ",(0,n.kt)("strong",{parentName:"p"},"- C.A.R. Hoare")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://vimeo.com/49718712"},"Rob Pike - 'Concurrency Is Not Parallelism'")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/demystifying-python-multiprocessing-and-multithreading-9b62f9875a27"},"Demystifying Python Multiprocessing and Multithreading | by David Chong | Towards Data Science")))}d.isMDXComponent=!0},405290:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/Concurrency-Threading-image1-e22b75bf57833090065863b7ac641176.jpg"},272321:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/Concurrency-Threading-image2-5ac1eb9bad2125f82e1f9bf0185da7ec.jpg"},583055:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/Concurrency-Threading-image3-dd2de0f6311677bb8c9e39b818575523.jpg"}}]);