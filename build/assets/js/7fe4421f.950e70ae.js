"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[32995],{603905:(e,t,i)=>{i.d(t,{Zo:()=>p,kt:()=>m});var a=i(667294);function r(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function n(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,a)}return i}function s(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?n(Object(i),!0).forEach((function(t){r(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):n(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function o(e,t){if(null==e)return{};var i,a,r=function(e,t){if(null==e)return{};var i,a,r={},n=Object.keys(e);for(a=0;a<n.length;a++)i=n[a],t.indexOf(i)>=0||(r[i]=e[i]);return r}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(a=0;a<n.length;a++)i=n[a],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(r[i]=e[i])}return r}var l=a.createContext({}),d=function(e){var t=a.useContext(l),i=t;return e&&(i="function"==typeof e?e(t):s(s({},t),e)),i},p=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},h="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var i=e.components,r=e.mdxType,n=e.originalType,l=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),h=d(i),u=r,m=h["".concat(l,".").concat(u)]||h[u]||c[u]||n;return i?a.createElement(m,s(s({ref:t},p),{},{components:i})):a.createElement(m,s({ref:t},p))}));function m(e,t){var i=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var n=i.length,s=new Array(n);s[0]=u;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[h]="string"==typeof e?e:r,s[1]=o;for(var d=2;d<n;d++)s[d]=i[d];return a.createElement.apply(null,s)}return a.createElement.apply(null,i)}u.displayName="MDXCreateElement"},851779:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>n,metadata:()=>o,toc:()=>d});var a=i(487462),r=(i(667294),i(603905));const n={},s="RAID",o={unversionedId:"computer-science/operating-system/raid",id:"computer-science/operating-system/raid",title:"RAID",description:'RAID (Redundant Array of Inexpensive Disks or Drives, or Redundant Array of Independent Disks) is a data storage virtualization technology that combines multiple physical disk drive components into one or more logical units for the purposes of data redundancy, performance improvement, or both. This was in contrast to the previous concept of highly reliable mainframe disk drives referred to as "single large expensive disk" (SLED).',source:"@site/docs/computer-science/operating-system/raid.md",sourceDirName:"computer-science/operating-system",slug:"/computer-science/operating-system/raid",permalink:"/computer-science/operating-system/raid",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/computer-science/operating-system/raid.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Parallel Processing",permalink:"/computer-science/operating-system/parallel-processing"},next:{title:"Scheduling",permalink:"/computer-science/operating-system/scheduling"}},l={},d=[{value:"Standard levels",id:"standard-levels",level:2},{value:"RAID0",id:"raid0",level:2},{value:"RAID1",id:"raid1",level:2},{value:"RAID2",id:"raid2",level:2},{value:"RAID3",id:"raid3",level:2},{value:"RAID4",id:"raid4",level:2},{value:"RAID5",id:"raid5",level:2},{value:"RAID6",id:"raid6",level:2},{value:"Nested (hybrid) RAID",id:"nested-hybrid-raid",level:2},{value:"Non-standard levels",id:"non-standard-levels",level:2},{value:"Implementations",id:"implementations",level:2},{value:"Integrity",id:"integrity",level:2},{value:"Weaknesses",id:"weaknesses",level:2},{value:"Striping",id:"striping",level:2},{value:"Mirroring",id:"mirroring",level:2},{value:"Parity",id:"parity",level:2}],p={toc:d},h="wrapper";function c(e){let{components:t,...n}=e;return(0,r.kt)(h,(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"raid"},"RAID"),(0,r.kt)("p",null,"RAID (Redundant Array of Inexpensive Disks or Drives, or Redundant Array of Independent Disks) is a data ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Storage_virtualization"},"storage virtualization")," technology that combines multiple physical ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Disk_drive"},"disk drive")," components into one or more logical units for the purposes of ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Data_redundancy"},"data redundancy"),', performance improvement, or both. This was in contrast to the previous concept of highly reliable mainframe disk drives referred to as "single large expensive disk" (SLED).'),(0,r.kt)("p",null,"Data is distributed across the drives in one of several ways, referred to as RAID levels, depending on the required level of ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Redundancy_(engineering)"},"redundancy"),' and performance. The different schemes, or data distribution layouts, are named by the word "RAID" followed by a number, for example RAID0 or RAID1. Each scheme, or RAID level, provides a different balance among the key goals:',(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Reliability_engineering"},"reliability"),", ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Availability"},"availability"),", ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Computer_performance"},"performance"),", and ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Computer_data_storage#Capacity"},"capacity"),". RAID levels greater than RAID0 provide protection against unrecoverable ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Disk_sector"},"sector")," read errors, as well as against failures of whole physical drives."),(0,r.kt)("p",null,'Many RAID levels employ an error protection scheme called "',(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Parity_bit"},"parity"),'", a widely used method in information technology to provide ',(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Fault_tolerance"},"fault tolerance")," in a given set of data. Most use simple ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Exclusive_or"},"XOR"),", but RAID6 uses two separate parities based respectively on addition and multiplication in a particular ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Galois_field"},"Galois field")," or ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction"},"Reed--Solomon error correction"),"."),(0,r.kt)("p",null,"RAID can also provide data security with ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Solid-state_drive"},"solid-state drives"),"(SSDs) without the expense of an all-SSD system. For example, a fast SSD can be mirrored with a mechanical drive. For this configuration to provide a significant speed advantage an appropriate controller is needed that uses the fast SSD for all read operations.",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Adaptec"},"Adaptec"),' calls this "hybrid RAID".'),(0,r.kt)("h2",{id:"standard-levels"},"Standard levels"),(0,r.kt)("p",null,"A number of standard schemes have evolved. These are calledlevels. Originally, there were five RAID levels, but many variations have evolved, notably several ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Nested_RAID_levels"},"nested levels")," and many ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Non-standard_RAID_levels"},"non-standard levels"),"(mostly ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Proprietary_software"},"proprietary"),"). RAID levels and their associated data formats are standardized by the ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Storage_Networking_Industry_Association"},"Storage Networking Industry Association"),"(SNIA) in the Common RAID Disk Drive Format (DDF) standard:"),(0,r.kt)("h2",{id:"raid0"},"RAID0"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_0"},"RAID0")," consists of ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Data_striping"},"striping"),", but no ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Disk_mirroring"},"mirroring")," or ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Parity_bit"},"parity"),". Compared to a ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Spanned_volume"},"spanned volume"),", thecapacityof a RAID0 volume is the same; it is the sum of the capacities of the disks in the set. But because striping distributes the contents ofeachfile amongalldisks in the set, the failure of any disk causesallfiles, the entire RAID0 volume, to be lost. A broken spanned volume at least preserves the files on the unfailing disks. The benefit of RAID0 is that the ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Throughput"},"throughput")," of read and write operations to any file is multiplied by the number of disks because, unlike spanned volumes, reads and writes are done ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Concurrency_(computer_science)"},"concurrently"),", and the cost is complete vulnerability to drive failures. Indeed, the average failure rate is worse than that of an equivalent single non-RAID drive."),(0,r.kt)("h2",{id:"raid1"},"RAID1"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_1"},"RAID1"),' consists of data mirroring, without parity or striping. Data is written identically to two drives, thereby producing a "mirrored set" of drives. Thus, any read request can be serviced by any drive in the set. If a request is broadcast to every drive in the set, it can be serviced by the drive that accesses the data first (depending on its ',(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Seek_time"},"seek time")," and ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Rotational_latency"},"rotational latency"),"), improving performance. Sustained read throughput, if the controller or software is optimized for it, approaches the sum of throughputs of every drive in the set, just as for RAID0. Actual read throughput of most RAID1 implementations is slower than the fastest drive. Write throughput is always slower because every drive must be updated, and the slowest drive limits the write performance. The array continues to operate as long as at least one drive is functioning."),(0,r.kt)("h2",{id:"raid2"},"RAID2"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_2"},"RAID2")," consists of bit-level striping with dedicated ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Hamming_code"},"Hamming-code")," parity. All disk spindle rotation is synchronized and data is ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Data_striping"},"striped")," such that each sequential ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Bit"},"bit")," is on a different drive. Hamming-code parity is calculated across corresponding bits and stored on at least one parity drive.This level is of historical significance only; although it was used on some early machines (for example, the ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Thinking_Machines_Corporation"},"Thinking Machines")," CM-2), as of 2014it is not used by any commercially available system."),(0,r.kt)("h2",{id:"raid3"},"RAID3"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_3"},"RAID3")," consists of byte-level striping with dedicated parity. All disk spindle rotation is synchronized and data is striped such that each sequential ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Byte"},"byte")," is on a different drive. Parity is calculated across corresponding bytes and stored on a dedicated parity drive.Although implementations exist, RAID3 is not commonly used in practice."),(0,r.kt)("h2",{id:"raid4"},"RAID4"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_4"},"RAID4")," consists of block-level striping with dedicated parity. This level was previously used by ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/NetApp"},"NetApp"),", but has now been largely replaced by a proprietary implementation of RAID4 with two parity disks, called ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID-DP"},"RAID-DP"),". The main advantage of RAID4 over RAID2 and 3 is I/O parallelism: in RAID2 and 3, a single read I/O operation requires reading the whole group of data drives, while in RAID4 one I/O read operation does not have to spread across all data drives. As a result, more I/O operations can be executed in parallel, improving the performance of small transfers."),(0,r.kt)("h2",{id:"raid5"},"RAID5"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_5"},"RAID5"),' consists of block-level striping with distributed parity. Unlike RAID4, parity information is distributed among the drives, requiring all drives but one to be present to operate. Upon failure of a single drive, subsequent reads can be calculated from the distributed parity such that no data is lost. RAID5 requires at least three disks.Like all single-parity concepts, large RAID5 implementations are susceptible to system failures because of trends regarding array rebuild time and the chance of drive failure during rebuild (see "',(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID#Increasing_rebuild_time_and_failure_probability"},"Increasing rebuild time and failure probability"),'" section, below).Rebuilding an array requires reading all data from all disks, opening a chance for a second drive failure and the loss of the entire array.'),(0,r.kt)("h2",{id:"raid6"},"RAID6"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_6"},"RAID6")," consists of block-level striping with double distributed parity. Double parity provides fault tolerance up to two failed drives. This makes larger RAID groups more practical, especially for high-availability systems, as large-capacity drives take longer to restore. RAID6 requires a minimum of four disks. As with RAID5, a single drive failure results in reduced performance of the entire array until the failed drive has been replaced.With a RAID6 array, using drives from multiple sources and manufacturers, it is possible to mitigate most of the problems associated with RAID5. The larger the drive capacities and the larger the array size, the more important it becomes to choose RAID6 instead of RAID5.RAID10 also minimizes these problems."),(0,r.kt)("h2",{id:"nested-hybrid-raid"},"Nested (hybrid) RAID"),(0,r.kt)("p",null,'In what was originally termedhybrid RAID, many storage controllers allow RAID levels to be nested. The elements of aRAIDmay be either individual drives or arrays themselves. Arrays are rarely nested more than one level deep.\nThe final array is known as the top array. When the top array is RAID0 (such as in RAID1+0 and RAID5+0), most vendors omit the "+" (yielding ',(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID_10"},"RAID10")," and RAID50, respectively)."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"RAID0+1:")," creates two stripes and mirrors them. If a single drive failure occurs then one of the stripes has failed, at this point it is running effectively as RAID 0 with no redundancy. Significantly higher risk is introduced during a rebuild than RAID 1+0 as all the data from all the drives in the remaining stripe has to be read rather than just from one drive, increasing the chance of an unrecoverable read error (URE) and significantly extending the rebuild window."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"RAID1+0:")," (see:",(0,r.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/RAID_10"},"RAID10"),") creates a striped set from a series of mirrored drives. The array can sustain multiple drive losses so long as no mirror loses all its drives.\nRAID 10 is a nested RAID system created by combining RAID 1 and RAID 0. The combination is known as a stripe of mirrors.- ",(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("a",{parentName:"strong",href:"https://en.wikipedia.org/wiki/JBOD"},"JBOD")," RAID N+N"),":With ",(0,r.kt)("strong",{parentName:"li"},"JBOD (just a bunch of disks)"),", it is possible to concatenate disks, but also volumes such as RAID sets. With larger drive capacities, write delay and rebuilding time increase dramatically (especially, as described above, with RAID 5 and RAID 6). By splitting a larger RAID N set into smaller subsets and concatenating them with linear JBOD, write and rebuilding time will be reduced. If a hardware RAID controller is not capable of nesting linear JBOD with RAID N, then linear JBOD can be achieved with OS-level software RAID in combination with separate RAID N subset volumes created within one, or more, hardware RAID controller(s). Besides a drastic speed increase, this also provides a substantial advantage: the possibility to start a linear JBOD with a small set of disks and to be able to expand the total set with disks of different size, later on (in time, disks of bigger size become available on the market). There is another advantage in the form of disaster recovery (if a RAID N subset happens to fail, then the data on the other RAID N subsets is not lost, reducing restore time).")),(0,r.kt)("h2",{id:"non-standard-levels"},"Non-standard levels"),(0,r.kt)("p",null,"Many configurations other than the basic numbered RAID levels are possible, and many companies, organizations, and groups have created their own non-standard configurations, in many cases designed to meet the specialized needs of a small niche group. Such configurations include the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Linux_MD_RAID_10"},"Linux MD RAID 10"),' provides a general RAID driver that in its "near" layout defaults to a standard RAID1 with two drives, and a standard RAID1+0 with four drives; however, it can include any number of drives, including odd numbers. With its "far" layout, MD RAID10 can run both striped and mirrored, even with only two drives inf2layout; this runs mirroring with striped reads, giving the read performance of RAID0. Regular RAID1, as provided by ',(0,r.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Linux_software_RAID"},"Linux software RAID"),", does not stripe reads, but can perform reads in parallel."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Hadoop"},"Hadoop")," has a RAID system that generates a parity file by xor-ing a stripe of blocks in a single HDFS file."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/BeeGFS"},"BeeGFS"),", the parallel file system, has internal striping (comparable to file-based RAID0) and replication (comparable to file-based RAID10) options to aggregate throughput and capacity of multiple servers and is typically based on top of an underlying RAID to make disk failures transparent.")),(0,r.kt)("h2",{id:"implementations"},"Implementations"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Hardware based"),(0,r.kt)("li",{parentName:"ol"},"Software based"),(0,r.kt)("li",{parentName:"ol"},"Firmware and driver based")),(0,r.kt)("h2",{id:"integrity"},"Integrity"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Data_scrubbing"},"Data scrubbing"),"(referred to in some environments aspatrol read) involves periodic reading and checking by the RAID controller of all the blocks in an array, including those not otherwise accessed. This detects bad blocks before use.Data scrubbing checks for bad blocks on each storage device in an array, but also uses the redundancy of the array to recover bad blocks on a single drive and to reassign the recovered data to spare blocks elsewhere on the drive."),(0,r.kt)("h2",{id:"weaknesses"},"Weaknesses"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Correlated failures"),(0,r.kt)("li",{parentName:"ol"},"Unrecoverable read errors during rebuild"),(0,r.kt)("li",{parentName:"ol"},"Increasing rebuild time and failure probability"),(0,r.kt)("li",{parentName:"ol"},"Atomicity: including parity inconsistency due to system crashes"),(0,r.kt)("li",{parentName:"ol"},"Write-cache reliability")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/RAID"},"https://en.wikipedia.org/wiki/RAID")),(0,r.kt)("h2",{id:"striping"},"Striping"),(0,r.kt)("p",null,"We all know that, RAID is collection of multiple disk'sand in these disk predefined number of contiguously addressable disk blocks are defined which are called asstripsand collection of such strips in aligned in multiple disk is called stripe."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:i(852848).Z,width:"573",height:"315"})),(0,r.kt)("p",null,"Suppose you have ",(0,r.kt)("a",{parentName:"p",href:"https://www.storagetutorials.com/add-storage-vm-host-without-reboot/"},"hard disk"),", which is a collection of multiple addressable block and these blocks are stacked together and called strip and you have multiple such hard disk, which are place parallel or serially. Then such combination of disk is called stripe."),(0,r.kt)("p",null,"Note: Without mirroring and parity, Striped RAID cannot protect data but striping may significantly improve I/O performance."),(0,r.kt)("h2",{id:"mirroring"},"Mirroring"),(0,r.kt)("p",null,"Mirroring is very simple to understand and one of the most reliable way of data protection. In this technique, you just make amirror copy of diskwhich you want to protect and in this way you have two copies of data. In the time of failure, the controller use second disk to serve the data, thus making data availability continuous."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:i(315943).Z,width:"487",height:"355"})),(0,r.kt)("p",null,"When the failed disk is replaced with a new disk, the controller copies the data from the surviving disk of themirrored pair. Data is simultaneously recorded on both the disk. Though this type of RAID gives you highest availability of data but it is costly as it requires double amount of disk space and thus increasing the cost."),(0,r.kt)("h2",{id:"parity"},"Parity"),(0,r.kt)("p",null,"As explained above, mirroring involves high cost, so to protect the data new technique is used with striping called parity. This is reliable andlow cost solution for data protection. In this method and additional HDD or disk is added to the stripe width to hold parity bit."),(0,r.kt)("p",null,"Parity is a redundancy check that ensures full protection of data without maintaining a full set of duplicate data."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:i(621749).Z,width:"694",height:"359"})),(0,r.kt)("p",null,"The parity bits are used to re-create the data at the time of failure. Parity information can be stored on separate, dedicated HDDsor distributed across all the drives in a RAID set. In the above image, parity is stored on a separate disk."),(0,r.kt)("p",null,"The first three disks, labeled D, contain the data. The fourth disk, labeled P, stores the parity information, which in this case is the sum of the elements in each row. Now, if one of the Disks (D) fails, the missing value can be calculated by subtracting the sum of the rest of the elements from the parity value."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.storagetutorials.com/understanding-concept-striping-mirroring-parity"},"https://www.storagetutorials.com/understanding-concept-striping-mirroring-parity")))}c.isMDXComponent=!0},852848:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/RAID-image1-b8a6483f0eb9ba30ea5b54014e8f1789.jpg"},315943:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/RAID-image2-adf765abcd62661d2bcf753c2ceed7e4.jpg"},621749:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/RAID-image3-1f7a8e8d3ded10d69494c20d7fe6d3ac.jpg"}}]);