"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[1536],{603905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>h});var o=t(667294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);a&&(o=o.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,o)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,o,n=function(e,a){if(null==e)return{};var t,o,n={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=o.createContext({}),p=function(e){var a=o.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},c=function(e){var a=p(e.components);return o.createElement(l.Provider,{value:a},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return o.createElement(o.Fragment,{},a)}},m=o.forwardRef((function(e,a){var t=e.components,n=e.mdxType,r=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),d=p(t),m=n,h=d["".concat(l,".").concat(m)]||d[m]||u[m]||r;return t?o.createElement(h,s(s({ref:a},c),{},{components:t})):o.createElement(h,s({ref:a},c))}));function h(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var r=t.length,s=new Array(r);s[0]=m;var i={};for(var l in a)hasOwnProperty.call(a,l)&&(i[l]=a[l]);i.originalType=e,i[d]="string"==typeof e?e:n,s[1]=i;for(var p=2;p<r;p++)s[p]=t[p];return o.createElement.apply(null,s)}return o.createElement.apply(null,t)}m.displayName="MDXCreateElement"},549118:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var o=t(487462),n=(t(667294),t(603905));const r={},s="Apache Hadoop",i={unversionedId:"technologies/apache/apache-hadoop/about",id:"technologies/apache/apache-hadoop/about",title:"Apache Hadoop",description:"- MapReduce API (Processing large parallel data)",source:"@site/docs/technologies/apache/apache-hadoop/about.md",sourceDirName:"technologies/apache/apache-hadoop",slug:"/technologies/apache/apache-hadoop/about",permalink:"/technologies/apache/apache-hadoop/about",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/technologies/apache/apache-hadoop/about.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Apache Hadoop",permalink:"/technologies/apache/apache-hadoop/"},next:{title:"Big Data Hadoop Stack",permalink:"/technologies/apache/apache-hadoop/big-data-hadoop-stack"}},l={},p=[{value:"Key points",id:"key-points",level:2},{value:"Layers",id:"layers",level:2},{value:"Hadoop",id:"hadoop",level:2},{value:"Working",id:"working",level:2},{value:"YARN - Yet Another Resource Manager",id:"yarn---yet-another-resource-manager",level:2},{value:"References",id:"references",level:2}],c={toc:p},d="wrapper";function u(e){let{components:a,...r}=e;return(0,n.kt)(d,(0,o.Z)({},c,r,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"apache-hadoop"},"Apache Hadoop"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"MapReduce API (Processing large parallel data)"),(0,n.kt)("li",{parentName:"ul"},"MapReduce job management"),(0,n.kt)("li",{parentName:"ul"},"Distributed Filesystem (HDFS)")),(0,n.kt)("p",null,"Java based file system which is distributed and fault-tolerant and hadoop relies on HDFS for doing all its processing"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Enormous ecosystem")),(0,n.kt)("h2",{id:"key-points"},"Key points"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Open source framework"),(0,n.kt)("li",{parentName:"ul"},"Stores data in a distributed manner"),(0,n.kt)("li",{parentName:"ul"},"Process data in parallel")),(0,n.kt)("h2",{id:"layers"},"Layers"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"HDFS, reliable storage layer"),(0,n.kt)("li",{parentName:"ul"},"MapReduce, batch processing engine"),(0,n.kt)("li",{parentName:"ul"},"YARN, resouce management layer")),(0,n.kt)("h2",{id:"hadoop"},"Hadoop"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"It has two basic parts",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Hadoop Distributed File System (HDFS) is the storage system of Hadoop which splits big data and distribute across many nodes in a cluster",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Scaling out of H/W resources"),(0,n.kt)("li",{parentName:"ul"},"Fault Tolerant"))),(0,n.kt)("li",{parentName:"ul"},"MapReduce: Programming model that simplifies parallel programming",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Map -> apply()"),(0,n.kt)("li",{parentName:"ul"},"Reduce -> summarize()")))))),(0,n.kt)("p",null,"Google uses MapReduce for indexing websites"),(0,n.kt)("p",null,"Hadoop is an open-source framework to store and process Big Data in a distributed environment. It contains two modules, one is MapReduce and another is Hadoop Distributed File System (HDFS)."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"MapReduce:It is a parallel programming model for processing large amounts of structured, semi-structured, and unstructured data on large clusters of commodity hardware."),(0,n.kt)("li",{parentName:"ul"},"HDFS:Hadoop Distributed File System is a part of Hadoop framework, used to store and process the datasets. It provides a fault-tolerant file system to run on commodity hardware.")),(0,n.kt)("p",null,"The Hadoop ecosystem contains different sub-projects (tools) such as Sqoop, Pig, and Hive that are used to help Hadoop modules."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Sqoop:It is used to import and export data to and from between HDFS and RDBMS."),(0,n.kt)("li",{parentName:"ul"},"Pig:It is a procedural language platform used to develop a script for MapReduce operations."),(0,n.kt)("li",{parentName:"ul"},"Hive:It is a platform used to develop SQL type scripts to do MapReduce operations.")),(0,n.kt)("p",null,"Note:There are various ways to execute MapReduce operations:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The traditional approach using Java MapReduce program for structured, semi-structured, and unstructured data."),(0,n.kt)("li",{parentName:"ul"},"The scripting approach for MapReduce to process structured and semi structured data using Pig."),(0,n.kt)("li",{parentName:"ul"},"The Hive Query Language (HiveQL or HQL) for MapReduce to process structured data using Hive.")),(0,n.kt)("p",null,"5 daemons run on Hadoop in these 3 layers. Daemons are the processes that run in the background. 5 daemons of Hadoop are as follows:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"NameNode"))),(0,n.kt)("p",null,"It works asMasterin Hadoop cluster. Namenode stores meta-data i.e. number of blocks, replicas and other details. Meta-data is present in memory in the master. NameNode also assigns tasks to the slave node. As it is the centerpiece of HDFS, so it should deploy on reliable hardware"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"DataNode"))),(0,n.kt)("p",null,"It works asSlavein Hadoop cluster. In Hadoop HDFS, DataNode is responsible for storing actual data in HDFS. DataNode performs read and write operation as per request for the clients. DataNodes can also deploy on commodity hardware."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Secondary NameNode"))),(0,n.kt)("p",null,"Its main function is to take checkpoints of the file system metadata present on namenode. It is not the backup namenode. It is a helper to the primary NameNode but it does not replace the primary namenode."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"ResourceManager"))),(0,n.kt)("p",null,"It is a cluster level component and runs on the Master machine. Hence it manages resources and schedule applications running on the top of YARN. It has two components: Scheduler & Application Manager."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"NodeManager"))),(0,n.kt)("p",null,"It is a node level component. NodeManager runs on each slave machine. It continuously communicate with Resource Manager to remain up-to-date"),(0,n.kt)("h2",{id:"working"},"Working"),(0,n.kt)("p",null,"To process any data, the client first submits data and program. Hadoop store data usingHDFSand then process the data using MapReduce."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Hadoop Data Storage")),(0,n.kt)("p",{parentName:"li"},"Hadoop Distributed File System-- HDFS is the primary storage system of Hadoop. It stores very large files running on a cluster of commodity hardware. HDFS stores data reliably even in the case of machine failure. It also provides high throughput access to the application by accessing in parallel."),(0,n.kt)("p",{parentName:"li"},"The data is broken into small chunks as blocks. Block is the smallest unit of data that the file system store. Hadoop application distributes data blocks across the multiple nodes. Then, each block is replicated as per the replication factor (by default 3). Once all the blocks of the data are stored on datanode, the user can process the data.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Hadoop Data Processing")),(0,n.kt)("p",{parentName:"li"},"Hadoop MapReduce is the data processing layer. It is the framework for writing applications that process the vast amount of data stored in the HDFS. MapReduce processes a huge amount of data in parallel by dividing the job into a set of independent tasks (sub-job). In Hadoop, MapReduce works by breaking the processing into phases: Map and Reduce."),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Map --It is the first phase of processing. In which we specify all the complex logic/business rules/costly code. The map takes a set of data and converts it into another set of data. It also breaks individual elements into tuples (key-value pairs)."),(0,n.kt)("li",{parentName:"ul"},"Reduce --It is the second phase of processing. In which we specify light-weight processing like aggregation/summation. The output from the map is the input to Reducer. Then, reducer combines tuples (key-value) based on the key. And then, modifies the value of the key accordingly.")))),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:t(305547).Z,width:"1440",height:"1080"})),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:t(604268).Z,width:"1440",height:"1080"})),(0,n.kt)("p",null,"Map Reduce"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"MapReduce is a programming model and an associated implmentation for processing and generating large data sets"),(0,n.kt)("li",{parentName:"ul"},"Users specify a map function that processs a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:t(976768).Z,width:"1440",height:"1080"})),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:t(939281).Z,width:"1440",height:"1080"})),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:t(78555).Z,width:"1440",height:"1080"})),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:t(42993).Z,width:"1440",height:"1080"})),(0,n.kt)("h2",{id:"yarn---yet-another-resource-manager"},"YARN - Yet Another Resource Manager"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Apache Hadoop YARN is the resource management and job scheduling technology in the open source Hadoop distributed processing framework"),(0,n.kt)("li",{parentName:"ul"},"YARN is responsible for allocating system resources to the various applications running in a Hadoop cluster and scheduling tasks to be executed on different cluster nodes")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image",src:t(658097).Z,width:"1440",height:"1080"})),(0,n.kt)("h2",{id:"references"},"References"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://techvidvan.com/tutorials/how-hadoop-works-internally"},"https://techvidvan.com/tutorials/how-hadoop-works-internally")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://www.toptal.com/hadoop/interview-questions"},"https://www.toptal.com/hadoop/interview-questions")))}u.isMDXComponent=!0},305547:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/Technologies-Apache-Apache-Hadoop-image1-caa6305e17a0d74f7902c5c4db9545c4.jpg"},604268:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/Technologies-Apache-Apache-Hadoop-image2-d2a2c140748b0edac529bda4946fa24b.jpg"},976768:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/Technologies-Apache-Apache-Hadoop-image3-7055460ef8e0a6632e267603df8a41b0.jpg"},939281:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/Technologies-Apache-Apache-Hadoop-image4-f33562c8755cee77b99bdc5ab8c63fa2.jpg"},78555:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/Technologies-Apache-Apache-Hadoop-image5-429bd2fec044244ab25d25198fde3ce9.jpg"},42993:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/Technologies-Apache-Apache-Hadoop-image6-9dad0683d0965077bbd116f8f4bf7cc4.jpg"},658097:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/Technologies-Apache-Apache-Hadoop-image7-15f73e667926cdaceb7cfbd66ff69397.jpg"}}]);