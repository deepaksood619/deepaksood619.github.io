"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[40320],{603905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>d});var a=r(667294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var s=a.createContext({}),p=function(e){var t=a.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(r),h=n,d=u["".concat(s,".").concat(h)]||u[h]||m[h]||o;return r?a.createElement(d,i(i({ref:t},c),{},{components:r})):a.createElement(d,i({ref:t},c))}));function d(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:n,i[1]=l;for(var p=2;p<o;p++)i[p]=r[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}h.displayName="MDXCreateElement"},821704:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=r(487462),n=(r(667294),r(603905));const o={},i="Others",l={unversionedId:"ai/move-37/others",id:"ai/move-37/others",title:"Others",description:"MCMC - Markov Chain Monte Carlo",source:"@site/docs/ai/move-37/others.md",sourceDirName:"ai/move-37",slug:"/ai/move-37/others",permalink:"/ai/move-37/others",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/ai/move-37/others.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Open AI Gym",permalink:"/ai/move-37/open-ai-gym"},next:{title:"Q-Learning Algorithms",permalink:"/ai/move-37/q-learning-algorithms"}},s={},p=[{value:"AlphaGO",id:"alphago",level:2},{value:"Deep Q Neural Network",id:"deep-q-neural-network",level:2},{value:"Asynchronous Actor-Critic Agent",id:"asynchronous-actor-critic-agent",level:2}],c={toc:p},u="wrapper";function m(e){let{components:t,...r}=e;return(0,n.kt)(u,(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"others"},"Others"),(0,n.kt)("p",null,"MCMC - Markov Chain Monte Carlo"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://skymind.ai/wiki/markov-chain-monte-carlo"},"https://skymind.ai/wiki/markov-chain-monte-carlo")),(0,n.kt)("p",null,"Bayesian approach"),(0,n.kt)("p",null,"In the Bayesian approach to decision-making, you first start with the prior, this is what your beliefs are, then as data comes in, you incorporate that data to update these priors to get the posterior."),(0,n.kt)("p",null,"Bayesian Model"),(0,n.kt)("p",null,"A Bayesian model is a statistical model where you use probability to represent all uncertainty within the model."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://www.toptal.com/algorithms/metropolis-hastings-bayesian-inference"},"https://www.toptal.com/algorithms/metropolis-hastings-bayesian-inference")),(0,n.kt)("h2",{id:"alphago"},"AlphaGO"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Policy Network")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Value Network")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Tree Search"))),(0,n.kt)("p",null,"Q-Learning: an algorithm which produces a Q-table that an agent uses to find the best action to take given a state."),(0,n.kt)("h2",{id:"deep-q-neural-network"},"Deep Q Neural Network"),(0,n.kt)("p",null,"A Neural Network that takes a state and approximates Q-values for each action based on that state."),(0,n.kt)("h2",{id:"asynchronous-actor-critic-agent"},"Asynchronous Actor-Critic Agent"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"},"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2")))}m.isMDXComponent=!0}}]);