"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[60145],{603905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>g});var a=r(667294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},i=Object.keys(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var l=a.createContext({}),m=function(e){var t=a.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},c=function(e){var t=m(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=m(r),u=n,g=d["".concat(l,".").concat(u)]||d[u]||p[u]||i;return r?a.createElement(g,o(o({ref:t},c),{},{components:r})):a.createElement(g,o({ref:t},c))}));function g(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=r.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:n,o[1]=s;for(var m=2;m<i;m++)o[m]=r[m];return a.createElement.apply(null,o)}return a.createElement.apply(null,r)}u.displayName="MDXCreateElement"},695778:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>m});var a=r(487462),n=(r(667294),r(603905));const i={},o="Multi Armed Bandit (MAB)",s={unversionedId:"algorithms/advanced-algorithms/multi-armed-bandit-mab",id:"algorithms/advanced-algorithms/multi-armed-bandit-mab",title:"Multi Armed Bandit (MAB)",description:"The multi-armed bandit (MAB) is a classic problem in decision sciences. Effectively, it is one of optimal resource allocation under uncertainty.",source:"@site/docs/algorithms/advanced-algorithms/multi-armed-bandit-mab.md",sourceDirName:"algorithms/advanced-algorithms",slug:"/algorithms/advanced-algorithms/multi-armed-bandit-mab",permalink:"/algorithms/advanced-algorithms/multi-armed-bandit-mab",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/algorithms/advanced-algorithms/multi-armed-bandit-mab.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Linear Programming",permalink:"/algorithms/advanced-algorithms/linear-programming"},next:{title:"Others",permalink:"/algorithms/advanced-algorithms/others"}},l={},m=[],c={toc:m},d="wrapper";function p(e){let{components:t,...r}=e;return(0,n.kt)(d,(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"multi-armed-bandit-mab"},"Multi Armed Bandit (MAB)"),(0,n.kt)("p",null,"The multi-armed bandit (MAB) is a classic problem in decision sciences. Effectively, it is one of optimal resource allocation under uncertainty."),(0,n.kt)("p",null,"The MAB problem is this; how do you most efficiently identify the best machine to play, whilst sufficiently exploring the many options in real time? This problem is not an exercise in theoretical abstraction, it is an analogy for a common problem that organisations face all the time, that is, how to identify the best message to present to customers (message is broadly defined here i.e. webpages, advertising, images) such that it maximises some business objective (e.g. clickthrough rate, signups)."),(0,n.kt)("p",null,"Bandit algorithms or samplers, are a means of testing and optimising variant allocation quickly."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Thompson sampling (TS)"),(0,n.kt)("li",{parentName:"ul"},"epsilon-greedy algorithm")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50"},"https://towardsdatascience.com/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50")))}p.isMDXComponent=!0}}]);