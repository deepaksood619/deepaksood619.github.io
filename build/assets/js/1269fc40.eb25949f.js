"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[79610],{603905:(e,a,n)=>{n.d(a,{Zo:()=>p,kt:()=>m});var t=n(667294);function o(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function r(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function k(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?r(Object(n),!0).forEach((function(a){o(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function s(e,a){if(null==e)return{};var n,t,o=function(e,a){if(null==e)return{};var n,t,o={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||(o[n]=e[n]);return o}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=t.createContext({}),i=function(e){var a=t.useContext(c),n=a;return e&&(n="function"==typeof e?e(a):k(k({},a),e)),n},p=function(e){var a=i(e.components);return t.createElement(c.Provider,{value:a},e.children)},l="mdxType",f={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},d=t.forwardRef((function(e,a){var n=e.components,o=e.mdxType,r=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),l=i(n),d=o,m=l["".concat(c,".").concat(d)]||l[d]||f[d]||r;return n?t.createElement(m,k(k({ref:a},p),{},{components:n})):t.createElement(m,k({ref:a},p))}));function m(e,a){var n=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var r=n.length,k=new Array(r);k[0]=d;var s={};for(var c in a)hasOwnProperty.call(a,c)&&(s[c]=a[c]);s.originalType=e,s[l]="string"==typeof e?e:o,k[1]=s;for(var i=2;i<r;i++)k[i]=n[i];return t.createElement.apply(null,k)}return t.createElement.apply(null,n)}d.displayName="MDXCreateElement"},286446:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>c,contentTitle:()=>k,default:()=>f,frontMatter:()=>r,metadata:()=>s,toc:()=>i});var t=n(487462),o=(n(667294),n(603905));const r={},k="Kafka Commands",s={unversionedId:"technologies/kafka/kafka-commands",id:"technologies/kafka/kafka-commands",title:"Kafka Commands",description:"Configuration",source:"@site/docs/technologies/kafka/kafka-commands.md",sourceDirName:"technologies/kafka",slug:"/technologies/kafka/kafka-commands",permalink:"/technologies/kafka/kafka-commands",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/technologies/kafka/kafka-commands.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Kafka Architecture",permalink:"/technologies/kafka/kafka-architecture"},next:{title:"Kafka Connect",permalink:"/technologies/kafka/kafka-connect"}},c={},i=[{value:"Configuration",id:"configuration",level:2},{value:"CloudFormation template",id:"cloudformation-template",level:2},{value:"Kafka confluent single node client setup",id:"kafka-confluent-single-node-client-setup",level:2}],p={toc:i},l="wrapper";function f(e){let{components:a,...n}=e;return(0,o.kt)(l,(0,t.Z)({},p,n,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"kafka-commands"},"Kafka Commands"),(0,o.kt)("h2",{id:"configuration"},"Configuration"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION"},"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION")),(0,o.kt)("h2",{id:"cloudformation-template"},"CloudFormation template"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://aws-quickstart.s3.amazonaws.com/quickstart-confluent-kafka/templates/confluent-kafka.template"},"https://aws-quickstart.s3.amazonaws.com/quickstart-confluent-kafka/templates/confluent-kafka.template")),(0,o.kt)("h2",{id:"kafka-confluent-single-node-client-setup"},"Kafka confluent single node client setup"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://docs.confluent.io/current/installation/docker/docs/installation/single-node-client.html"},"https://docs.confluent.io/current/installation/docker/docs/installation/single-node-client.html")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'## Create docker network\ndocker network create confluent\n\n## Start Zookeeper\ndocker run -d --net=example-docker --name=zookeeper -e ZOOKEEPER_CLIENT_PORT=2181 confluentinc/cp-zookeeper:5.1.0\n\n## Start Confluent Kafka\ndocker run -d\n--net=example-docker\n--name=kafka -p 9092:9092\n-e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092\n-e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1\nconfluentinc/cp-kafka:5.1.0\n\n## Create topic\ndocker run\n--net=example-docker\n--rm confluentinc/cp-kafka:5.1.0\nkafka-topics --create --topic smap_telemetry_data --partitions 3 --replication-factor 1 --config retention.ms=-1\n--if-not-exists --zookeeper zookeeper1:2181,zookeeper2:2182,zookeeper3:2183\n\n## Alter topic\ndocker run\n--net=example-docker\n--rm confluentinc/cp-kafka:5.1.0\nkafka-topics --alter --topic smap_telemetry_data --partitions 3 -config retention.ms=-1 --zookeeper zookeeper1:2181,zookeeper2:2181,zookeeper3:2181\n\ndocker run\n--net=example-docker\n--rm confluentinc/cp-kafka:5.1.0\nkafka-topics --alter --topic iot_data --config retention.ms=-1 --zookeeper zookeeper1:2181,zookeeper2:2181,zookeeper3:2181\n\n## Delete topic\n# Topic is marked for deletion and if kafka is configured with KAFKA_DELETE_TOPIC_ENABLE:"true" then it is deleted\n\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\n\nkafka-topics --delete --topic _schemas --zookeeper zookeeper1:2181,zookeeper2:2182,zookeeper3:2183\n\n## Show all Topics\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\nkafka-topics --describe --zookeeper zookeeper1:2181,zookeeper2:2182,zookeeper3:2183\n\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\nkafka-topics --describe --topic smap_telemetry_data --zookeeper zookeeper1:2181,zookeeper2:2182,zookeeper3:2183\n\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\nkafka-topics --describe --topic smap_telemetry_data --zookeeper zookeeper1:2181,zookeeper2:2182,zookeeper3:2183\n\n## Start confluent kafka control center\ndocker run -d\n--name=control-center\n--net=example-docker\n--ulimit nofile=16384:16384\n-p 9021:9021\n-v /tmp/control-center/data:/var/lib/confluent-control-center\n-e CONTROL_CENTER_ZOOKEEPER_CONNECT=zookeeper:2181\n-e CONTROL_CENTER_BOOTSTRAP_SERVERS=kafka:9092\n-e CONTROL_CENTER_REPLICATION_FACTOR=1\n-e CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS=1\n-e CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS=1\n-e CONTROL_CENTER_STREAMS_NUM_STREAM_THREADS=2\n-e CONTROL_CENTER_CONNECT_CLUSTER=<http://kafka-connect:8082>\nconfluentinc/cp-enterprise-control-center:5.1.0\n\n## Create data\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\nbash -c "seq 42 | kafka-console-producer --request-required-acks 1\n--broker-list kafka1:19091,kafka2:19092,kafka3:19093 --topic smap_telemetry_data && echo \'Produced 42 messages.\'"\n\n## Receive data\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\nkafka-console-consumer --bootstrap-server kafka1:19091,kafka2:19092,kafka3:19093 --topic smap_telemetry_data --from-beginning\n\n# kafka.example.com - 9091,9092,9093\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\nkafka-console-consumer --bootstrap-server kafka.example.com:9091,kafka.example.com:9092,kafka.example.com:9093 --topic dev_smap_telemetry_data --from-beginning\n\n# consume druid_telemetry_data\ndocker run\n--net=example-docker\n--rm\nconfluentinc/cp-kafka:5.1.0\nkafka-console-consumer --bootstrap-server kafka.example.com:9091,kafka.example.com:9092,kafka.example.com:9093 --topic druid_telemetry_data --from-beginning\n\n## Show consumer group offsets\nkafka-consumer-groups --bootstrap-server kafka1:19091,kafka2:19092,kafka3:19093 --list\n\nkafka-consumer-groups --bootstrap-server ke-cp-kafka-headless:9092 --describe --group kafka_influx_republisher_group\n\nkafka-consumer-groups --bootstrap-server kafka1:19091,kafka2:19092,kafka3:19093 --describe --group kafka_druid_republisher_group\n\n## Kafka Configs\n## Describe a topic\n\nkafka-configs --bootstrap-server ke-cp-kafka-headless:9092 --entity-type brokers --entity-default --describe\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name smap_telemetry_data --describe\n\n## Add config\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name smap_telemetry_data --alter --add-config retention.ms=604800000\n\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name druid_telemetry_data --alter --add-config retention.ms=172800000\n\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name test_smap_telemetry_data --alter --add-config retention.ms=172800000\n\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name dev_druid_telemetry_data --alter --add-config retention.ms=172800000\n\n## Barebones Command\n\n## Installing and running\n\nbrew update\n\nbrew cask install caskroom/versions/java8\n\nbrew install kafka\n\n## Start zookeeper server\nzkserver start\n\nbin/zookeeper-server-start.sh config/zookeeper.properties &\n\n## Stop zookeeper server\nbin/zookeeper-server-stop.sh\n\n## Start kafka server\nsh /usr/local/Cellar/kafka/2.0.0/bin/kafka-server-start /usr/local/etc/kafka/server.properties\n\nbin/kafka-server-start.sh config/server.properties &\n\n## Stop kafka server\nbin/kafka-server-stop.sh\n\n## Create Topic\nsh /usr/local/Cellar/kafka/2.0.0/bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\n## List all topics\nsh /usr/local/Cellar/kafka/2.0.0/bin/kafka-topics --list --zookeeper localhost:2181\n\nbin/kafka-topics.sh --list --zookeeper localhost:2181\n\n## Start producer\nsh /usr/local/Cellar/kafka/2.0.0/bin/kafka-console-producer --broker-list localhost:9092 --topic test\n\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n\n## Start consumer\nsh /usr/local/Cellar/kafka/2.0.0/bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning\n\nbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning\n\n## RPI\nwget <http://mirrordirector.raspbian.org/raspbian/pool/main/libr/librdkafka/librdkafka-dev_0.9.3-1_armhf.deb>\n\nsudo dpkg -i librdkafka-dev_0.9.3-1_armhf.deb\n\nsudo apt-get install -f\n\nsudo apt-get install libstdc++6\n\n## Other commands\n- ./kafka-topics.sh --create --bootstrap-server my-cluster-kafka-brokers.kafka:9092 --replication-factor 2 --partitions 3 --topic test_bank_data --config compression.type="snappy"\n- ./kafka-topics --describe --topic _schemas4 --zookeeper localhost:2181\n- ./kafka-console-producer --broker-list localhost:9092 --topic test\n- ./kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning\n\n## Kafka Commands (inside docker container)\n\ncd /usr/bin\n\nkafka-acls\nkafka-broker-api-versions\nkafka-configs\nkafka-console-consumer\nkafka-console-producer\nkafka-consumer-groups\nkafka-consumer-perf-test\nkafka-delegation-tokens\nkafka-delete-records\nkafka-dump-log\nkafka-log-dirs\nkafka-mirror-maker\nkafka-preferred-replica-election\nkafka-producer-perf-test\nkafka-reassign-partitions\nkafka-replica-verification\nkafka-run-class\n\nbin/kafka-run-class.sh kafka.tools.DumpLogSegments --files /var/lib/kafka/data-0/kafka-log0/test/00000000000000000000.log --print-data-log | grep compresscodec\n\nkafka-server-start\nkafka-server-stop\nkafka-streams-application-reset\nkafka-topics\nkafka-verifiable-consumer\nkafka-verifiable-producer\n\n## Explore with kafka commands\n\n# kafka version\nkafka-topics --version\n\n# List all topics\nkafka-topics --zookeeper ke-cp-zookeeper-headless:2181 --list\n\n# Create the topic\nkafka-topics --zookeeper ke-cp-zookeeper-headless:2181 --topic telemetry_data --create --partitions 3 --replication-factor 1 --if-not-exists\n\n# Describe a topic\nkafka-topics --describe --topic smap_samhi --zookeeper ke-cp-zookeeper-headless:2181\n\nkafka-configs --bootstrap-server ke-cp-kafka-headless:9092 --entity-type brokers --entity-default --describe\n\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name telemetry_data --describe\n\n# Add config\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name telemetry_data --alter --add-config retention.ms=604800000\n\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name druid_telemetry_data --alter --add-config retention.ms=172800000\n\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name test_telemetry_data --alter --add-config retention.ms=172800000\n\nkafka-configs --zookeeper ke-cp-zookeeper-headless:2181 --entity-type topics --entity-name dev_druid_telemetry_data --alter --add-config retention.ms=172800000\n\n# Add partitions\nkafka-topics --zookeeper ke-cp-zookeeper-headless:2181 --alter --topic smap_samhi --partitions 3\n\n## sh add_partitions.sh\n\n# !/bin/bash\nVAL="$(kafka-topics --zookeeper ke-cp-zookeeper-headless:2181 --list | grep druid)"\n\nfor i in $VAL\ndo\nkafka-topics --zookeeper ke-cp-zookeeper-headless:2181 --alter --topic $i --partitions 3\ndone\n\n# Create a message\nMESSAGE="`date -u`"\n\n# Produce a test message to the topic\necho "$MESSAGE" | kafka-console-producer --broker-list ke-cp-kafka-headless:9092 --topic ke-topic\n\n# Consume a test message from the topic\nkafka-console-consumer --bootstrap-server ke-cp-kafka-headless:9092 --topic bench_data --max-messages 1\n\nkafka-console-consumer --bootstrap-server ke-cp-kafka-headless:9092 --topic test_telemetry_data\n\nkafka-console-consumer --bootstrap-server ke-cp-kafka.kafka:9092 --topic test_telemetry_data\n\nkafka-console-consumer --bootstrap-server ke-cp-kafka-external-0:31090,ke-cp-kafka-external-1:31091,ke-cp-kafka-external-2:31092 --topic test_telemetry_data\n\nkafka-console-consumer --bootstrap-server ke-cp-kafka-headless:9092 --topic telemetry_data\n\nkafka-console-consumer --bootstrap-server ke-cp-kafka-headless:9092 --topic druid_telemetry_data --from-beginning\n\n# consume first message from kafka topic\n./kafka-console-consumer --bootstrap-server ke-cp-kafka-headless:9092 --topic druid_telemetry_data_Samhi --from-beginning --max-messages 1\n\n# number of messages in a topic in apache kafka\n./kafka-run-class kafka.tools.GetOffsetShell --broker-list ke-cp-kafka-headless:9092 --topic druid_telemetry_data_Samhi --time -1 --offsets 1 | awk -F ":" \'{sum += $3} END {print sum}\'\n\n## Kafka Consumer Group\ncd /usr/bin\n- kafka-consumer-groups --bootstrap-server ke-cp-kafka-headless:9092 --list\n- ./kafka-consumer-groups --bootstrap-server ke-cp-kafka-headless:9092 --describe --group kafka_prod_to_staging --members --verbose\n- kafka-consumer-groups --bootstrap-server ke-cp-kafka-headless:9092 --describe --group kafka_prod_to_staging --offsets\n- kafka-consumer-groups --bootstrap-server ke-cp-kafka-headless:9092 --describe --group kafka_prod_to_staging --offsets --verbose\n- kafka-consumer-groups --bootstrap-server ke-cp-kafka-headless:9092 --delete --group kafka_archiver_consumer_group\n\n## Kafka Log Storage Directory\n\n/var/lib/kafka/data-0/kafka-log0/\n')),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://kafka.apache.org/quickstart"},"https://kafka.apache.org/quickstart")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://gist.github.com/sam95/d7aed31770883bd272728ad0483629d4"},"https://gist.github.com/sam95/d7aed31770883bd272728ad0483629d4")))}f.isMDXComponent=!0}}]);