"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[72834],{603905:(e,t,r)=>{r.d(t,{Zo:()=>l,kt:()=>f});var a=r(667294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function p(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var i=a.createContext({}),c=function(e){var t=a.useContext(i),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},l=function(e){var t=c(e.components);return a.createElement(i.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,i=e.parentName,l=p(e,["components","mdxType","originalType","parentName"]),m=c(r),u=n,f=m["".concat(i,".").concat(u)]||m[u]||d[u]||o;return r?a.createElement(f,s(s({ref:t},l),{},{components:r})):a.createElement(f,s({ref:t},l))}));function f(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,s=new Array(o);s[0]=u;var p={};for(var i in t)hasOwnProperty.call(t,i)&&(p[i]=t[i]);p.originalType=e,p[m]="string"==typeof e?e:n,s[1]=p;for(var c=2;c<o;c++)s[c]=r[c];return a.createElement.apply(null,s)}return a.createElement.apply(null,r)}u.displayName="MDXCreateElement"},73840:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>i,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>p,toc:()=>c});var a=r(487462),n=(r(667294),r(603905));const o={},s="Examples",p={unversionedId:"technologies/apache/apache-spark/examples",id:"technologies/apache/apache-spark/examples",title:"Examples",description:"",source:"@site/docs/technologies/apache/apache-spark/examples.md",sourceDirName:"technologies/apache/apache-spark",slug:"/technologies/apache/apache-spark/examples",permalink:"/technologies/apache/apache-spark/examples",draft:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/main/docs/technologies/apache/apache-spark/examples.md",tags:[],version:"current",lastUpdatedAt:1677955187,formattedLastUpdatedAt:"Mar 4, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Apache Spark",permalink:"/technologies/apache/apache-spark/"},next:{title:"Apache Spark",permalink:"/technologies/apache/apache-spark/intro"}},i={},c=[],l={toc:c},m="wrapper";function d(e){let{components:t,...r}=e;return(0,n.kt)(m,(0,a.Z)({},l,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"examples"},"Examples"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'# SQL table to parquet\n from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.jdbc("YOUR_MYSQL_JDBC_CONN_STRING",  "YOUR_TABLE", properties={"user": "YOUR_USER", "password": "YOUR_PASSWORD"})\ndf.write.parquet("YOUR_HDFS_FILE")\n\n# CSV to Parquet\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nif __name__ == "__main__":\n    sc = SparkContext(appName="CSV2Parquet")\n    sqlContext = SQLContext(sc)\n\n    schema = StructType([\n            StructField("col1", IntegerType(), True),\n            StructField("col2", IntegerType(), True),\n            StructField("col3", StringType(), True),\n            StructField("col4", StringType(), True),\n            StructField("col5", StringType(), True),\n            StructField("col6", DoubleType(), True)])\n\n    rdd = sc.textFile("/home/sarvesh/Desktop/input.csv").map(lambda line: line.split(","))\n    df = sqlContext.createDataFrame(rdd, schema)\n    df.write.parquet(\'/home/sarvesh/Desktop/input-parquet\')\n\n# Parquet to csv\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nif __name__ == "__main__":\n    sc = SparkContext(appName="Parquet2CSV")\n    sqlContext = SQLContext(sc)\nreaddf = sqlContext.read.parquet(\'/home/sarvesh/Desktop/submissions-parquet\')\n    readdf.rdd.map(tuple).map(lambda row: str(row [0]) + "," + str(row [1]) + ","+ str(row [2]) + ","+ str(row [3])+ ","+\n                              str(row [4])+","+ str(row [5])).saveAsTextFile("/home/sarvesh/Desktop/parquet-to-csv.csv")\n\n http://blogs.quovantis.com/how-to-convert-csv-to-parquet-files/\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName("Python Spark SQL basic example") \\\n    .config("spark.some.config.option", "some-value") \\\n    .getOrCreate()\n\ndf = spark.read.csv("/Users/deepak/Repositories/Python-Competitive-Programming/Experiments/creditcard.csv", header=True, sep=",", inferSchema=True)\n\ndf.write.parquet(\'/Users/deepak/Repositories/Python-Competitive-Programming/Experiments/output-parquet\')\n\ndata.count(), len(data.columns)\ndata.show(5)\ndata.printSchema()\ndata.select("Name","Platform","User_Score","User_Count").show(15, truncate=False)\ndata.describe(["User_Score","User_Count"]).show()\ndata.groupBy("Platform").count().orderBy("count", ascending=False).show(10)\n\ncondition1 = (data.User_Score.isNotNull()) | (data.User_Count.isNotNull())\ncondition2 = data.User_Score != "tbd"\ndata = data.filter(condition1).filter(condition2)\n\n https://towardsdatascience.com/pyspark-import-any-data-f2856cda45fd\n\nGlue Transformation from Aurora DB to Parquet in s3\n import sys\n from awsglue.transforms import *\n from awsglue.utils import getResolvedOptions\n from pyspark.context import SparkContext\n from awsglue.context import GlueContext\n from awsglue.job import Job\n\n args = getResolvedOptions(sys.argv, [\'JOB_NAME\'])\n\n sc = SparkContext()\n glueContext = GlueContext(sc)\n spark = glueContext.spark_session\n job = Job(glueContext)\n job.init(args [\'JOB_NAME\'], args)\n\n datasource0 = glueContext.create_dynamic_frame.from_catalog(database = "aurora", table_name = "aurorasttash_website_live_userdevicesms_old", transformation_ctx = "datasource0")\n # datasource0 = glueContext.create_dynamic_frame_from_options("s3", {\'paths\': ["s3://example-migration-data/test"], \'recurse\':True, \'groupFiles\': \'inPartition\', \'groupSize\': \'104857600\'}, format="json")\n\n applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("device_id", "string", "device_id", "string"), ("sender", "string", "sender", "string"), ("message_type", "string", "message_type", "string"), ("id", "int", "id", "int"), ("customer_id", "int", "customer_id", "int"), ("message", "string", "message", "string"), ("create_date", "timestamp", "create_date", "timestamp"), ("sms_time", "timestamp", "sms_time", "timestamp")], transformation_ctx = "applymapping1")\n\n resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = "make_struct", transformation_ctx = "resolvechoice2")\n\n dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = "dropnullfields3")\n # datasource_df = dropnullfields3.repartition(2)\n datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = "s3", connection_options = {"path": "s3://example-migration-data/glue/user_device_sms_old"}, format = "parquet", transformation_ctx = "datasink4")\n job.commit()\n\nPySpark Tutorial\n # File location and type\n file_location = "/FileStore/tables/game_skater_stats.csv"\n file_type = "csv"\n\n # CSV options\n infer_schema = "true"\n first_row_is_header = "true"\n delimiter = ","\n\n # The applied options are for CSV files. For other file types, these will be ignored.\n df = spark.read.format(file_type) \\\n   .option("inferSchema", infer_schema) \\\n   .option("header", first_row_is_header) \\\n   .option("sep", delimiter) \\\n   .load(file_location)\n\n display(df)\n\n # how to take the dataframe from the past snippet and save it as a parquet file on DBFS, and then reload the dataframe from the saved parquet file.\n df.write.save(\'/FileStore/parquet/game_skater_stats\',\n                format=\'parquet\')\n df = spark.read.load("/FileStore/parquet/game_skater_stats")\n display(df)\n\n')),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873"},"https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873")))}d.isMDXComponent=!0}}]);