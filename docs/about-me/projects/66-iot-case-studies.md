# IoT Case Studies

## Zenatix Case Study

Client Name - Zenatix Solutions Pvt Ltd

### Overview

A leading IOT firm focussed on powered energy and asset management solutions for small and mid-sized buildings.

### Challenges

The IoT industry faces several challenges, including managing the vast amount of data generated by connected devices, real-time analysis and alerting, and efficient storage and archival of this data. In addition to the challenges related to big data and real-time analysis, the IoT industry often grapples with the complexity of maintaining cron jobs over multiple microservices, managing alerting, handling retries, and managing dependencies efficiently.

### Solution

After careful consideration of various solutions such as Vernemq, RabbitMQ, Cassandra, Influx, BigQuery etc the team opted for a combination of Kafka and Druid. Kafka efficiently handles the high volume of data, providing real-time streaming capabilities. Druid, on the other hand, was chosen for its powerful analytics and storage capabilities. The integration of Kafka and Druid enabled the organization to overcome challenges related to big data, real-time analysis, and data storage, providing a robust and scalable solution for their IoT data needs.

To address the cron challenges, we implemented Apache Airflow. Airflow provided a centralized platform for managing, scheduling, and monitoring cron jobs across various microservices. Its DAG (Directed Acyclic Graph) structure allowed for defining workflows, handling dependencies, and automating retries in case of failures. This streamlined approach significantly improved the management of cronjobs, alerting mechanisms, and the overall reliability of the IoT system.

### Outcomes

- Setup Kafka, Druid, Airflow in HA mode, increasing the resiliency and uptime of the system
- Migrated 10 TB of data from old databases to Druid
- Reduced query time from hours to seconds and minutes
- Real time processing of 1 gbps of streaming data
- Migrated around 100 crons from disparate sources to Airflow

### Tools Used

Kafka + Kafka Connectors + Kafka Manager, Druid + S3, Airflow, Kubernetes + Terraform

## Barco Case Study: Data Engineering Services in Healthcare

Client Name: Barco

### Overview

Barco is a prominent player in the healthcare sector, renowned for its innovative solutions. Their product, NexxisCare, focuses on revolutionizing healthcare delivery through advanced technology integration. In collaboration with Barco, our team provided comprehensive data engineering services to establish a robust IoT infrastructure for NexxisCare. The project aimed to streamline data collection from hospital equipment, enable real-time analytics, and ensure seamless data flow from edge devices to cloud servers.

### Challenges

- Data Management: Managing the influx of data generated by diverse hospital equipment posed a significant challenge.
- Real-time Analysis: Enabling real-time analytics capabilities to derive actionable insights from streaming data.
- Data Flow Optimization: Establishing efficient data flow mechanisms from edge devices to centralized servers and then to the cloud.
- System Integration: Integrating various components seamlessly while ensuring compatibility and scalability.

### Solution

After careful evaluation, our team devised a comprehensive solution leveraging industry-leading technologies tailored to Barco's requirements.

- Kafka Implementation: We deployed Kafka as the central streaming platform to handle the high volume of data generated by hospital equipment. Kafka facilitated real-time data ingestion, processing, and routing.
- Streaming Solution Setup: Our team configured the entire streaming solution, ensuring smooth data transmission from edge devices (hospital monitors and equipment) to centralized servers within the hospital premises.
- Integration with AWS Cloud: Data from hospital servers was seamlessly transmitted to AWS cloud for further analysis and storage, ensuring scalability and reliability.
- Logstash Deployment: Logstash was deployed to streamline data collection, parsing, and enrichment, ensuring data quality and compatibility across the pipeline.
- Telemetry Implementation: End-to-end telemetry was established, enabling comprehensive monitoring and management of data flow, ensuring system reliability and performance.

### Outcomes

- Efficient Data Flow: Setup of Kafka and streaming solution ensured efficient data flow from edge devices to AWS cloud, enhancing data accessibility and reliability.
- Real-time Analytics: Enabled real-time analytics capabilities, empowering healthcare professionals with timely insights for improved decision-making.
- Scalability and Resilience: The implemented solution ensured scalability to accommodate future data growth and resilience to handle high volumes of streaming data without compromising performance.
- Enhanced Data Quality: Logstash deployment improved data quality through effective parsing and enrichment, ensuring compatibility and consistency across the pipeline.

### Tools Used

- Kafka
- Logstash
- AWS Cloud Services
- Kubernetes
- Terraform

By leveraging cutting-edge technologies and expertise in data engineering, our team successfully addressed Barco's challenges, providing a robust IoT infrastructure for NexxisCare, ultimately enhancing healthcare delivery and patient outcomes.
