# JAX

- J - Just-in-time
- A - Autograd
- X - XLA - Accelerated Linear Algebra

JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning.

JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.

JAX a library for array-oriented numerical computation (_à la_ [NumPy](https://numpy.org/)), with automatic differentiation and JIT compilation to enable high-performance machine learning research.

- JAX provides a unified NumPy-like interface to computations that run on CPU, GPU, or TPU, in local or distributed settings.
- JAX features built-in Just-In-Time (JIT) compilation via [Open XLA](https://github.com/openxla), an open-source machine learning compiler ecosystem.
- JAX functions support efficient evaluation of gradients via its automatic differentiation transformations.
- JAX functions can be automatically vectorized to efficiently map them over arrays representing batches of inputs.

## Links

- [GitHub - google/jax: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more](https://github.com/google/jax)
- [JAX Quickstart — JAX documentation](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)
- [JAX in 100 Seconds](https://youtu.be/_0D5lXDjNpw)
- [Google JAX - Wikipedia](https://en.wikipedia.org/wiki/Google_JAX)
- [JAX Guide | Kaggle](https://www.kaggle.com/learn-guide/jax)
- [neural-network-and-data-loading.ipynb - Colab](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)
- [What is JAX? - YouTube](https://www.youtube.com/watch?v=uySOfXq-II0)
