# Roadmap

![complete roadmap to prepare for deep learning](../../media/Screenshot%202024-09-20%20at%2011.18.50%20PM.jpg)

- Foundational - Introduction to Neural Network, Loss Function, Optimizers - Gradient Descent, SGD, Adagrad, RMSProp, Adam
	- Everyone is using Adam optimizer, since it is able to change the momentum i.e. the learning rate as your training is going on
- Activation function - ReLU, Sigmoid, Tanh
- Geoffrey Hinton - inventor of backpropogation algorithm
- Inputs, weights, bias

### Artificial Neural Network (ANN)

- Weight Initialization
- Hyper parameter tuning
- How to decide, how many number of hidden layers will be there.
- How to decide on number of neurons should I take in the hidden layer
- Keras Tuner
- Auto Keras

### Convolutional Neural Network (CNN)

- Convolution
- Image + Video
- Filters, Strides, Layers
- Transfer Learning

### Recurrent Neural Network (RNN)

- NLP
- Sequence to sequence data
	- Sentence
	- Sales Forecasting
- Neural language translation
- HuggingFace, Ktrain

[Complete Road Map To Prepare For Deep LearningðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ - YouTube](https://www.youtube.com/watch?v=9jA0KjS7V_c&ab_channel=KrishNaik)
