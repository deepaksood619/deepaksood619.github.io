"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[90064],{720322:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"technologies/apache-airflow/dags-example","title":"Dags Example","description":"https://airflow.apache.org/docs/apache-airflow/1.10.12/api/airflow/contrib/operators/kubernetespod_operator/index.html","source":"@site/docs/technologies/apache-airflow/dags-example.md","sourceDirName":"technologies/apache-airflow","slug":"/technologies/apache-airflow/dags-example","permalink":"/technologies/apache-airflow/dags-example","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache-airflow/dags-example.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1734022610000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Concepts","permalink":"/technologies/apache-airflow/concepts"},"next":{"title":"DBT (Data Build Tool)","permalink":"/technologies/apache-airflow/dbt"}}');var o=a(474848),r=a(28453);const s={},i="Dags Example",l={},c=[{value:"Maintenance Dags",id:"maintenance-dags",level:2},{value:"DAG for stress testing",id:"dag-for-stress-testing",level:2},{value:"Example Standard DAG",id:"example-standard-dag",level:2},{value:"Python Operator",id:"python-operator",level:2},{value:"Others",id:"others",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"dags-example",children:"Dags Example"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://airflow.apache.org/docs/apache-airflow/1.10.12/_api/airflow/contrib/operators/kubernetes_pod_operator/index.html",children:"https://airflow.apache.org/docs/apache-airflow/1.10.12/_api/airflow/contrib/operators/kubernetes_pod_operator/index.html"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"execution_timeout(datetime.timedelta) -- max time allowed for the execution of this task instance, if it goes beyond it will raise and fail (used of instance)"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"dagrun_timeout(datetime.timedelta) -- specify how long a Dag Run should be up before timing out / failing, so that new DagRuns can be created (used for whole dag)"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Weekly cron - days_ago(1) start date can't keep in the memory the actual start date and we need to actually have more than one week difference from start date to trigger job"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.code,{children:'schedule_interval="@daily"'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example 1\nfrom datetime import datetime, timedelta\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\nargs = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(year=2019, month=9, day=11),\n    'email': ['username@example.com'],\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=15),\n    'catchup': False,\n}\n\ndag = DAG(\n    dag_id='cache_metrics_readings_dag',\n    default_args=args,\n    schedule_interval='0 */3 * * *',\n max_active_runs=1,\n)\n\ncache_metrics_readings = BashOperator(\n    task_id='cache_metrics_readings',\n    bash_command='python /root/example/manage.py cache_metrics_readings',\n    dag=dag,\n)\n\nif __name__ == '__main__':\n    dag.cli()\n\n# Example 2 Kubernetes Pod Operator\nfrom airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.contrib.operators.kubernetes_pod_operator import KubernetesPodOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(year=2019, month=9, day=11),\n    'email': ['username@example.com'],\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    'kubernetes_sample',\n    default_args=default_args,\n    schedule_interval=\"*/1 * * * *\",\n max_active_runs=1,)\n\nstart = DummyOperator(task_id='run_this_first', dag=dag)\n\npassing = KubernetesPodOperator(namespace='default',\n                                image=\"Python:3.6\",\n                                cmds=[\"Python\", \"-c\"],\n                                arguments=[\"print('hello world')\"],\n                                labels={\"foo\": \"bar\"},\n                                name=\"passing-test\",\n                                task_id=\"passing-task\",\n                                get_logs=True,\n                                dag=dag\n                                )\n\nfailing = KubernetesPodOperator(namespace='default',\n                                image=\"ubuntu:1604\",\n                                cmds=[\"Python\", \"-c\"],\n                                arguments=[\"print('hello world')\"],\n                                labels={\"foo\": \"bar\"},\n                                name=\"fail\",\n                                task_id=\"failing-task\",\n                                get_logs=True,\n                                dag=dag\n                                )\n\npassing.set_upstream(start)\nfailing.set_upstream(start)\n\n"})}),"\n",(0,o.jsx)(n.h2,{id:"maintenance-dags",children:"Maintenance Dags"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["clear-missing-dags","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A maintenance workflow that you can deploy into Airflow to periodically clean out entries in the DAG table of which there is no longer a corresponding Python File for it. This ensures that the DAG table doesn't have needless items in it and that the Airflow Web Server displays only those available DAGs."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["db-cleanup","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A maintenance workflow that you can deploy into Airflow to periodically clean out the DagRun, TaskInstance, Log, XCom, Job DB and SlaMiss entries to avoid having too much data in your Airflow MetaStore."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["kill-halted-tasks","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A maintenance workflow that you can deploy into Airflow to periodically kill off tasks that are running in the background that don't correspond to a running task in the DB."}),"\n",(0,o.jsx)(n.li,{children:"This is useful because when you kill off a DAG Run or Task through the Airflow Web Server, the task still runs in the background on one of the executors until the task is complete."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["log-cleanup","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A maintenance workflow that you can deploy into Airflow to periodically clean out the task logs to avoid those getting too big."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["delete-broken-dags","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A maintenance workflow that you can deploy into Airflow to periodically delete DAG files and clean out entries in the ImportError table for DAGs which Airflow cannot parse or import properly. This ensures that the ImportError table is cleaned every day."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://github.com/teamclairvoyant/airflow-maintenance-dags",children:"https://github.com/teamclairvoyant/airflow-maintenance-dags"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://github.com/teamclairvoyant/airflow-maintenance-dags/tree/master/log-cleanup",children:"https://github.com/teamclairvoyant/airflow-maintenance-dags/tree/master/log-cleanup"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://github.com/teamclairvoyant/airflow-maintenance-dags/tree/master/db-cleanup",children:"https://github.com/teamclairvoyant/airflow-maintenance-dags/tree/master/db-cleanup"})}),"\n",(0,o.jsx)(n.h2,{id:"dag-for-stress-testing",children:"DAG for stress testing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import datetime\nimport multiprocessing\nimport requests\n url = 'http://airflow.localhost:8080/api/experimental/dags/tutorial/dag_runs'\nheaders = {\n    'Cache-Control': 'no-cache',\n    'Content-Type': 'application/json',\n}\n def test(instance, runs):\n    for x in range(runs):\n        payload = {\n            'replace_microseconds': False,\n        }\n response = requests.post(url, headers=headers, json=payload)\n        print(response.text)\n        if not response.ok:\n            return\n def run_tests(threads):\n    thread_pool = []\n    start_batch = datetime.datetime.now()\n    print('Test: Calling Airflow example_trigger_controller_dag')\n    print('Running test with {} threads'.format(threads))\n for thread in range(threads):\n        thread_pool.append(multiprocessing.Process(target=test, args=(thread, 20)))\n        thread_pool [thread].start()\n for thread in range(threads):\n        thread_pool [thread].join()\n end_batch = datetime.datetime.now()\n    return \"total duration: {:7.5f}s\".format(\n        end_batch.timestamp() - start_batch.timestamp()\n    )\n if __name__ == '__main__':\n    result = run_tests(threads=2)\n    print(result)\n\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-standard-dag",children:"Example Standard DAG"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from airflow import DAG\nfrom airflow.contrib.kubernetes.volume import Volume\nfrom airflow.contrib.kubernetes.volume_mount import VolumeMount\nfrom airflow.contrib.operators.kubernetes_pod_operator import KubernetesPodOperator\nfrom airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.utils.dates import days_ago\n\ncron_path = "curl https://lms.stasheasy.com/cronjobEasy/Elev8AutomateMessageScript?stage_code=ELVKYC"\ncron_name = "lms_elev8_elvkyc"\nimage_name = "331916247734.dkr.ecr.ap-south-1.amazonaws.com/lms/prod:latest"\n\nschedule_interval = "0 4 * * *" / "@once" / "0 * * * Tue"\nlabels = {"project": "lms"}\n\nSLACK_CONN_ID = "monitoring"\nvolume_mount1 = VolumeMount(\n    "system",\n    mount_path="/var/www/html/application/config/system.php",\n    sub_path="system.php",\n    read_only=True,\n)\nvolume_mount2 = VolumeMount(\n    "database",\n    mount_path="/var/www/html/application/config/database.php",\n    sub_path="database.php",\n    read_only=True,\n)\nvolume_mount3 = VolumeMount(\n    "config",\n    mount_path="/var/www/html/application/config/config.php",\n    sub_path="config.php",\n    read_only=True,\n)\nvolume_mount4 = VolumeMount(\n    "index",\n    mount_path="/var/www/html/index.php",\n    sub_path="index.php",\n    read_only=True,\n)\nvolume1 = Volume(name="system", configs={"configMap": {"name": "lms-system-configmap"}})\nvolume2 = Volume(\n    name="database", configs={"configMap": {"name": "lms-database-configmap"}}\n)\nvolume3 = Volume(name="config", configs={"configMap": {"name": "lms-config-configmap"}})\nvolume4 = Volume(name="index", configs={"configMap": {"name": "lms-index-configmap"}})\n annotations = {\n     "vault.hashicorp.com/agent-inject": "true",\n     "vault.hashicorp.com/agent-pre-populate-only": "true",\n     "vault.hashicorp.com/agent-inject-secret-credentials.json": "crons/maintenance-crons",\n     "vault.hashicorp.com/role": "crons",\n     "vault.hashicorp.com/agent-inject-template-credentials.json": """{{ with secret "crons/maintenance-crons" }}\n     {\n     {{ range $k, $v := .Data.data }} "{{ $k }}": "{{ $v }}",\n     {{ end }} "dummy": "yes"\n     }\n     {{ end }}""",\n }\n\nenv_vars = {\n    "AWS_ACCESS_KEY_ID": "xxx",\n    "AWS_SECRET_ACCESS_KEY": "xxx",\n    "AWS_REGION_NAME": "ap-south-1",\n    "config": "{{ dag_run.conf }}",\n}\n\ndef task_fail_slack_alert(context):\nslack_hook = BaseHook.get_connection(SLACK_CONN_ID).password\nslack_msg = f"""\n        :red_circle: Task Failed.\n        *Task*: {context.get("task_instance").task_id}\n        *Dag*: {context.get("task_instance").dag_id}\n        *Execution Time*: {context.get("execution_date")}\n        *Owner*: <@U012YUW8QJK>\n        *Log Url*: {context.get("task_instance").log_url}\n        """\nfailed_alert = SlackWebhookOperator(\n    task_id="alerting",\n    http_conn_id=SLACK_CONN_ID,\n    webhook_token=slack_hook,\n    message=slack_msg,\n    channel="#cron",\n    username="airflow",\n)\nreturn failed_alert.execute(context=context)\ndefault_args = {\n"owner": "airflow",\n"depends_on_past": False,\n"start_date": days_ago(1),/"start_date": datetime(2021, 1, 19),(for weekly/monthly)\n"on_failure_callback": task_fail_slack_alert,\n}\n\ndag = DAG(\ncron_name,\ndefault_args=default_args,\ncatchup=False,\nschedule_interval=schedule_interval,\nmax_active_runs=1,\n)\n\npassing = KubernetesPodOperator(\nnamespace="crons",\nimage=image_name,\nstartup_timeout_seconds=240,\n    cmds=["/bin/bash", "-c", cron_path],\nlabels=labels,\nname=cron_name,\nimage_pull_policy="Always",\ntask_id=cron_name,\nannotations=annotations,\n    env_vars=env_vars,\n    get_logs=True,\ndag=dag,\nvolumes=[volume1, volume2, volume3, volume4],\nvolume_mounts=[volume_mount1, volume_mount2, volume_mount3, volume_mount4],\nis_delete_operator_pod=True,\n)\n\npassing\n\n'})}),"\n",(0,o.jsx)(n.h2,{id:"python-operator",children:"Python Operator"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:' # utils.py\n  import requests\n  from airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator\n  from airflow.hooks.base_hook import BaseHook\n\n  SLACK_GROUP = "monitoring"\n\n  def call_api(url, method, body):\n      """\n      call an api\n      :param url: url of the api\n      :param method: get/post method to use\n      :param body: url args in case of get and request body in case of post\n      :return: api response\n      """\n\n      if method == "GET":\n          response = requests.get(url, body)\n\n      elif method == "POST":\n          response = requests.post(url, body)\n\n      else:\n          raise NotImplementedError\n\n      if response.status_code == 200:\n          print(response.text)\n\n      else:\n          raise Exception(\n              f"API returned error :{response.status_code} \\ndetails: {response.text}"\n          )\n\n  def task_fail_slack_alert(owner, context):\n      slack_hook = BaseHook.get_connection(SLACK_GROUP).password\n      slack_msg = f"""\n              :red_circle: Task Failed.\n              *Task*: {context.get("task_instance").task_id}\n              *Dag*: {context.get("task_instance").dag_id}\n              *Execution Time*: {context.get("execution_date")}\n              *Owner*: {owner}\n              *Log Url*: {context.get("task_instance").log_url}\n              """\n\n      failed_alert = SlackWebhookOperator(\n          task_id="alerting",\n          http_conn_id=SLACK_GROUP,\n          webhook_token=slack_hook,\n          message=slack_msg,\n          channel="#cron",\n          username="airflow",\n      )\n\n      return failed_alert.execute(context=context)\n\n # dag.py\n  from functools import partial\n\n  from airflow import DAG\n  from airflow.operators.python_operator import PythonOperator\n  from airflow.utils.dates import days_ago\n\n  import utils\n\n  cron_name = "api-v1_softcell_pull_mode_0"\n  schedule_interval = "0 * * * *"\n\n  method = "GET"\n  url = "http://api-v1.prod/softcellCron/softcellPull?limit=50&mode=10&mode_value=0"\n  body = {}\n\n  owner = "<@U013CA4QJR3>"\n\n  default_args = {\n      "owner": "airflow",\n      "depends_on_past": False,\n      "start_date": days_ago(1),\n      "on_failure_callback": partial(utils.task_fail_slack_alert, owner),\n  }\n\n  dag = DAG(\n      cron_name,\n      default_args=default_args,\n      catchup=False,\n      schedule_interval=schedule_interval,\n   max_active_runs=1,\n  )\n\n  PythonOperator(\n      task_id=cron_name,\n      python_callable=utils.call_api,\n      op_kwargs={"url": url, "method": method, "body": body},\n      dag=dag,\n)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"others",children:"Others"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://github.com/EcZachly/microbatch-hourly-deduped-tutorial",children:"GitHub - EcZachly/microbatch-hourly-deduped-tutorial"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://github.com/EcZachly/little-book-of-pipelines",children:"GitHub - EcZachly/little-book-of-pipelines: This repository goes over how to handle massive variety in data engineering"})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>i});var t=a(296540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);