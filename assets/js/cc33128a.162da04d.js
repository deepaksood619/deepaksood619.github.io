"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[97851],{519375:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"ai/llm/design-patterns","title":"Design patterns","description":"In-context learning","source":"@site/docs/ai/llm/design-patterns.md","sourceDirName":"ai/llm","slug":"/ai/llm/design-patterns","permalink":"/ai/llm/design-patterns","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/design-patterns.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1748628854000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Code Generators","permalink":"/ai/llm/code-generators"},"next":{"title":"Ethics","permalink":"/ai/llm/ethics"}}');var a=n(474848),s=n(28453);const o={},r="Design patterns",l={},c=[{value:"In-context learning",id:"in-context-learning",level:2},{value:"LoRA (Low Rank Adaptation)",id:"lora-low-rank-adaptation",level:2},{value:"HSNW (Hierarchical Navigable Small Worlds)",id:"hsnw-hierarchical-navigable-small-worlds",level:2},{value:"FAISS (Facebook AI Similarity Search)",id:"faiss-facebook-ai-similarity-search",level:2},{value:"Context Window / Tokens",id:"context-window--tokens",level:2},{value:"Links",id:"links",level:2}];function h(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"design-patterns",children:"Design patterns"})}),"\n",(0,a.jsx)(t.h2,{id:"in-context-learning",children:"In-context learning"}),"\n",(0,a.jsx)(t.p,{children:'The core idea of in-context learning is to use LLMs off the shelf (i.e., without any fine-tuning), then control their behavior through clever prompting and conditioning on private "contextual" data.'}),"\n",(0,a.jsxs)(t.p,{children:["For example, say you\u2019re building a chatbot to answer questions about a set of legal documents. Taking a naive approach, you could paste all the documents into a ChatGPT or GPT-4 prompt, then ask a question about them at the end. This may work for very small datasets, but it doesn\u2019t scale. The biggest GPT-4 model can only process ~50 pages of input text, and performance (measured by inference time and accuracy) degrades badly as you approach this limit, called a ",(0,a.jsx)(t.strong,{children:"context window."})]}),"\n",(0,a.jsx)(t.p,{children:"In-context learning solves this problem with a clever trick: instead of sending all the documents with each LLM prompt, it sends only a handful of the most relevant documents. And the most relevant documents are determined with the help of . . . you guessed it . . . LLMs."}),"\n",(0,a.jsx)(t.p,{children:"At a very high level, the workflow can be divided into three stages:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Data preprocessing / embedding:"})," This stage involves storing private data (legal documents, in our example) to be retrieved later. Typically, the documents are broken into chunks, passed through an embedding model, then stored in a specialized database called a vector database."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Prompt construction / retrieval:"})," When a user submits a query (a legal question, in this case), the application constructs a series of prompts to submit to the language model. A compiled prompt typically combines a prompt template hard-coded by the developer; examples of valid outputs called few-shot examples; any necessary information retrieved from external APIs; and a set of relevant documents retrieved from the vector database."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Prompt execution / inference:"})," Once the prompts have been compiled, they are submitted to a pre-trained LLM for inference-including both proprietary model APIs and open-source or self-trained models. Some developers also add operational systems like logging, caching, and validation at this stage."]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"This looks like a lot of work, but it\u2019s usually easier than the alternative: training or fine-tuning the LLM itself. You don\u2019t need a specialized team of ML engineers to do in-context learning. You also don\u2019t need to host your own infrastructure or buy an expensive dedicated instance from OpenAI. This pattern effectively reduces an AI problem to a data engineering problem that most startups and big companies already know how to solve. It also tends to outperform fine-tuning for relatively small datasets-since a specific piece of information needs to occur at least ~10 times in the training set before an LLM will remember it through fine-tuning-and can incorporate new data in near real time."}),"\n",(0,a.jsxs)(t.p,{children:["One of the biggest questions around in-context learning is: What happens if we just change the underlying model to increase the context window? This is indeed possible, and it is an active area of research (e.g., see the ",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2302.10866",children:"Hyena paper"})," or this ",(0,a.jsx)(t.a,{href:"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c",children:"recent post"}),"). But this comes with a number of tradeoffs-primarily that cost and time of inference scale quadratically with the length of the prompt. Today, even linear scaling (the best theoretical outcome) would be cost-prohibitive for many applications. A single GPT-4 query over 10,000 pages would cost hundreds of dollars at current API rates. So, we don\u2019t expect wholesale changes to the stack based on expanded context windows"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/",children:"Emerging Architectures for LLM Applications | Andreessen Horowitz"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c",children:"The Secret Sauce behind 100K context window in LLMs: all tricks in one place | by Galina Alperovich | GoPenAI"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://aws.amazon.com/blogs/big-data/exploring-real-time-streaming-for-generative-ai-applications/",children:"Exploring real-time streaming for generative AI Applications | AWS Big Data Blog"})}),"\n",(0,a.jsx)(t.h2,{id:"lora-low-rank-adaptation",children:"LoRA (Low Rank Adaptation)"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation",children:"Mastering Low-Rank Adaptation (LoRA): Enhancing Large Language Models for Efficient Adaptation | DataCamp"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://huggingface.co/docs/diffusers/main/en/training/lora",children:"LoRA"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://github.com/microsoft/LoRA",children:'GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"'})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.youtube.com/watch?v=Bq9zqTJDsjg",children:"LoRA - Explained! - YouTube"})}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"hsnw-hierarchical-navigable-small-worlds",children:"HSNW (Hierarchical Navigable Small Worlds)"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.youtube.com/watch?v=H9Qdm8_JBAs",children:"System Design of ChatGPT | Mock interview @gkcs - YouTube"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.pinecone.io/learn/series/faiss/hnsw/",children:"Hierarchical Navigable Small Worlds (HNSW) | Pinecone"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://towardsdatascience.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37",children:"Similarity Search, Part 4: Hierarchical Navigable Small World (HNSW) | by Vyacheslav Efimov | Towards Data Science"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://medium.com/@vanshkharidia7/hsnw-intuitively-explained-the-best-algorithm-for-billion-scale-vector-search-540527e5278e",children:"HSNW Intuitively Explained: The Best Algorithm for Billion Scale Vector Search | by Vansh Kharidia | Medium"})}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"faiss-facebook-ai-similarity-search",children:"FAISS (Facebook AI Similarity Search)"}),"\n",(0,a.jsxs)(t.p,{children:["Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta's ",(0,a.jsx)(t.a,{href:"https://ai.facebook.com/",children:"Fundamental AI Research"})," group."]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://github.com/facebookresearch/faiss",children:"GitHub - facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors."})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.datacamp.com/blog/faiss-facebook-ai-similarity-search",children:"What Is Faiss (Facebook AI Similarity Search)?"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://ai.meta.com/tools/faiss/",children:"FAISS"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/",children:"Faiss: A library for efficient similarity search - Engineering at Meta"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://faiss.ai/index.html",children:"Welcome to Faiss Documentation \u2014 Faiss documentation"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://python.langchain.com/docs/integrations/vectorstores/faiss/",children:"Faiss | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"})}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"context-window--tokens",children:"Context Window / Tokens"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://github.com/NVIDIA/RULER",children:"GitHub - NVIDIA/RULER: This repo contains the source code for RULER: What\u2019s the Real Context Size of Your Long-Context Language Models?"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"RULER - Real Context Size of Your Long-Context Language Models"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.reddit.com/r/LocalLLaMA/comments/1eplndh/what_is_the_current_largest_context_window_for_an/",children:"What is the current largest context window for an open LLM? : r/LocalLLaMA"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://codingscape.com/blog/llms-with-largest-context-windows",children:"LLMs with largest context windows"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://www.thecloudgirl.dev/blog/rag-vs-large-context-window",children:"RAG vs Large Context Window LLMs: When to use which one? \u2014 The Cloud Girl"})}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"links",children:"Links"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsxs)(t.a,{href:"https://www.youtube.com/watch?v=aEA6X_IElpc",children:["AWS re",":Invent"," 2023 - Generative AI: Architectures and applications in depth (BOA308) - YouTube"]})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsxs)(t.a,{href:"https://www.youtube.com/watch?v=oBhP44wowoY",children:["AWS re",":Invent"," 2023 - SaaS meets AI/ML & generative AI: Multi-tenant patterns & strategies (SAS306) - YouTube"]})}),"\n"]})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var i=n(296540);const a={},s=i.createContext(a);function o(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);