"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[14570],{629223:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"cloud/azure/azure-event-hubs","title":"Azure Event Hubs","description":"Azure Event Hubs is a native data-streaming service in the cloud that can stream millions of events per second, with low latency, from any source to any destination. Event Hubs is compatible with Apache Kafka. It enables you to run existing Kafka workloads without any code changes.","source":"@site/docs/cloud/azure/azure-event-hubs.md","sourceDirName":"cloud/azure","slug":"/cloud/azure/azure-event-hubs","permalink":"/cloud/azure/azure-event-hubs","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/cloud/azure/azure-event-hubs.md","tags":[],"version":"current","lastUpdatedBy":"Deeapak Sood","lastUpdatedAt":1767333344000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Azure","permalink":"/cloud/azure/"},"next":{"title":"Azure SQL Databases","permalink":"/cloud/azure/azure-sql-databases"}}');var i=t(474848),a=t(28453);const r={},o="Azure Event Hubs",c={},l=[{value:"Key capabilities",id:"key-capabilities",level:2},{value:"Apache Kafka on Azure Event Hubs",id:"apache-kafka-on-azure-event-hubs",level:3},{value:"Schema Registry in Event Hubs",id:"schema-registry-in-event-hubs",level:3},{value:"Others",id:"others",level:3},{value:"Working",id:"working",level:2},{value:"Partition Key",id:"partition-key",level:2},{value:"Messages",id:"messages",level:2},{value:"ContentType",id:"contenttype",level:3},{value:"Reading Bulk Message",id:"reading-bulk-message",level:2},{value:"Key Parameters",id:"key-parameters",level:3},{value:"Optional Fine-Tuning",id:"optional-fine-tuning",level:3},{value:"Checkpointing / Commit",id:"checkpointing--commit",level:2},{value:"Automatic Checkpointing",id:"automatic-checkpointing",level:3},{value:"Manual Checkpointing",id:"manual-checkpointing",level:3},{value:"Comparison of Event hubs and Confluent Cloud",id:"comparison-of-event-hubs-and-confluent-cloud",level:2},{value:"Azure Event Hubs",id:"azure-event-hubs-1",level:3},{value:"Confluent Cloud",id:"confluent-cloud",level:3},{value:"Key Differences",id:"key-differences",level:3},{value:"Links",id:"links",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"azure-event-hubs",children:"Azure Event Hubs"})}),"\n",(0,i.jsx)(n.p,{children:"Azure Event Hubs is a native data-streaming service in the cloud that can stream millions of events per second, with low latency, from any source to any destination. Event Hubs is compatible with Apache Kafka. It enables you to run existing Kafka workloads without any code changes."}),"\n",(0,i.jsx)(n.p,{children:"Businesses can use Event Hubs to ingest and store streaming data. By using streaming data, businesses can gain valuable insights, drive real-time analytics, and respond to events as they happen. They can use this data to enhance their overall efficiency and customer experience."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about",children:"Azure Event Hubs: Data streaming platform with Kafka support - Azure Event Hubs | Microsoft Learn"})}),"\n",(0,i.jsx)(n.h2,{id:"key-capabilities",children:"Key capabilities"}),"\n",(0,i.jsx)(n.h3,{id:"apache-kafka-on-azure-event-hubs",children:"Apache Kafka on Azure Event Hubs"}),"\n",(0,i.jsx)(n.p,{children:"Event Hubs is a multi-protocol event streaming engine that natively supports Advanced Message Queuing Protocol (AMQP), Apache Kafka, and HTTPS protocols. Because it supports Apache Kafka, you can bring Kafka workloads to Event Hubs without making any code changes. You don't need to set up, configure, or manage your own Kafka clusters or use a Kafka-as-a-service offering that's not native to Azure."}),"\n",(0,i.jsx)(n.p,{children:"Event Hubs is built as a cloud native broker engine. For this reason, you can run Kafka workloads with better performance, better cost efficiency, and no operational overhead."}),"\n",(0,i.jsx)(n.h3,{id:"schema-registry-in-event-hubs",children:"Schema Registry in Event Hubs"}),"\n",(0,i.jsx)(n.p,{children:"Azure Schema Registry in Event Hubs provides a centralized repository for managing schemas of event streaming applications. Schema Registry comes free with every Event Hubs namespace. It integrates with your Kafka applications or Event Hubs SDK-based applications."}),"\n",(0,i.jsx)(n.p,{children:"Schema Registry ensures data compatibility and consistency across event producers and consumers. It enables schema evolution, validation, and governance and promotes efficient data exchange and interoperability."}),"\n",(0,i.jsx)(n.p,{children:"Schema Registry integrates with your existing Kafka applications and supports multiple schema formats, including Avro and JSON schemas."}),"\n",(0,i.jsx)(n.h3,{id:"others",children:"Others"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Real-time processing of streaming events with Stream Analytics"}),"\n",(0,i.jsx)(n.li,{children:"Explore streaming data with Azure Data Explorer"}),"\n",(0,i.jsx)(n.li,{children:"Azure functions, SDKs, and the Kafka ecosystem"}),"\n",(0,i.jsx)(n.li,{children:"Supports Local development with Event Hubs emulator"}),"\n",(0,i.jsx)(n.li,{children:"Flexible and cost-efficient event streaming"}),"\n",(0,i.jsx)(n.li,{children:"Scalable"}),"\n",(0,i.jsxs)(n.li,{children:["Supports streaming large messages","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["In most streaming scenarios, data is characterized by being lightweight, typically less than 1 MB, and having a high throughput. There are also instances where messages can't be divided into smaller segments. Event Hubs can ",(0,i.jsx)(n.strong,{children:"accommodate events up to 20 MB with self-serve scalable"}),"\xa0",(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-dedicated-overview",children:"dedicated clusters"}),"\xa0at no extra charge. This capability allows Event Hubs to handle a wide range of message sizes to ensure uninterrupted business operations."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Capture streaming data for long-term retention and batch analytics","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Capture your data in near real time in Azure Blob Storage or Azure Data Lake Storage for long-term retention or micro-batch processing. You can achieve this behavior on the same stream that you use for deriving real-time analytics. Setting up capture of event data is fast."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"working",children:"Working"}),"\n",(0,i.jsx)(n.p,{children:"Event Hubs provides a unified event streaming platform with a time-retention buffer, decoupling event producers from event consumers. The producer and consumer applications can perform large-scale data ingestion through multiple protocols."}),"\n",(0,i.jsx)(n.p,{children:"The following diagram shows the main components of Event Hubs architecture."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://learn.microsoft.com/en-us/azure/event-hubs/media/event-hubs-about/components.png",alt:"Diagram that shows the main components of Event Hubs."})}),"\n",(0,i.jsx)(n.p,{children:"The key functional components of Event Hubs include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Producer applications"}),": These applications can ingest data to an event hub by using Event Hubs SDKs or any Kafka producer client."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Namespace"}),": The management container for one or more event hubs or Kafka topics. The management tasks such as allocating streaming capacity, configuring network security, and enabling geo-disaster recovery are handled at the namespace level."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Event Hubs/Kafka topic"}),": In Event Hubs, you can organize events into an event hub or a Kafka topic. It's an append-only distributed log, which can comprise one or more partitions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Partitions"}),": They're used to scale an event hub. They're like lanes in a freeway. If you need more streaming throughput, you can add more partitions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consumer applications"}),": These applications can consume data by seeking through the event log and maintaining consumer offset. Consumers can be Kafka consumer clients or Event Hubs SDK clients."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consumer group"}),": This logical group of consumer instances reads data from an event hub or Kafka topic. It enables multiple consumers to read the same streaming data in an event hub independently at their own pace and with their own offsets."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about",children:"Azure Event Hubs: Data streaming platform with Kafka support - Azure Event Hubs | Microsoft Learn"})}),"\n",(0,i.jsx)(n.h2,{id:"partition-key",children:"Partition Key"}),"\n",(0,i.jsx)(n.p,{children:"You can use a partition key to map incoming event data into specific partitions for the purpose of data organization. The partition key is a sender-supplied value passed into an event hub. It's processed through a static hashing function, which creates the partition assignment. If you don't specify a partition key when publishing an event, a round-robin assignment is used."}),"\n",(0,i.jsx)(n.p,{children:"The event publisher is only aware of its partition key, not the partition to which the events are published. This decoupling of key and partition insulates the sender from needing to know too much about the downstream processing. A per-device or user unique identity makes a good partition key, but other attributes such as geography can also be used to group related events into a single partition."}),"\n",(0,i.jsxs)(n.p,{children:["Specifying a partition key enables keeping related events together in the same partition and in the exact order in which they arrived. The partition key is some string that is derived from your application context and identifies the interrelationship of the events. A sequence of events identified by a partition key is a\xa0",(0,i.jsx)(n.em,{children:"stream"}),". A partition is a multiplexed log store for many such streams."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-scalability#mapping-of-events-to-partitions",children:"Scalability - Azure Event Hubs - Azure Event Hubs | Microsoft Learn"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-features#mapping-of-events-to-partitions",children:"Overview of features - Azure Event Hubs | Microsoft Learn"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/answers/questions/1687898/partition-key-in-azure-event-hub-and-stream-analyt",children:"Partition Key in Azure Event Hub and Stream Analytics - Microsoft Q&A"})}),"\n",(0,i.jsx)(n.h2,{id:"messages",children:"Messages"}),"\n",(0,i.jsx)(n.h3,{id:"contenttype",children:"ContentType"}),"\n",(0,i.jsxs)(n.p,{children:["Azure Event Hubs allows you to specify a\xa0",(0,i.jsx)(n.code,{children:"ContentType"}),"\xa0for the data contained within an\xa0",(0,i.jsx)(n.code,{children:"EventData"}),"\xa0object.\xa0This property is a MIME type that describes the format of the event body, enabling consumers to understand and process the data appropriately."]}),"\n",(0,i.jsxs)(n.p,{children:["Key aspects of\xa0",(0,i.jsx)(n.code,{children:"ContentType"}),"\xa0in Azure Event Hubs:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"MIME Type:"})}),"\n",(0,i.jsxs)(n.p,{children:["The\xa0",(0,i.jsx)(n.code,{children:"ContentType"}),"\xa0property expects a MIME type string, such as\xa0",(0,i.jsx)(n.code,{children:"application/json"}),",\xa0",(0,i.jsx)(n.code,{children:"text/plain"}),",\xa0",(0,i.jsx)(n.code,{children:"application/xml"}),", or any other relevant MIME type that accurately describes the data format."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Application-Managed:"})}),"\n",(0,i.jsxs)(n.p,{children:["The\xa0",(0,i.jsx)(n.code,{children:"ContentType"}),"\xa0is set by the application sending the event and is intended to facilitate coordination between event producers and consumers.\xa0It is not automatically inferred or enforced by Event Hubs itself."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Consumer Guidance:"})}),"\n",(0,i.jsxs)(n.p,{children:["Consumers of the event stream can use the\xa0",(0,i.jsx)(n.code,{children:"ContentType"}),"\xa0to make informed decisions about how to deserialize and process the\xa0",(0,i.jsx)(n.code,{children:"EventBody"}),".\xa0For example, if the\xa0",(0,i.jsx)(n.code,{children:"ContentType"}),"\xa0is\xa0",(0,i.jsx)(n.code,{children:"application/json"}),", the consumer would parse the\xa0",(0,i.jsx)(n.code,{children:"EventBody"}),"\xa0as a JSON object."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Flexibility:"})}),"\n",(0,i.jsx)(n.p,{children:"While common MIME types like JSON or plain text are frequently used, you can define and use custom MIME types if your application requires a specific data format not covered by standard types."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"reading-bulk-message",children:"Reading Bulk Message"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Use ",(0,i.jsx)(n.strong,{children:"EventHubConsumerClient.receive_batch()"})," or manual loop using ",(0,i.jsx)(n.strong,{children:"receive()"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Determine the ",(0,i.jsx)(n.strong,{children:"available message count"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["If available < ",(0,i.jsx)(n.code,{children:"batch_size"})," \u2192 consume immediately.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["If available \u2265 ",(0,i.jsx)(n.code,{children:"batch_size"})," \u2192 consume in batches of size ",(0,i.jsx)(n.code,{children:"batch_size"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from azure.eventhub import EventHubConsumerClient\n\nconnection_str = "<EVENT_HUB_CONNECTION_STRING>"\nconsumer_group = "$Default"\neventhub_name = "<EVENT_HUB_NAME>"\n\nBATCH_SIZE = 100  # desired batch size\nclient = EventHubConsumerClient.from_connection_string(\n    conn_str=connection_str,\n    consumer_group=consumer_group,\n    eventhub_name=eventhub_name,\n)\n\ndef on_event_batch(partition_context, events):\n    if not events:\n        return\n\n    total_events = len(events)\n    if total_events <= BATCH_SIZE:\n        # Process all immediately\n        process_events(events)\n    else:\n        # Process in chunks\n        for i in range(0, total_events, BATCH_SIZE):\n            chunk = events[i:i + BATCH_SIZE]\n            process_events(chunk)\n\n    partition_context.update_checkpoint(events[-1])\n\ndef process_events(events):\n    print(f"Processing {len(events)} events...")\n    for e in events:\n        print(f"  Event: {e.body_as_str()}")\n\nwith client:\n    client.receive_batch(\n        on_event_batch=on_event_batch,\n        max_batch_size=BATCH_SIZE,\n        starting_position="-1"  # from start\n    )\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n    "version": "2.0",\n    "extensions": {\n        "eventHubs": {\n            "maxEventBatchSize" : 100,\n            "minEventBatchSize" : 25,\n            "maxWaitTime" : "00:05:00",\n            "batchCheckpointFrequency" : 1,\n            "prefetchCount" : 300,\n        }\n    }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://stackoverflow.com/questions/76105351/azure-event-hub-receive-events-as-a-batch-that-were-sent-to-hub-as-individual-m",children:"Azure Event Hub: Receive events as a batch that were sent to hub as individual messages? - Stack Overflow"})}),"\n",(0,i.jsx)(n.h3,{id:"key-parameters",children:"Key Parameters"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"receive_batch()"})})," allows you to fetch multiple events at once."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"max_batch_size"})})," defines the maximum number of events per fetch."]}),"\n",(0,i.jsxs)(n.li,{children:["If fewer messages are available, it immediately returns what exists after the ",(0,i.jsx)(n.strong,{children:"wait time"})," (default is small; configurable via ",(0,i.jsx)(n.code,{children:"max_wait_time"}),")."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"optional-fine-tuning",children:"Optional Fine-Tuning"}),"\n",(0,i.jsxs)(n.p,{children:["If you want true ",(0,i.jsx)(n.em,{children:"immediate reads even when less than batch size"}),", set:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"client.receive_batch(\n    on_event_batch=on_event_batch,\n    max_batch_size=BATCH_SIZE,\n    max_wait_time=0  # return immediately if messages exist\n)\n"})}),"\n",(0,i.jsx)(n.p,{children:"That ensures:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"If queue has < BATCH_SIZE \u2192 return them immediately."}),"\n",(0,i.jsx)(n.li,{children:"If queue has > BATCH_SIZE \u2192 returns exactly BATCH_SIZE and continues next poll."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://chatgpt.com/share/6911d341-db80-8008-9362-97c1f1aa3072",children:"ChatGPT - Azure Event Hub batching"})}),"\n",(0,i.jsx)(n.h2,{id:"checkpointing--commit",children:"Checkpointing / Commit"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Checkpoint too often \u2192 overhead."}),"\n",(0,i.jsx)(n.li,{children:"Checkpoint too rarely \u2192 risk of duplicate processing on restart."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"if events and partition_context.sequence_number % 1000 == 0:\n    partition_context.update_checkpoint(events[-1])\n"})}),"\n",(0,i.jsx)(n.p,{children:"This checkpoints every 1,000 messages."}),"\n",(0,i.jsx)(n.h3,{id:"automatic-checkpointing",children:"Automatic Checkpointing"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"When it happens automatically:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["You use ",(0,i.jsx)(n.strong,{children:"EventProcessorClient"})," (Java/.NET) or ",(0,i.jsx)(n.strong,{children:"EventHubConsumerClient with receive_batch() and checkpoint_store"})," (Python)."]}),"\n",(0,i.jsxs)(n.li,{children:["After each batch or event callback, the SDK automatically updates the checkpoint ",(0,i.jsx)(n.strong,{children:"for the last successfully processed event"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from azure.eventhub import EventHubConsumerClient\nfrom azure.eventhub.extensions.checkpointstoreblob import BlobCheckpointStore\n\ncheckpoint_store = BlobCheckpointStore.from_connection_string("<BLOB_CONN>", "<CONTAINER>")\nclient = EventHubConsumerClient.from_connection_string(\n    conn_str="<EH_CONN>",\n    consumer_group="$Default",\n    eventhub_name="<EH_NAME>",\n    checkpoint_store=checkpoint_store\n)\n\ndef on_event(partition_context, event):\n    print(f"Event: {event.body_as_str()}")\n    partition_context.update_checkpoint(event)  # <- triggers checkpoint\n\nwith client:\n    client.receive(on_event=on_event, starting_position="-1")\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Each event triggers a checkpoint update."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automatic"})," in the sense that once you call ",(0,i.jsx)(n.code,{children:"update_checkpoint()"}),", the SDK writes the offset into your blob store automatically."]}),"\n",(0,i.jsxs)(n.li,{children:["Ideal for ",(0,i.jsx)(n.strong,{children:"simple consumers"})," or ",(0,i.jsx)(n.strong,{children:"no at-least-once requirement"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"manual-checkpointing",children:"Manual Checkpointing"}),"\n",(0,i.jsxs)(n.p,{children:["You choose ",(0,i.jsx)(n.em,{children:"when"})," to mark progress."]}),"\n",(0,i.jsxs)(n.p,{children:["This gives control for ",(0,i.jsx)(n.strong,{children:"exactly-once"})," or ",(0,i.jsx)(n.strong,{children:"at-least-once"})," semantics and to avoid checkpointing failed batches."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def on_event_batch(partition_context, events):\n    print(f"Received {len(events)} events")\n\n    try:\n        process_events(events)\n        # checkpoint only after processing is successful\n        last_event = events[-1]\n        partition_context.update_checkpoint(last_event)\n    except Exception as e:\n        print(f"Processing failed: {e}, skipping checkpoint")\n'})}),"\n",(0,i.jsx)(n.p,{children:"You decide:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"How frequently"})," (every N messages, every minute, or end of batch)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"On success only"})," (avoid corrupt progress tracking on failure)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"What to store"})," (latest processed event only)."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"comparison-of-event-hubs-and-confluent-cloud",children:"Comparison of Event hubs and Confluent Cloud"}),"\n",(0,i.jsx)(n.h3,{id:"azure-event-hubs-1",children:"Azure Event Hubs"}),"\n",(0,i.jsx)(n.p,{children:"Azure Event Hubs is a fully managed, real-time data ingestion service designed for large-scale data streaming. It offers seamless integration with the Azure ecosystem, making it ideal for applications within the Azure environment. Event Hubs supports some Kafka APIs, allowing Kafka clients to interact with them without significant modifications."}),"\n",(0,i.jsx)(n.h3,{id:"confluent-cloud",children:"Confluent Cloud"}),"\n",(0,i.jsx)(n.p,{children:"Confluent Cloud is a fully managed data streaming platform built on Apache Kafka. It offers a comprehensive suite of tools for data streaming, including stream processing with Apache Flink and data governance features. Confluent Cloud integrates seamlessly with Azure services, providing unified security, management, and billing experience. It supports the full range of Kafka APIs, ensuring compatibility with existing Kafka clients and applications."}),"\n",(0,i.jsx)(n.h3,{id:"key-differences",children:"Key Differences"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Kafka Compatibility:"})," Event Hubs offers partial Kafka API compatibility, which may not support all Kafka features. In contrast, Confluent Cloud provides full Kafka API compatibility, ensuring seamless integration with existing Kafka clients."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalability and Features:"})," Event Hubs can elastically scale but has certain limitations in scalability and advanced features. Confluent Cloud offers a comprehensive suite of tools for data streaming, including stream processing and data governance features, making it suitable for complex enterprise architectures."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration with Azure:"})," Both services integrate with Azure, but Confluent Cloud offers a more unified security, management, and billing experience, as well as seamless integration with various Azure services."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Azure Events Hub / Azure Event Hub"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://chatgpt.com/share/685d9105-6864-8005-89e9-35f639cf420f",children:"ChatGPT - Epoch Receiver Conflict Fix"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://towardsdev.com/avoid-duplicate-event-processing-in-event-streaming-system-6c7efc151a40",children:"Avoid duplicate event processing in Event Streaming system | by Soumyadev Bhattacharyya | Towards Dev"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsxs)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-faq",children:[(0,i.jsx)(n.strong,{children:"Frequently asked questions - Azure Event Hubs"})," | Microsoft Learn"]})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-features",children:"Overview of features - Azure Event Hubs | Microsoft Learn"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/event-hubs/dynamically-add-partitions",children:"Dynamically add partitions to an event hub in Azure Event Hubs - Azure Event Hubs | Microsoft Learn"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(296540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);