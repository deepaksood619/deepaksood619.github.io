"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[11360],{633757:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"technologies/kafka/kafka-architecture","title":"Kafka Architecture","description":"Why is Kafka so Fast?","source":"@site/docs/technologies/kafka/kafka-architecture.md","sourceDirName":"technologies/kafka","slug":"/technologies/kafka/kafka-architecture","permalink":"/technologies/kafka/kafka-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/kafka/kafka-architecture.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1734554726000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Intro","permalink":"/technologies/kafka/intro"},"next":{"title":"Kafka Commands","permalink":"/technologies/kafka/kafka-commands"}}');var i=t(474848),r=t(28453);const a={},o="Kafka Architecture",c={},l=[{value:"Why is Kafka so Fast?",id:"why-is-kafka-so-fast",level:2},{value:"Zero-copy",id:"zero-copy",level:3},{value:"Kafka Zero Copy",id:"kafka-zero-copy",level:3},{value:"Other Optimizations - Messages batching &amp; Compression",id:"other-optimizations---messages-batching--compression",level:3},{value:"Concepts",id:"concepts",level:2},{value:"Key concepts",id:"key-concepts",level:2},{value:"Retention Policy",id:"retention-policy",level:2},{value:"Kafka Internals (Definitive Guide)",id:"kafka-internals-definitive-guide",level:2},{value:"Replication",id:"replication",level:3},{value:"Leader replica",id:"leader-replica",level:4},{value:"Follower replica",id:"follower-replica",level:4},{value:"Request Processing",id:"request-processing",level:3},{value:"The most common types of requests are",id:"the-most-common-types-of-requests-are",level:4},{value:"Produce requests",id:"produce-requests",level:5},{value:"Fetch requests",id:"fetch-requests",level:5},{value:"Physical Storage",id:"physical-storage",level:3},{value:"Kafka Rebalancing Protocol",id:"kafka-rebalancing-protocol",level:2},{value:"Compression",id:"compression",level:2},{value:"Others",id:"others",level:2},{value:"Others",id:"others-1",level:2},{value:"References",id:"references",level:2}];function d(e){const s={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"kafka-architecture",children:"Kafka Architecture"})}),"\n",(0,i.jsx)(s.h2,{id:"why-is-kafka-so-fast",children:"Why is Kafka so Fast?"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"The first one is Kafka\u2019s reliance on Sequential I/O."}),"\n",(0,i.jsx)(s.li,{children:"The second design choice that gives Kafka its performance advantage is its focus on efficiency: zero copy principle."}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Kafka relies heavily on the OS kernel to move data around quickly. It relies on the principals of ",(0,i.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Zero-copy",children:"Zero Copy"}),". Kafka enables you to batch data records into chunks. These batches of data can be seen end to end from Producer to file system (Kafka Topic Log) to the Consumer. Batching allows for more efficient data compression and reduces I/O latency. Kafka writes to the immutable commit log to the disk sequential; thus, avoids random disk access, slow disk seeking. Kafka provides horizontal Scale through sharding. It shards a Topic Log into hundreds potentially thousands of partitions to thousands of servers. This sharding allows Kafka to handle massive load."]}),"\n",(0,i.jsx)(s.p,{children:"Zero-copy means that Kafka sends messages from the file (or more likely, the Linux filesystem cache) directly to the network channel without any intermediate buffers."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"Why is Kafka Fast",src:t(390281).A+"",width:"999",height:"1203"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://www.youtube.com/channel/UCZgt6AzoyjslHTC9dz0UoTw/community?lb=UgkxKPCx8UjOik2iB9rKHDWqgzv_y59aCrDW",children:"ByteByteGo - YouTube"})}),"\n",(0,i.jsx)(s.h3,{id:"zero-copy",children:"Zero-copy"}),"\n",(0,i.jsxs)(s.p,{children:['"',(0,i.jsx)(s.strong,{children:"Zero-copy"}),'" describes computer operations in which the ',(0,i.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Central_processing_unit",children:"CPU"})," does not perform the task of copying data from one ",(0,i.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/RAM",children:"memory"})," area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network."]}),"\n",(0,i.jsx)(s.h3,{id:"kafka-zero-copy",children:"Kafka Zero Copy"}),"\n",(0,i.jsx)(s.p,{children:"Ideally, the data written to the log segment is written in protocol format. That is, what gets written to disk is exactly what gets sent over the wire. This allows for zero-copy reads. Let's take a look at how this otherwise works."}),"\n",(0,i.jsx)(s.p,{children:"When you read messages from the log, the kernel will attempt to pull the data from the page cache. If it's not there, it will be read from disk. The data is copied from disk to page cache, which all happens in kernel space. Next, the data is copied into the application (i.e. user space). This all happens with the read system call. Now the application writes the data out to a socket using send, which is going to copy it back into kernel space to a socket buffer before it's copied one last time to the NIC. All in all, we have four copies (including one from page cache) and two system calls."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image",src:t(704515).A+"",width:"999",height:"401"})}),"\n",(0,i.jsx)(s.p,{children:"However, if the data is already in wire format, we can bypass user space entirely using the send filesystem call, which will copy the data directly from the page cache to the NIC buffer - two copies (including one from page cache) and one system call. This turns out to be an important optimization, especially in garbage-collected languages since we're bringing less data into application memory. Zero-copy also reduces CPU cycles and memory bandwidth."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image",src:t(865990).A+"",width:"997",height:"320"})}),"\n",(0,i.jsx)(s.h3,{id:"other-optimizations---messages-batching--compression",children:"Other Optimizations - Messages batching & Compression"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Zero-copy",children:"https://en.wikipedia.org/wiki/Zero-copy"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics",children:"https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics"})}),"\n",(0,i.jsx)(s.h2,{id:"concepts",children:"Concepts"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image",src:t(810036).A+"",width:"1000",height:"584"})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Zookeeper"}),"\n",(0,i.jsx)(s.li,{children:"Producer"}),"\n",(0,i.jsx)(s.li,{children:"Consumer"}),"\n",(0,i.jsx)(s.li,{children:"Kafka cluster"}),"\n",(0,i.jsx)(s.li,{children:"Failovers"}),"\n",(0,i.jsx)(s.li,{children:"ISRs"}),"\n",(0,i.jsx)(s.li,{children:"Kafka disaster recovery"}),"\n",(0,i.jsx)(s.li,{children:"Topic"}),"\n",(0,i.jsx)(s.li,{children:"Topic Partition"}),"\n",(0,i.jsx)(s.li,{children:"Consumer Group"}),"\n",(0,i.jsx)(s.li,{children:"Offsets"}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Starting from version 0.8.2.0, the offsets committed by the consumers aren't saved in ",(0,i.jsx)(s.a,{href:"https://dzone.com/articles/an-introduction-to-zookeeper-1",children:"ZooKeeper"})," but on a partitioned and replicated topic named ",(0,i.jsx)(s.code,{children:"__consumer_offsets"}),", which is hosted on the Kafka brokers in the cluster."]}),"\n",(0,i.jsxs)(s.p,{children:["When a consumer commits some offsets (for different partitions), it sends a message to the broker to the ",(0,i.jsx)(s.code,{children:"__consumer_offsets"})," topic. The message has the following structure :"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-bash",children:"key = [group, topic, partition\nvalue = offset\n"})}),"\n",(0,i.jsx)(s.h2,{id:"key-concepts",children:"Key concepts"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{}),(0,i.jsx)(s.th,{})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"topic"}),(0,i.jsx)(s.td,{children:"Defines a logical name for producing and consuming records."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"partition"}),(0,i.jsx)(s.td,{children:"Defines a non-overlapping subset of records within a topic."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"offset"}),(0,i.jsx)(s.td,{children:"A unique sequential number assigned to each record within a topic partition."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"record"}),(0,i.jsx)(s.td,{children:"A record contains a key, a value, a timestamp, and a list of headers."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"broker"}),(0,i.jsx)(s.td,{children:"Servers where records are stored. Multiple brokers can be used to form a cluster."})]})]})]}),"\n",(0,i.jsx)(s.h2,{id:"retention-policy",children:"Retention Policy"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:(0,i.jsx)(s.strong,{children:"RETENTION POLICY"})}),(0,i.jsx)(s.th,{children:(0,i.jsx)(s.strong,{children:"MEANING"})})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"log.retention.hours"}),(0,i.jsx)(s.td,{children:"The number of hours to keep a record on the broker."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"log.retention.bytes"}),(0,i.jsx)(s.td,{children:"The maximum size of records retained in a partition."})]})]})]}),"\n",(0,i.jsx)(s.h2,{id:"kafka-internals-definitive-guide",children:"Kafka Internals (Definitive Guide)"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Cluster Membership","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Kafka uses Apache Zookeeper to maintain the list of brokers that are currently members of a cluster"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["Controller","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["The first broker that starts in the cluster becomes the controller by creating an ephemeral node in ZooKeeper called ",(0,i.jsx)(s.code,{children:"/controller"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"replication",children:"Replication"}),"\n",(0,i.jsx)(s.p,{children:"Two types of replicas"}),"\n",(0,i.jsx)(s.h4,{id:"leader-replica",children:"Leader replica"}),"\n",(0,i.jsx)(s.p,{children:"Each partition has a single replica designated as the leader. All produce and consume requests go through the leader, in order to guarantee consistency."}),"\n",(0,i.jsx)(s.h4,{id:"follower-replica",children:"Follower replica"}),"\n",(0,i.jsx)(s.p,{children:"All replicas for a partition that are not leaders are called followers. Followers don't serve client requests; their only job is to replicate messages from the leader and stay up-to-date with the most recent messages the leader has. In the event that a leader replica for a partition crashes, one of the follower replicas will be promoted to become the new leader for the partition."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Replicas that are consistently asking for the latest messages, is called ",(0,i.jsx)(s.em,{children:"in-sync replicas"}),". Only in-sync replicas are eligible to be elected as partition leaders in case the existing leader fails."]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"request-processing",children:"Request Processing"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Most of what a Kafka broker does is process requests sent to the partition leaders from clients, partition replicas, and the controller."}),"\n",(0,i.jsxs)(s.li,{children:["For each port the broker listens on, the broker runs an ",(0,i.jsx)(s.em,{children:"acceptor"})," thread that creates a connection and hands it over to a ",(0,i.jsx)(s.em,{children:"processor"})," thread for handling. The number of processor threads (also called ",(0,i.jsx)(s.em,{children:"network threads"}),") is configurable. The network threads are responsible for taking requests from client connections, placing them in a ",(0,i.jsx)(s.em,{children:"request queue"}),", and picking up responses from a ",(0,i.jsx)(s.em,{children:"response queue"})," and sending them back to clients."]}),"\n",(0,i.jsxs)(s.li,{children:["Once requests are placed on the request queue, ",(0,i.jsx)(s.em,{children:"IO threads"})," are responsible for picking them up and processing them."]}),"\n"]}),"\n",(0,i.jsx)(s.h4,{id:"the-most-common-types-of-requests-are",children:"The most common types of requests are"}),"\n",(0,i.jsx)(s.h5,{id:"produce-requests",children:"Produce requests"}),"\n",(0,i.jsx)(s.p,{children:"Sent by producers and contain messages the clients write to Kafka brokers."}),"\n",(0,i.jsx)(s.h5,{id:"fetch-requests",children:"Fetch requests"}),"\n",(0,i.jsx)(s.p,{children:"Sent by consumers and follower replicas when they read messages from Kafka brokers."}),"\n",(0,i.jsx)(s.h3,{id:"physical-storage",children:"Physical Storage"}),"\n",(0,i.jsx)(s.p,{children:"The basic storage unit of Kafka is a partition replica."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Partition allocation"}),"\n",(0,i.jsx)(s.li,{children:"File management"}),"\n",(0,i.jsx)(s.li,{children:"File format"}),"\n",(0,i.jsx)(s.li,{children:"Indexes"}),"\n",(0,i.jsx)(s.li,{children:"Compaction"}),"\n",(0,i.jsxs)(s.li,{children:["Deleted Events","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"tombstone"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"kafka-rebalancing-protocol",children:"Kafka Rebalancing Protocol"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"Eager Rebalancing Protocol (Stop the world rebalancing)"})}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Incremental cooperative rebalancing"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Incremental because the final desired state of rebalancing is reached in stages. A globally balanced final state does not have to be reached at the end of each round of rebalancing. A small number of consecutive rebalancing rounds can be used in order for the group of Kafka clients to converge to the desired state of balanced resources. In addition, you can configure a grace period to allow a departing member to return and regain its previously assigned resources."}),"\n",(0,i.jsx)(s.li,{children:"Cooperative because each process in the group is asked to voluntarily release resources that need to be redistributed. These resources are then made available for rescheduling given that the client that was asked to release them does so on time"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka",children:"https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://medium.com/streamthoughts/apache-kafka-rebalance-protocol-or-the-magic-behind-your-streams-applications-e94baf68e4f2",children:"https://medium.com/streamthoughts/apache-kafka-rebalance-protocol-or-the-magic-behind-your-streams-applications-e94baf68e4f2"})}),"\n",(0,i.jsx)(s.h2,{id:"compression",children:"Compression"}),"\n",(0,i.jsx)(s.p,{children:"compression.type: Specify the final compression type for a given topic. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer' which means retain the original compression codec set by the producer"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Type"}),": string"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Default"}),": producer"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Valid Values"}),": [uncompressed, zstd, lz4, snappy, gzip, producer]"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Server Default Property"}),": compression.type"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Importance"}),": medium"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"In Kafka compression, multiple messages are bundled and compressed. Then the compressed messages are turned into a special kind of message and appended to Kafka's log file. The reason to compress a batch of messages, rather than individual messages, is to increase compression efficiency, i.e., compressors work better with bigger data."}),"\n",(0,i.jsx)(s.p,{children:"There are tradeoffs with enabling compression that should be considered. Compression, of course, saves space needed for data storage. On the other hand, it consumes extra computing resources, namely CPU cycles and memory, to perform compression. The use of compression should be therefore decided with consideration of the balance of cost and benefit. To make the compression applicable to wider usages of Kafka, we set out to improve Kafka compression support by reducing the cost."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://www.confluent.io/blog/compression-in-apache-kafka-is-now-34-percent-faster",children:"https://www.confluent.io/blog/compression-in-apache-kafka-is-now-34-percent-faster"})}),"\n",(0,i.jsx)(s.h2,{id:"others",children:"Others"}),"\n",(0,i.jsxs)(s.p,{children:["Another caveat with Kafka is unclean leader elections. That is, if all replicas become unavailable, there are two options: choose the first replica to come back to life (not necessarily in the ISR) and elect this replica as leader (which could result in data loss) or wait for a replica in the ISR to come back to life and elect it as leader (which could result in prolonged unavailability). Initially, Kafka favoured availability by default by choosing the first strategy. If you preferred consistency, you needed to set ",(0,i.jsx)(s.code,{children:"unclean.leader.election.enable"})," to ",(0,i.jsx)(s.code,{children:"false"}),". However, as of 0.11, ",(0,i.jsx)(s.code,{children:"unclean.leader.election.enable"})," now defaults to this."]}),"\n",(0,i.jsx)(s.h2,{id:"others-1",children:"Others"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Idempotent Consumers"}),"\n",(0,i.jsx)(s.li,{children:"Idempotent Producers"}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"references",children:"References"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"http://cloudurable.com/blog/what-is-kafka/index.html",children:"http://cloudurable.com/blog/what-is-kafka/index.html"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03",children:"https://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03"})})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},390281:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/Pasted image 20240213012230-fe58daa64c3bca48d272dfa690a1702d.jpg"},810036:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/Technologies-Kafka-Kafka-Architecture-image1-f9c3459df4492e1f82065aef699896dd.jpg"},704515:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/Technologies-Kafka-Kafka-Architecture-image2-8a664a31a2ddcf0809cc2ee20bbad21e.jpg"},865990:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/Technologies-Kafka-Kafka-Architecture-image3-cd81da28ba7e25b3bf27ebfe34f80266.jpg"},28453:(e,s,t)=>{t.d(s,{R:()=>a,x:()=>o});var n=t(296540);const i={},r=n.createContext(i);function a(e){const s=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),n.createElement(r.Provider,{value:s},e.children)}}}]);