"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[98628],{586953:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var t=n(785893),s=n(511151);const a={},o="CV - Libraries / Tools",r={id:"ai/computer-vision-cv/cv-libraries-tools",title:"CV - Libraries / Tools",description:"SSD - Single Shot MultiBox Detector",source:"@site/docs/ai/computer-vision-cv/cv-libraries-tools.md",sourceDirName:"ai/computer-vision-cv",slug:"/ai/computer-vision-cv/cv-libraries-tools",permalink:"/ai/computer-vision-cv/cv-libraries-tools",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/computer-vision-cv/cv-libraries-tools.md",tags:[],version:"current",lastUpdatedAt:1726649448,formattedLastUpdatedAt:"Sep 18, 2024",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Convolutional Neural Network (CNN)",permalink:"/ai/computer-vision-cv/convolutional-neural-network-cnn"},next:{title:"Examples",permalink:"/ai/computer-vision-cv/examples"}},l={},c=[{value:"SSD - Single Shot MultiBox Detector",id:"ssd---single-shot-multibox-detector",level:2},{value:"OpenCV (CV2)",id:"opencv-cv2",level:2},{value:"Functions",id:"functions",level:3},{value:"MLKit Vision APIs",id:"mlkit-vision-apis",level:2},{value:"Key capabilities",id:"key-capabilities",level:3},{value:"Image Similarity API",id:"image-similarity-api",level:2},{value:"Darknet",id:"darknet",level:2},{value:"Albumentations",id:"albumentations",level:2}];function h(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h1,{id:"cv---libraries--tools",children:"CV - Libraries / Tools"}),"\n",(0,t.jsx)(i.h2,{id:"ssd---single-shot-multibox-detector",children:"SSD - Single Shot MultiBox Detector"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Single Shot:"})," this means that the tasks of object localization and classificationare done in asingleforward passof the network"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"MultiBox:"})," this is the name of a technique for bounding box regression developed by Szegedy et al."]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Detector:"})," The network is an object detector that also classifies those detected objects"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"opencv-cv2",children:"OpenCV (CV2)"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Most used computer vision library. Highly efficient. Facilitates real-time image processing."}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://www.youtube.com/watch?v=P4Z8_qe2Cu0",children:"https://www.youtube.com/watch?v=P4Z8_qe2Cu0"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://www.freecodecamp.org/news/opencv-full-course",children:"https://www.freecodecamp.org/news/opencv-full-course"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://www.freecodecamp.org/news/how-to-use-opencv-and-python-for-computer-vision-and-ai",children:"https://www.freecodecamp.org/news/how-to-use-opencv-and-python-for-computer-vision-and-ai"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://learnopencv.com",children:"https://learnopencv.com"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://opencv.org",children:"https://opencv.org"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.code,{children:"pip install opencv-python"})}),"\n",(0,t.jsx)(i.h3,{id:"functions",children:"Functions"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:"import cv2\n\ncv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\ncv2.Canny(blur_gray, low_threshold, high_threshold)\n\ncv2.GaussianBlur(gray,(kernel_size, kernel_size), 0)\n\ncv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)\n"})}),"\n",(0,t.jsx)(i.p,{children:"First off, rho and theta are the distance and angular resolution of our grid in Hough space. Remember that, in Hough space, we have a grid laid out along the (\u0398, \u03c1) axis. You need to specify rho in units of pixels and theta in units of radians."}),"\n",(0,t.jsx)(i.p,{children:"The threshold parameter specifies the minimum number of votes (intersections in a given grid cell) a candidate line needs to have to make it into the output."}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.code,{children:"min_line_length"})," is the minimum length of a line (in pixels) that you will accept in the output, and ",(0,t.jsx)(i.code,{children:"max_line_gap"})," is the maximum distance (again, in pixels) between segments that you will allow to be connected into a single line."]}),"\n",(0,t.jsx)(i.h2,{id:"mlkit-vision-apis",children:"MLKit Vision APIs"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Barcode scanning"}),"\n",(0,t.jsx)(i.li,{children:"Face detection"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"With ML Kit's face detection API, you can detect faces in an image, identify key facial features, and get the contours of detected faces. Note that the API detects faces, it does not recognize people."}),"\n",(0,t.jsx)(i.p,{children:"With face detection, you can get the information you need to perform tasks like embellishing selfies and portraits, or generating avatars from a user's photo. Because ML Kit can perform face detection in real time, you can use it in applications like video chat or games that respond to the player's expressions."}),"\n",(0,t.jsx)(i.h3,{id:"key-capabilities",children:"Key capabilities"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Recognize and locate facial features. Get the coordinates of the eyes, ears, cheeks, nose, and mouth of every face detected."}),"\n",(0,t.jsx)(i.li,{children:"Get the contours of facial features Get the contours of detected faces and their eyes, eyebrows, lips, and nose."}),"\n",(0,t.jsx)(i.li,{children:"Recognize facial expressions Determine whether a person is smiling or has their eyes closed."}),"\n",(0,t.jsx)(i.li,{children:"Track faces across video frames Get an identifier for each unique detected face. The identifier is consistent across invocations, so you can perform image manipulation on a particular person in a video stream."}),"\n",(0,t.jsx)(i.li,{children:"Process video frames in real time Face detection is performed on the device, and is fast enough to be used in real-time applications, such as video manipulation."}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://developers.google.com/ml-kit/vision/face-detection",children:"https://developers.google.com/ml-kit/vision/face-detection"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://github.com/ipazc/mtcnn",children:"https://github.com/ipazc/mtcnn"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsxs)(i.a,{href:"https://github.com/ageitgey/face_recognition",children:[(0,t.jsx)(i.strong,{children:"GitHub - ageitgey/face_recognition"}),": The world's simplest facial recognition api for Python and the command line"]})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv",children:"https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Image labeling"}),"\n",(0,t.jsx)(i.li,{children:"Object detection and tracking"}),"\n",(0,t.jsx)(i.li,{children:"Text recognition"}),"\n",(0,t.jsx)(i.li,{children:"Digital ink recognition"}),"\n",(0,t.jsx)(i.li,{children:"Pose detection"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"image-similarity-api",children:"Image Similarity API"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://deepai.org/machine-learning-model/image-similarity",children:"https://deepai.org/machine-learning-model/image-similarity"})}),"\n",(0,t.jsx)(i.h2,{id:"darknet",children:"Darknet"}),"\n",(0,t.jsx)(i.p,{children:"Convolutional Neural Networks"}),"\n",(0,t.jsx)(i.p,{children:"YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object Detection (Windows and Linux version of Darknet)"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://github.com/pjreddie/darknet",children:"https://github.com/pjreddie/darknet"})}),"\n",(0,t.jsx)(i.p,{children:"Pillow - images/Python Imaging Library"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://python-pillow.org",children:"https://python-pillow.org"})}),"\n",(0,t.jsx)(i.h2,{id:"albumentations",children:"Albumentations"}),"\n",(0,t.jsx)(i.p,{children:"Albumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data."}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["Albumentations\xa0",(0,t.jsx)(i.strong,{children:(0,t.jsx)(i.a,{href:"https://github.com/albumentations-team/albumentations?tab=readme-ov-file#i-want-to-use-albumentations-for-the-specific-task-such-as-classification-or-segmentation",children:"supports all common computer vision tasks"})}),"\xa0such as classification, semantic segmentation, instance segmentation, object detection, and pose estimation."]}),"\n",(0,t.jsxs)(i.li,{children:["The library provides\xa0",(0,t.jsx)(i.strong,{children:(0,t.jsx)(i.a,{href:"https://github.com/albumentations-team/albumentations?tab=readme-ov-file#a-simple-example",children:"a simple unified API"})}),"\xa0to work with all data types: images (RBG-images, grayscale images, multispectral images), segmentation masks, bounding boxes, and keypoints."]}),"\n",(0,t.jsxs)(i.li,{children:["The library contains\xa0",(0,t.jsx)(i.strong,{children:(0,t.jsx)(i.a,{href:"https://github.com/albumentations-team/albumentations?tab=readme-ov-file#list-of-augmentations",children:"more than 70 different augmentations"})}),"\xa0to generate new training samples from the existing data."]}),"\n",(0,t.jsxs)(i.li,{children:["Albumentations is\xa0",(0,t.jsx)(i.a,{href:"https://github.com/albumentations-team/albumentations?tab=readme-ov-file#benchmarking-results",children:(0,t.jsx)(i.strong,{children:"fast"})}),". We benchmark each new release to ensure that augmentations provide maximum speed."]}),"\n",(0,t.jsxs)(i.li,{children:["It\xa0",(0,t.jsx)(i.strong,{children:(0,t.jsx)(i.a,{href:"https://github.com/albumentations-team/albumentations?tab=readme-ov-file#i-want-to-know-how-to-use-albumentations-with-deep-learning-frameworks",children:"works with popular deep learning frameworks"})}),"\xa0such as PyTorch and TensorFlow. By the way, Albumentations is a part of the\xa0",(0,t.jsx)(i.a,{href:"https://pytorch.org/ecosystem/",children:"PyTorch ecosystem"}),"."]}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://github.com/albumentations-team/albumentations",children:"GitHub - albumentations-team/albumentations: Fast and flexible image augmentation library. Paper about the library: https://www.mdpi.com/2078-2489/11/2/125"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://albumentations.ai/",children:"Albumentations: fast and flexible image augmentations"})})]})}function d(e={}){const{wrapper:i}={...(0,s.a)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},511151:(e,i,n)=>{n.d(i,{Z:()=>r,a:()=>o});var t=n(667294);const s={},a=t.createContext(s);function o(e){const i=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);