"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[40799],{438722:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var s=e(785893),l=e(511151);const r={},t="Syllabus",o={id:"ai/move-37/syllabus",title:"Syllabus",description:"1. Markov Decision Processes",source:"@site/docs/ai/move-37/syllabus.md",sourceDirName:"ai/move-37",slug:"/ai/move-37/syllabus",permalink:"/ai/move-37/syllabus",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/syllabus.md",tags:[],version:"current",lastUpdatedAt:1678191863,formattedLastUpdatedAt:"Mar 7, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Reinforcement Learning",permalink:"/ai/move-37/reinforcement-learning"},next:{title:"NLP",permalink:"/ai/nlp/"}},c={},d=[];function a(n){const i={h1:"h1",li:"li",ol:"ol",p:"p",ul:"ul",...(0,l.a)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.h1,{id:"syllabus",children:"Syllabus"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Markov Decision Processes"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Introduction"}),"\n",(0,s.jsx)(i.li,{children:"Sensor Networks"}),"\n",(0,s.jsx)(i.li,{children:"Supply Chain Management"}),"\n",(0,s.jsx)(i.li,{children:"Energy Efficiency"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (Policy Functions)"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (Bellman Equation)"}),"\n",(0,s.jsx)(i.li,{children:"Markov Decision Processes"}),"\n",(0,s.jsx)(i.li,{children:"The Bellman Equations"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Dynamic Programming"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Route Planning"}),"\n",(0,s.jsx)(i.li,{children:"Options Pricing"}),"\n",(0,s.jsx)(i.li,{children:"Scheduling"}),"\n",(0,s.jsx)(i.li,{children:"Operating Systems"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (History of DP)"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (Value Iteration)"}),"\n",(0,s.jsx)(i.li,{children:"Dynamic Programming"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Monte Carlo Methods"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Medical Diagnosis"}),"\n",(0,s.jsx)(i.li,{children:"Network Routing Optimization"}),"\n",(0,s.jsx)(i.li,{children:"Physics Research"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (Exploration vs Exploitation)"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (Greedy Policies)"}),"\n",(0,s.jsx)(i.li,{children:"MC Prediction and MC Control"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Model Free Learning"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Delivery Management"}),"\n",(0,s.jsx)(i.li,{children:"Automated Trading"}),"\n",(0,s.jsx)(i.li,{children:"Backgammon"}),"\n",(0,s.jsx)(i.li,{children:"Dopamine in Neuroscience"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (SARSA)"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (Q Learning)"}),"\n",(0,s.jsx)(i.li,{children:"Temporal Difference Learning"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"RL in Continuous Spaces"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Self Driving Cars"}),"\n",(0,s.jsx)(i.li,{children:"Delivery Drones"}),"\n",(0,s.jsx)(i.li,{children:"Rescue Robots"}),"\n",(0,s.jsx)(i.li,{children:"Assembly Robots"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (Control Theory)"}),"\n",(0,s.jsx)(i.li,{children:"Midterm Assignment (Make a Bipedal Robot Walk )"}),"\n",(0,s.jsx)(i.li,{children:"Continuous Space Techniques"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Deep Reinforcement Learning"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Traffic Optimization"}),"\n",(0,s.jsx)(i.li,{children:"Gaming"}),"\n",(0,s.jsx)(i.li,{children:"Meta Learning"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (Deep Q Learning)"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (DQN Improvements)"}),"\n",(0,s.jsx)(i.li,{children:"The Evolution of Deep Q Learning"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Policy Based Methods"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Web System Configuration"}),"\n",(0,s.jsx)(i.li,{children:"Text Summarization"}),"\n",(0,s.jsx)(i.li,{children:"AI Assisted Design"}),"\n",(0,s.jsx)(i.li,{children:"Portfolio Optimization"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (Evolutionary Algorithms)"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (REINFORCE)"}),"\n",(0,s.jsx)(i.li,{children:"Stochastic Policy Search"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Policy Gradient Methods"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Dialogue Systems"}),"\n",(0,s.jsx)(i.li,{children:"Photo Editing"}),"\n",(0,s.jsx)(i.li,{children:"Language Translation"}),"\n",(0,s.jsx)(i.li,{children:"Tutoring Systems"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (Evolved Policy Gradients)"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (TRPO)"}),"\n",(0,s.jsx)(i.li,{children:"Generalized Advatange Estimation"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Actor Critic Methods"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Advanced Trading Techniques"}),"\n",(0,s.jsx)(i.li,{children:"Human-Machine Cooperation"}),"\n",(0,s.jsx)(i.li,{children:"Insurance Cost Analysis"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (Actor Critic Algorithms)"}),"\n",(0,s.jsx)(i.li,{children:"Homework Assignment (Bayesian Actor Critic)"}),"\n",(0,s.jsx)(i.li,{children:"Asynchronous Advantage Actor Critic"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Multi Agent RL"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Move 37"}),"\n",(0,s.jsx)(i.li,{children:"Transporation Networks"}),"\n",(0,s.jsx)(i.li,{children:"Decentralized Autonomous Organizations"}),"\n",(0,s.jsx)(i.li,{children:"The Future of AI"}),"\n",(0,s.jsx)(i.li,{children:"Reading Assignment (Cooperative Agents)"}),"\n",(0,s.jsx)(i.li,{children:"Inverse Reinforcement Learning"}),"\n",(0,s.jsx)(i.li,{children:"Final Project (Multi Agent Research Project)"}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:i}={...(0,l.a)(),...n.components};return i?(0,s.jsx)(i,{...n,children:(0,s.jsx)(a,{...n})}):a(n)}},511151:(n,i,e)=>{e.d(i,{Z:()=>o,a:()=>t});var s=e(667294);const l={},r=s.createContext(l);function t(n){const i=s.useContext(r);return s.useMemo((function(){return"function"==typeof n?n(i):{...i,...n}}),[i,n])}function o(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(l):n.components||l:t(n.components),s.createElement(r.Provider,{value:i},n.children)}}}]);