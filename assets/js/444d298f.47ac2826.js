"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[94434],{20426:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"technologies/kafka/kafkacat","title":"kafkacat","description":"https://github.com/edenhill/kafkacat","source":"@site/docs/technologies/kafka/kafkacat.md","sourceDirName":"technologies/kafka","slug":"/technologies/kafka/kafkacat","permalink":"/technologies/kafka/kafkacat","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/kafka/kafkacat.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1709144125000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Kafka Topic/Replication","permalink":"/technologies/kafka/kafka-topic-replication"},"next":{"title":"Migration / Mirroring / Replication","permalink":"/technologies/kafka/migration-mirroring-replication"}}');var s=n(474848),i=n(28453);const r={},o="kafkacat",c={},l=[{value:"Installing",id:"installing",level:4},{value:"Commands",id:"commands",level:2},{value:"Consumers",id:"consumers",level:3},{value:"Metadata listing",id:"metadata-listing",level:2},{value:"Producers",id:"producers",level:2},{value:"Kafkacat documentation",id:"kafkacat-documentation",level:2},{value:"Example Commands",id:"example-commands",level:2},{value:"metadata listing",id:"metadata-listing-1",level:3},{value:"consumer - get data from bank_data",id:"consumer---get-data-from-bank_data",level:3},{value:"get size of the packets",id:"get-size-of-the-packets",level:3},{value:"producer",id:"producer",level:3}];function d(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"kafkacat",children:"kafkacat"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://github.com/edenhill/kafkacat",children:"https://github.com/edenhill/kafkacat"})}),"\n",(0,s.jsx)(a.p,{children:"kafkacat is a generic non-JVM producer and consumer for Apache Kafka >=0.8, think of it as a netcat for Kafka."}),"\n",(0,s.jsx)(a.p,{children:"In producer mode kafkacat reads messages from stdin, delimited with a configurable delimiter (-D, defaults to newline), and produces them to the provided Kafka cluster (-b), topic (-t) and partition (-p)."}),"\n",(0,s.jsx)(a.p,{children:"In consumer mode kafkacat reads messages from a topic and partition and prints them to stdout using the configured message delimiter."}),"\n",(0,s.jsxs)(a.p,{children:["There's also support for the Kafka >=0.9 high-level balanced consumer, use the ",(0,s.jsx)(a.code,{children:"-G <group>"})," switch and provide a list of topics to join the group."]}),"\n",(0,s.jsx)(a.p,{children:"kafkacat also features a Metadata list (-L) mode to display the current state of the Kafka cluster and its topics and partitions."}),"\n",(0,s.jsx)(a.p,{children:"Supports Avro message deserialization using the Confluent Schema-Registry, and generic primitive deserializers (see examples below)."}),"\n",(0,s.jsx)(a.p,{children:"kafkacat is fast and lightweight; statically linked it is no more than 150Kb."}),"\n",(0,s.jsx)(a.h4,{id:"installing",children:"Installing"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"brew install kafkacat\napt-get install kafkacat\n\n# use kcat command instead of kafkacat\n"})}),"\n",(0,s.jsx)(a.h2,{id:"commands",children:"Commands"}),"\n",(0,s.jsx)(a.h3,{id:"consumers",children:"Consumers"}),"\n",(0,s.jsx)(a.p,{children:"High-level balanced KafkaConsumer: subscribe to topic1 and topic2 (requires broker >=0.9.0 and librdkafka version >=0.9.1)"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b localhost:9091 -G mygroup topic1 topic2"})}),"\n",(0,s.jsx)(a.p,{children:"Read messages from Kafka 'syslog' topic, print to stdout"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -t druid_uncompressed -c 10"})}),"\n",(0,s.jsx)(a.p,{children:"Read the last 2000 messages from 'syslog' topic, then exit"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"kafkacat -C -b mybroker -t syslog -p 0 -o -2000 -e\n\nkafkacat -C -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -t druid_compressed -p 0 -o -2000 -e\n"})}),"\n",(0,s.jsx)(a.p,{children:"Consume from all partitions from 'syslog' topic"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -C -b mybroker -t syslog"})}),"\n",(0,s.jsx)(a.p,{children:"Output consumed messages in JSON envelope:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -t syslog -J"})}),"\n",(0,s.jsx)(a.p,{children:"Decode Avro key (-s key=avro), value (-s value=avro) or both (-s avro) to JSON using schema from the Schema-Registry:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -t ledger -s avro -r http://schema-registry-url:8080"})}),"\n",(0,s.jsx)(a.p,{children:'Decode Avro message value and extract Avro record\'s "age" field:'}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -t ledger -s value=avro -r http://schema-registry-url:8080 | jq .payload.age"})}),"\n",(0,s.jsx)(a.p,{children:"Decode key as 32-bit signed integer and value as 16-bit signed integer followed by an unsigned byte followed by string:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -t mytopic -s key='i$' -s value='hB s'"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"Hint: see./kafkacat -h for all available deserializer options."})}),"\n",(0,s.jsx)(a.p,{children:"Output consumed messages according to format string:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -t syslog -f 'Topic %t [%p], offset: %o, key: %k, payload: %S bytes: %sn'"})}),"\n",(0,s.jsx)(a.p,{children:"Read the last 100 messages from topic 'syslog' with librdkafka configuration parameter 'broker.version.fallback' set to '0.8.2.1' :"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -C -b mybroker -X broker.version.fallback=0.8.2.1 -t syslog -p 0 -o -100 -e"})}),"\n",(0,s.jsx)(a.p,{children:"Print headers in consumer:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -C -t mytopic -f 'Headers: %h: Message value: %sn'"})}),"\n",(0,s.jsx)(a.p,{children:"Enable the idempotent producer, providing exactly-once and strict-orderingproducerguarantees:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -X enable.idempotence=true -P -t mytopic ...."})}),"\n",(0,s.jsx)(a.h2,{id:"metadata-listing",children:"Metadata listing"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -L -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092"})}),"\n",(0,s.jsx)(a.p,{children:"Metadata for all topics (from broker 1: mybroker:9092/1):"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:'3 brokers:\nbroker 1 at mybroker:9092\nbroker 2 at mybrokertoo:9092\nbroker 3 at thirdbroker:9092\n16 topics:\ntopic "syslog" with 3 partitions:\npartition 0, leader 3, replicas: 1,2,3, isrs: 1,2,3\npartition 1, leader 1, replicas: 1,2,3, isrs: 1,2,3\npartition 2, leader 1, replicas: 1,2, isrs: 1,2\ntopic "rdkafkatest1_auto_49f744a4327b1b1e" with 2 partitions:\npartition 0, leader 3, replicas: 3, isrs: 3\npartition 1, leader 1, replicas: 1, isrs: 1\ntopic "rdkafkatest1_auto_e02f58f2c581cba" with 2 partitions:\npartition 0, leader 3, replicas: 3, isrs: 3\npartition 1, leader 1, replicas: 1, isrs: 1\n'})}),"\n",(0,s.jsx)(a.p,{children:"JSON metadata listing"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -L -J"})}),"\n",(0,s.jsx)(a.p,{children:"Pretty-printed JSON metadata listing"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -b mybroker -L -J | jq ."})}),"\n",(0,s.jsx)(a.p,{children:"Query offset(s) by timestamp(s)"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"kafkacat -b **kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092** -Q -t druid_telemetry_data_Samhi:0:1569048234230\n\nkafkacat -b **kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092** -Q -t druid_telemetry_data_Samhi:0:**1568989500000**\n"})}),"\n",(0,s.jsx)(a.p,{children:"Consume messages between two timestamps"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"# Working\nkafkacat -b **kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092** -C -t druid_telemetry_data_Samhi -o s@1574063938000 -o e@1574063940000 **-**f 'nKey (%K bytes): %ktnValue (%S bytes): %snTimestamp: %TtPartition: %ptOffset: %on--n'\n\n# Redirect logs to different topic\nkafkacat -b kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -C -t **smap_samhi**-o s@**1568989590000**-o e@**1568989620000 | kafkacat -b**kafka0.example.com:31090,kafka1.example.com:31091,kafka2.example.com:31092 -P -t samhi_logs\n"})}),"\n",(0,s.jsx)(a.h2,{id:"producers",children:"Producers"}),"\n",(0,s.jsx)(a.p,{children:"Read messages from stdin, produce to 'syslog' topic with snappy compression"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"tail -f /var/log/syslog | kafkacat -b mybroker -t syslog -z snappy"})}),"\n",(0,s.jsx)(a.p,{children:"Produce messages from file (one file is one message)"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat -P -b mybroker -t filedrop -p 0 myfile1.bin /etc/motd thirdfile.tgz"})}),"\n",(0,s.jsx)(a.p,{children:'Produce a tombstone (a "delete" for compacted topics) for key "abc" by providing an empty message value which-Zinterpretes as NULL:'}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:'echo "abc:" | kafkacat -b mybroker -t mytopic -Z -K:'})}),"\n",(0,s.jsx)(a.p,{children:"Produce with headers:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:'echo "hello there" | kafkacat -b mybroker -H "header1=header value" -H "nullheader" -H "emptyheader=" -H "header1=duplicateIsOk"'})}),"\n",(0,s.jsx)(a.h2,{id:"kafkacat-documentation",children:"Kafkacat documentation"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"Usage: kafkacat <options> [file1 file2 .. | topic1 topic2 ..]]"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"kafkacat - Apache Kafka producer and consumer tool"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://github.com/edenhill/kafkacat",children:"https://github.com/edenhill/kafkacat"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:'-C | -P | -L | -Q  Mode: Consume, Produce, Metadata List, Query mode\n   -G <group-id>      Mode: High-level KafkaConsumer (Kafka >=0.9 balanced consumer groups)\n                      Expects a list of topics to subscribe to\n   -t <topic>         Topic to consume from, produce to, or list\n   -p <partition>     Partition\n   -b <brokers,..>    Bootstrap broker(s) (host [:port])\n   -D <delim>         Message delimiter character:\n                      a-z.. | \\r | \\n | \\t | \\xNN\n                      Default: \\n\n   -E                 Do not exit on non fatal error\n   -K <delim>         Key delimiter (same format as -D)\n   -c <cnt>           Limit message count\n   -F <config-file>   Read configuration properties from file,\n                      file format is "property=value".\n                      The KAFKACAT_CONFIG=path environment can also be used, but -F takes preceedence.\n                      The default configuration file is $HOME/.config/kafkacat.conf\n   -X list            List available librdkafka configuration properties\n   -X prop=val        Set librdkafka configuration property.\n                      Properties prefixed with "topic." are\n                      applied as topic properties.\n   -X dump            Dump configuration and exit.\n   -d <dbg1,...>      Enable librdkafka debugging:\n                      all,generic,broker,topic,metadata,feature,queue,msg,protocol,cgrp,security,fetch,interceptor,plugin,consumer,admin,eos\n   -q                 Be quiet (verbosity set to 0)\n   -v                 Increase verbosity\n   -V                 Print version\n   -h                 Print usage help\n\n Producer options:\n   -z snappy|gzip|lz4 Message compression. Default: none\n   -p -1              Use random partitioner\n   -D <delim>         Delimiter to split input into messages\n   -K <delim>         Delimiter to split input key and message\n   -k <str>           Use a fixed key for all messages.\n                      If combined with -K, per-message keys\n                      takes precendence.\n   -H <header=value>  Add Message Headers (may be specified multiple times)\n   -l                 Send messages from a file separated by\n                      delimiter, as with stdin.\n                      (only one file allowed)\n   -T                 Output sent messages to stdout, acting like tee.\n   -c <cnt>           Exit after producing this number of messages\n   -Z                 Send empty messages as NULL messages\n   file1 file2..      Read messages from files.\n                      With -l, only one file permitted.\n                      Otherwise, the entire file contents will\n                      be sent as one single message.\n\n Consumer options:\n   -o <offset>        Offset to start consuming from:\n                      beginning | end | stored |\n                      <value>  (absolute offset) |\n                      -<value> (relative offset from end)\n                      s@<value> (timestamp in ms to start at)\n                      e@<value> (timestamp in ms to stop at (not included))\n   -e                 Exit successfully when last message received\n   -f <fmt..>         Output formatting string, see below.\n                      Takes precedence over -D and -K.\n   -J                 Output with JSON envelope\n   -s key=<serdes>    Deserialize non-NULL keys using <serdes>.\n   -s value=<serdes>  Deserialize non-NULL values using <serdes>.\n   -s <serdes>        Deserialize non-NULL keys and values using <serdes>.\n                      Available deserializers (<serdes>):\n                        <pack-str> - A combination of:\n                                     <: little-endian,\n                                     >: big-endian (recommended),\n                                     b: signed 8-bit integer\n                                     B: unsigned 8-bit integer\n                                     h: signed 16-bit integer\n                                     H: unsigned 16-bit integer\n                                     i: signed 32-bit integer\n                                     I: unsigned 32-bit integer\n                                     q: signed 64-bit integer\n                                     Q: unsigned 64-bit integer\n                                     c: ASCII character\n                                     s: remaining data is string\n                                     $: match end-of-input (no more bytes remaining or a parse error is raised).\n                                        Not including this token skips any\n                                        remaining data after the pack-str is\n                                        exhausted.\n   -D <delim>         Delimiter to separate messages on output\n   -K <delim>         Print message keys prefixing the message\n                      with specified delimiter.\n   -O                 Print message offset using -K delimiter\n   -c <cnt>           Exit after consuming this number of messages\n   -Z                 Print NULL values and keys as "NULL"instead of empty.\n                      For JSON (-J) the nullstr is always null.\n   -u                 Unbuffered output\n\n Metadata options (-L):\n   -t <topic>         Topic to query (optional)\n\n Query options (-Q):\n   -t <t>:<p>:<ts>    Get offset for topic <t>,\n                      partition <p>, timestamp <ts>.\n                      Timestamp is the number of milliseconds\n                      since epoch UTC.\n                      Requires broker >= 0.10.0.0 and librdkafka >= 0.9.3.\n                      Multiple -t .. are allowed but a partition\n                      must only occur once.\n\n Format string tokens:\n   %s                 Message payload\n   %S                 Message payload length (or -1 for NULL)\n   %R                 Message payload length (or -1 for NULL) serialized\n                      as a binary big endian 32-bit signed integer\n   %k                 Message key\n   %K                 Message key length (or -1 for NULL)\n   %T                 Message timestamp (milliseconds since epoch UTC)\n   %h                 Message headers (n=v CSV)\n   %t                 Topic\n   %p                 Partition\n   %o                 Message offset\n   \\n \\r \\t           Newlines, tab\n   \\xXX \\xNNN         Any ASCII character\n  Example:\n   -f \'Topic %t [%p] at offset %o: key %k: %s\\n\'\n\n JSON message envelope (on one line) when consuming with -J:\n  { "topic": str, "partition": int, "offset": int,\n    "tstype": "create|logappend|unknown", "ts": int, // timestamp in milliseconds since epoch\n    "headers": { "<name>": str, .. }, // optional\n    "key": str|json, "payload": str|json,\n    "key_error": str, "payload_error": str } //optional\n  (note: key_error and payload_error are only included if deserialization failed)\n\n Consumer mode (writes messages to stdout):\n    kafkacat -b <broker> -t <topic> -p <partition>\n  or:\n   kafkacat -C -b ...\n\n High-level KafkaConsumer mode:\n   kafkacat -b <broker> -G <group-id> topic1 top2 ^aregex\\d+\n\n Producer mode (reads messages from stdin):\n   ... | kafkacat -b <broker> -t <topic> -p <partition>\n  or:\n   kafkacat -P -b ...\n\n Metadata listing:\n   kafkacat -L -b <broker> [-t <topic>]\n\n Query offset by timestamp:\n  kafkacat -Q -b broker -t <topic>:<partition>:<timestamp>\n'})}),"\n",(0,s.jsx)(a.h2,{id:"example-commands",children:"Example Commands"}),"\n",(0,s.jsx)(a.h3,{id:"metadata-listing-1",children:"metadata listing"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"kafkacat -L -b kafka0.example.com:9094,kafka1.example.com,kafka2.example.com\n\nkafkacat -L -b localhost:9094\n"})}),"\n",(0,s.jsx)(a.h3,{id:"consumer---get-data-from-bank_data",children:"consumer - get data from bank_data"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"kafkacat -C -b my-cluster-kafka-brokers.kafka:9092 -t bank_data -p 0 -o -2000 -e\n\nkafkacat -C -b kafka0.example.com:9094,kafka1.example.com,kafka2.example.com -t test\n\nkafkacat -C -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test_bank_data -o -2000 -f 'nKey (%K bytes): %ktnValue (%S bytes): %snTimestamp: %TtPartition: %ptOffset: %on--n'\n"})}),"\n",(0,s.jsx)(a.h3,{id:"get-size-of-the-packets",children:"get size of the packets"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"kafkacat -C -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test_bank_data -o -2000 -f 'nValue (%S bytes) t Timestamp: %TtPartition: %ptOffset: %o'\n\nkafkacat -C -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test_bank_data -o -2000 -f 'n%S,%T,%p,%o'\n"})}),"\n",(0,s.jsx)(a.h3,{id:"producer",children:"producer"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:'echo "hello" | kafkacat -P -b my-cluster-kafka-brokers.kafka:9092 -t test\n\nwhile true; do echo $(($(date +%s%N)/1000000)) | kafkacat -P -b my-cluster-kafka-brokers.kafka:9092 -t test; sleep 2; echo $(($(date +%s%N)/1000000)); done\n\nkafkacat -b kafka0.example.com:9094,kafka1.example.com:9094,kafka2.example.com:9094 -t test -c 10\n'})})]})}function p(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,a,n)=>{n.d(a,{R:()=>r,x:()=>o});var t=n(296540);const s={},i=t.createContext(s);function r(e){const a=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:a},e.children)}}}]);