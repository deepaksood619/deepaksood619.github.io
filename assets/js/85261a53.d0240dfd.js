"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[11360],{633757:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"technologies/kafka/kafka-architecture","title":"Kafka Architecture","description":"Kafka consists of brokers that take messages from the producers and add to a partition of a topic. Brokers provide the messages to the consumers from the partitions.","source":"@site/docs/technologies/kafka/kafka-architecture.md","sourceDirName":"technologies/kafka","slug":"/technologies/kafka/kafka-architecture","permalink":"/technologies/kafka/kafka-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/kafka/kafka-architecture.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1769708334000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Intro to Kafka","permalink":"/technologies/kafka/intro-to-kafka"},"next":{"title":"Kafka Brokers","permalink":"/technologies/kafka/kafka-brokers"}}');var t=n(474848),a=n(28453);const r={},o="Kafka Architecture",l={},c=[{value:"Why is Kafka so Fast?",id:"why-is-kafka-so-fast",level:2},{value:"Zero-copy",id:"zero-copy",level:3},{value:"Kafka Zero Copy",id:"kafka-zero-copy",level:3},{value:"Other Optimizations - Messages batching &amp; Compression",id:"other-optimizations---messages-batching--compression",level:3},{value:"Concepts",id:"concepts",level:2},{value:"Key concepts",id:"key-concepts",level:2},{value:"Segment File",id:"segment-file",level:3},{value:"Retention Policy",id:"retention-policy",level:2},{value:"Kafka Internals (Definitive Guide)",id:"kafka-internals-definitive-guide",level:2},{value:"Replication",id:"replication",level:3},{value:"Leader replica",id:"leader-replica",level:4},{value:"Follower replica",id:"follower-replica",level:4},{value:"Request Processing",id:"request-processing",level:3},{value:"The most common types of requests are",id:"the-most-common-types-of-requests-are",level:4},{value:"Produce requests",id:"produce-requests",level:5},{value:"Fetch requests",id:"fetch-requests",level:5},{value:"Physical Storage",id:"physical-storage",level:3},{value:"Kafka Rebalancing Protocol",id:"kafka-rebalancing-protocol",level:2},{value:"Compression",id:"compression",level:2},{value:"Unclean Leader Election",id:"unclean-leader-election",level:2},{value:"Others",id:"others",level:2},{value:"References",id:"references",level:2}];function d(e){const s={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"kafka-architecture",children:"Kafka Architecture"})}),"\n",(0,t.jsx)(s.p,{children:"Kafka consists of brokers that take messages from the producers and add to a partition of a topic. Brokers provide the messages to the consumers from the partitions."}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"A topic is divided into multiple partitions"}),"\n",(0,t.jsx)(s.li,{children:"The messages are added to the partitions at one end and consumed in the same order"}),"\n",(0,t.jsx)(s.li,{children:"Each partition acts as a message queue"}),"\n",(0,t.jsx)(s.li,{children:"Consumers are divided into consumer groups"}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"why-is-kafka-so-fast",children:"Why is Kafka so Fast?"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsx)(s.li,{children:"The first one is Kafka\u2019s reliance on Sequential I/O."}),"\n",(0,t.jsx)(s.li,{children:"The second design choice that gives Kafka its performance advantage is its focus on efficiency: zero copy principle."}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:["Kafka relies heavily on the OS kernel to move data around quickly. It relies on the principals of ",(0,t.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Zero-copy",children:"Zero Copy"}),". Kafka enables you to batch data records into chunks. These batches of data can be seen end to end from Producer to file system (Kafka Topic Log) to the Consumer. Batching allows for more efficient data compression and reduces I/O latency. Kafka writes to the immutable commit log to the disk sequential; thus, avoids random disk access, slow disk seeking. Kafka provides horizontal Scale through sharding. It shards a Topic Log into hundreds potentially thousands of partitions to thousands of servers. This sharding allows Kafka to handle massive load."]}),"\n",(0,t.jsx)(s.p,{children:"Zero-copy means that Kafka sends messages from the file (or more likely, the Linux filesystem cache) directly to the network channel without any intermediate buffers."}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"Why is Kafka Fast",src:n(390281).A+"",width:"999",height:"1203"})}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://www.youtube.com/channel/UCZgt6AzoyjslHTC9dz0UoTw/community?lb=UgkxKPCx8UjOik2iB9rKHDWqgzv_y59aCrDW",children:"ByteByteGo - YouTube"})}),"\n",(0,t.jsx)(s.h3,{id:"zero-copy",children:"Zero-copy"}),"\n",(0,t.jsxs)(s.p,{children:['"',(0,t.jsx)(s.strong,{children:"Zero-copy"}),'" describes computer operations in which the ',(0,t.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Central_processing_unit",children:"CPU"})," does not perform the task of copying data from one ",(0,t.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/RAM",children:"memory"})," area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network."]}),"\n",(0,t.jsx)(s.h3,{id:"kafka-zero-copy",children:"Kafka Zero Copy"}),"\n",(0,t.jsx)(s.p,{children:"Ideally, the data written to the log segment is written in protocol format. That is, what gets written to disk is exactly what gets sent over the wire. This allows for zero-copy reads. Let's take a look at how this otherwise works."}),"\n",(0,t.jsx)(s.p,{children:"When you read messages from the log, the kernel will attempt to pull the data from the page cache. If it's not there, it will be read from disk. The data is copied from disk to page cache, which all happens in kernel space. Next, the data is copied into the application (i.e. user space). This all happens with the read system call. Now the application writes the data out to a socket using send, which is going to copy it back into kernel space to a socket buffer before it's copied one last time to the NIC. All in all, we have four copies (including one from page cache) and two system calls."}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"image",src:n(704515).A+"",width:"999",height:"401"})}),"\n",(0,t.jsx)(s.p,{children:"However, if the data is already in wire format, we can bypass user space entirely using the send filesystem call, which will copy the data directly from the page cache to the NIC buffer - two copies (including one from page cache) and one system call. This turns out to be an important optimization, especially in garbage-collected languages since we're bringing less data into application memory. Zero-copy also reduces CPU cycles and memory bandwidth."}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"image",src:n(865990).A+"",width:"997",height:"320"})}),"\n",(0,t.jsx)(s.h3,{id:"other-optimizations---messages-batching--compression",children:"Other Optimizations - Messages batching & Compression"}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Zero-copy",children:"https://en.wikipedia.org/wiki/Zero-copy"})}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics",children:"https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics"})}),"\n",(0,t.jsx)(s.h2,{id:"concepts",children:"Concepts"}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"image",src:n(810036).A+"",width:"1000",height:"584"})}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Zookeeper"}),"\n",(0,t.jsx)(s.li,{children:"Producer"}),"\n",(0,t.jsx)(s.li,{children:"Consumer"}),"\n",(0,t.jsx)(s.li,{children:"Kafka cluster"}),"\n",(0,t.jsx)(s.li,{children:"Failovers"}),"\n",(0,t.jsx)(s.li,{children:"ISRs"}),"\n",(0,t.jsx)(s.li,{children:"Kafka disaster recovery"}),"\n",(0,t.jsx)(s.li,{children:"Topic"}),"\n",(0,t.jsx)(s.li,{children:"Topic Partition"}),"\n",(0,t.jsx)(s.li,{children:"Consumer Group"}),"\n",(0,t.jsx)(s.li,{children:"Offsets"}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:["Starting from version 0.8.2.0, the offsets committed by the consumers aren't saved in ",(0,t.jsx)(s.a,{href:"https://dzone.com/articles/an-introduction-to-zookeeper-1",children:"ZooKeeper"})," but on a partitioned and replicated topic named ",(0,t.jsx)(s.code,{children:"__consumer_offsets"}),", which is hosted on the Kafka brokers in the cluster."]}),"\n",(0,t.jsxs)(s.p,{children:["When a consumer commits some offsets (for different partitions), it sends a message to the broker to the ",(0,t.jsx)(s.code,{children:"__consumer_offsets"})," topic. The message has the following structure :"]}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-bash",children:"key = [group, topic, partition\nvalue = offset\n"})}),"\n",(0,t.jsx)(s.h2,{id:"key-concepts",children:"Key concepts"}),"\n",(0,t.jsxs)(s.table,{children:[(0,t.jsx)(s.thead,{children:(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.th,{}),(0,t.jsx)(s.th,{})]})}),(0,t.jsxs)(s.tbody,{children:[(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"topic"}),(0,t.jsx)(s.td,{children:"Defines a logical name for producing and consuming records."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"partition"}),(0,t.jsx)(s.td,{children:"Defines a non-overlapping subset of records within a topic."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"offset"}),(0,t.jsx)(s.td,{children:"A unique sequential number assigned to each record within a topic partition."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"record"}),(0,t.jsx)(s.td,{children:"A record contains a key, a value, a timestamp, and a list of headers."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"broker"}),(0,t.jsx)(s.td,{children:"Servers where records are stored. Multiple brokers can be used to form a cluster."})]})]})]}),"\n",(0,t.jsx)(s.h3,{id:"segment-file",children:"Segment File"}),"\n",(0,t.jsxs)(s.p,{children:["A Kafka ",(0,t.jsx)(s.strong,{children:"Partition"})," is technically a directory of files. It would be impossible to store all messages for a partition in a single massive file (it would be hard to delete old data or scale)."]}),"\n",(0,t.jsxs)(s.p,{children:["Instead, Kafka splits the partition log into smaller files called ",(0,t.jsx)(s.strong,{children:"Segments"}),"."]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Structure:"})," A segment consists of a ",(0,t.jsx)(s.code,{children:".log"})," file (actual data) and an ",(0,t.jsx)(s.code,{children:".index"})," file (maps offsets to byte positions)."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Rolling:"}),' When a segment reaches a size limit (e.g., 1GB) or a time limit (e.g., 7 days), the file is closed, and a new "active" segment is created.']}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Cleanup:"})," Segment files are the unit of deletion. When a log cleanup policy kicks in (delete data older than X days), Kafka deletes the entire old segment file."]}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"retention-policy",children:"Retention Policy"}),"\n",(0,t.jsxs)(s.table,{children:[(0,t.jsx)(s.thead,{children:(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.th,{children:(0,t.jsx)(s.strong,{children:"RETENTION POLICY"})}),(0,t.jsx)(s.th,{children:(0,t.jsx)(s.strong,{children:"MEANING"})})]})}),(0,t.jsxs)(s.tbody,{children:[(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"log.retention.hours"}),(0,t.jsx)(s.td,{children:"The number of hours to keep a record on the broker."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"log.retention.bytes"}),(0,t.jsx)(s.td,{children:"The maximum size of records retained in a partition."})]})]})]}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Time-based retention:"}),"\xa0Messages are retained for a specified duration of time. Once the time limit is reached, Kafka marks the messages as eligible for deletion. For example, you can set a retention period of 7 days, and any message older than 7 days will be deleted."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Size-based retention:"}),"\xa0This policy determines the retention of messages based on the size of the topic log. Kafka allows you to set a maximum size for the topic's log, and once that size is reached, Kafka starts deleting older messages to make room for new ones."]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Retention Policies"})}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsx)(s.li,{children:"Delete"}),"\n",(0,t.jsx)(s.li,{children:"Compact"}),"\n",(0,t.jsx)(s.li,{children:"Delete, Compact"}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Points to Remember"})}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["Active segments doesn't participate in cleaning up","\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Cleanup is done in old and closed segments"}),"\n",(0,t.jsx)(s.li,{children:"If the latest message in the segment is greater than the retention time, then that segment gets cleaned up."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:['A background process called the "log cleaner" scans the log segments.',"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"For each key, it keeps the most recent message and removes older messages with the same key from the tail of the log."}),"\n",(0,t.jsxs)(s.li,{children:["Messages with a\xa0",(0,t.jsx)(s.code,{children:"null"}),"\xa0payload (tombstones) are treated as delete markers and are also removed after a configurable period (default\xa0",(0,t.jsx)(s.code,{children:"delete.retention.ms"}),"\xa0is 24 hours)."]}),"\n",(0,t.jsx)(s.li,{children:"Message order is always maintained, and message offsets never change."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.li,{children:"Clean + Dirty = Total >= 50%, then the cleanup gets triggered"}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Clean and Dirty Segments"})}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"\ufeff\ufeffLog segments that have been compacted are called clean segments"}),"\n",(0,t.jsx)(s.li,{children:"\ufeff\ufeffLog segments that have not been compacted are called dirty segments"}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://docs.confluent.io/kafka/design/log_compaction.html",children:"Kafka Log Compaction | Confluent Documentation"})}),"\n",(0,t.jsx)(s.h2,{id:"kafka-internals-definitive-guide",children:"Kafka Internals (Definitive Guide)"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["Cluster Membership","\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Kafka uses Apache Zookeeper to maintain the list of brokers that are currently members of a cluster"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["Controller","\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["The first broker that starts in the cluster becomes the controller by creating an ephemeral node in ZooKeeper called ",(0,t.jsx)(s.code,{children:"/controller"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.h3,{id:"replication",children:"Replication"}),"\n",(0,t.jsx)(s.p,{children:"Two types of replicas"}),"\n",(0,t.jsx)(s.h4,{id:"leader-replica",children:"Leader replica"}),"\n",(0,t.jsx)(s.p,{children:"Each partition has a single replica designated as the leader. All produce and consume requests go through the leader, in order to guarantee consistency."}),"\n",(0,t.jsx)(s.h4,{id:"follower-replica",children:"Follower replica"}),"\n",(0,t.jsx)(s.p,{children:"All replicas for a partition that are not leaders are called followers. Followers don't serve client requests; their only job is to replicate messages from the leader and stay up-to-date with the most recent messages the leader has. In the event that a leader replica for a partition crashes, one of the follower replicas will be promoted to become the new leader for the partition."}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["Replicas that are consistently asking for the latest messages, is called ",(0,t.jsx)(s.em,{children:"in-sync replicas"}),". Only in-sync replicas are eligible to be elected as partition leaders in case the existing leader fails."]}),"\n"]}),"\n",(0,t.jsx)(s.h3,{id:"request-processing",children:"Request Processing"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Most of what a Kafka broker does is process requests sent to the partition leaders from clients, partition replicas, and the controller."}),"\n",(0,t.jsxs)(s.li,{children:["For each port the broker listens on, the broker runs an ",(0,t.jsx)(s.em,{children:"acceptor"})," thread that creates a connection and hands it over to a ",(0,t.jsx)(s.em,{children:"processor"})," thread for handling. The number of processor threads (also called ",(0,t.jsx)(s.em,{children:"network threads"}),") is configurable. The network threads are responsible for taking requests from client connections, placing them in a ",(0,t.jsx)(s.em,{children:"request queue"}),", and picking up responses from a ",(0,t.jsx)(s.em,{children:"response queue"})," and sending them back to clients."]}),"\n",(0,t.jsxs)(s.li,{children:["Once requests are placed on the request queue, ",(0,t.jsx)(s.em,{children:"IO threads"})," are responsible for picking them up and processing them."]}),"\n"]}),"\n",(0,t.jsx)(s.h4,{id:"the-most-common-types-of-requests-are",children:"The most common types of requests are"}),"\n",(0,t.jsx)(s.h5,{id:"produce-requests",children:"Produce requests"}),"\n",(0,t.jsx)(s.p,{children:"Sent by producers and contain messages the clients write to Kafka brokers."}),"\n",(0,t.jsx)(s.h5,{id:"fetch-requests",children:"Fetch requests"}),"\n",(0,t.jsx)(s.p,{children:"Sent by consumers and follower replicas when they read messages from Kafka brokers."}),"\n",(0,t.jsx)(s.h3,{id:"physical-storage",children:"Physical Storage"}),"\n",(0,t.jsx)(s.p,{children:"The basic storage unit of Kafka is a partition replica."}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Partition allocation"}),"\n",(0,t.jsx)(s.li,{children:"File management"}),"\n",(0,t.jsx)(s.li,{children:"File format"}),"\n",(0,t.jsx)(s.li,{children:"Indexes"}),"\n",(0,t.jsx)(s.li,{children:"Compaction"}),"\n",(0,t.jsxs)(s.li,{children:["Deleted Events","\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"tombstone"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"kafka-rebalancing-protocol",children:"Kafka Rebalancing Protocol"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.strong,{children:"Eager Rebalancing Protocol (Stop the world rebalancing)"})}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Incremental cooperative rebalancing"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Incremental because the final desired state of rebalancing is reached in stages. A globally balanced final state does not have to be reached at the end of each round of rebalancing. A small number of consecutive rebalancing rounds can be used in order for the group of Kafka clients to converge to the desired state of balanced resources. In addition, you can configure a grace period to allow a departing member to return and regain its previously assigned resources."}),"\n",(0,t.jsx)(s.li,{children:"Cooperative because each process in the group is asked to voluntarily release resources that need to be redistributed. These resources are then made available for rescheduling given that the client that was asked to release them does so on time"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka",children:"https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka"})}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://medium.com/streamthoughts/apache-kafka-rebalance-protocol-or-the-magic-behind-your-streams-applications-e94baf68e4f2",children:"https://medium.com/streamthoughts/apache-kafka-rebalance-protocol-or-the-magic-behind-your-streams-applications-e94baf68e4f2"})}),"\n",(0,t.jsx)(s.h2,{id:"compression",children:"Compression"}),"\n",(0,t.jsx)(s.p,{children:"compression.type: Specify the final compression type for a given topic. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer' which means retain the original compression codec set by the producer"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Type"}),": string"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Default"}),": producer"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Valid Values"}),": [uncompressed, zstd, lz4, snappy, gzip, producer]"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Server Default Property"}),": compression.type"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Importance"}),": medium"]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"In Kafka compression, multiple messages are bundled and compressed. Then the compressed messages are turned into a special kind of message and appended to Kafka's log file. The reason to compress a batch of messages, rather than individual messages, is to increase compression efficiency, i.e., compressors work better with bigger data."}),"\n",(0,t.jsx)(s.p,{children:"There are tradeoffs with enabling compression that should be considered. Compression, of course, saves space needed for data storage. On the other hand, it consumes extra computing resources, namely CPU cycles and memory, to perform compression. The use of compression should be therefore decided with consideration of the balance of cost and benefit. To make the compression applicable to wider usages of Kafka, we set out to improve Kafka compression support by reducing the cost."}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://www.confluent.io/blog/compression-in-apache-kafka-is-now-34-percent-faster",children:"https://www.confluent.io/blog/compression-in-apache-kafka-is-now-34-percent-faster"})}),"\n",(0,t.jsx)(s.h2,{id:"unclean-leader-election",children:"Unclean Leader Election"}),"\n",(0,t.jsxs)(s.p,{children:["Another caveat with Kafka is unclean leader elections. That is, if all replicas become unavailable, there are two options: choose the first replica to come back to life (not necessarily in the ISR) and elect this replica as leader (which could result in data loss) or wait for a replica in the ISR to come back to life and elect it as leader (which could result in prolonged unavailability). Initially, Kafka favoured availability by default by choosing the first strategy. If you preferred consistency, you needed to set ",(0,t.jsx)(s.code,{children:"unclean.leader.election.enable"})," to ",(0,t.jsx)(s.code,{children:"false"}),". However, as of 0.11, ",(0,t.jsx)(s.code,{children:"unclean.leader.election.enable"})," now defaults to this."]}),"\n",(0,t.jsxs)(s.p,{children:["When\xa0",(0,t.jsx)(s.code,{children:"unclean.leader.election.enable"}),"\xa0is set to\xa0",(0,t.jsx)(s.code,{children:"true"}),'\xa0in Kafka, it means that Kafka allows an out-of-sync replica to become the leader during a leader election if there are no in-sync replicas available. This is considered an "unclean" leader election because the selected leader might not have all the messages that were written while it was down.']}),"\n",(0,t.jsxs)(s.p,{children:['how Kafka knows if the new leader is "clean" or not, when\xa0',(0,t.jsx)(s.code,{children:"unclean.leader.election.enable"}),"\xa0is true:"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:'Kafka does not explicitly check if the new leader is "clean."'}),"\xa0The setting allows for the possibility of unclean leader elections, acknowledging that the elected leader might not be in sync with the latest messages."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"The leader may not have all the messages written while it was down."}),"\xa0This is an accepted trade-off for cases where ensuring availability is prioritized over strict consistency."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Efficiency vs. Consistency trade-off:"}),"\xa0Storing the latest offset for each received message in a distributed storage system like ZooKeeper for every leader replica could be resource-intensive and may introduce additional latency. Kafka's design, in this case, prioritizes availability and responsiveness."]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:["If strict consistency is a higher priority for your use case, you might choose to keep\xa0",(0,t.jsx)(s.code,{children:"unclean.leader.election.enable"}),"\xa0set to\xa0",(0,t.jsx)(s.code,{children:"false"}),"\xa0(the default) to prevent unclean leader elections and ensure that a new leader must be fully caught up with the latest messages."]}),"\n",(0,t.jsxs)(s.p,{children:['Here\'s how Kafka determines whether a new leader is "clean" or not when\xa0',(0,t.jsx)(s.code,{children:"unclean.leader.election.enable"}),"\xa0is set to\xa0",(0,t.jsx)(s.code,{children:"false"}),":"]}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"In-Sync Replica (ISR) Set:"})," Kafka maintains a concept called the In-Sync Replica (ISR) set for each partition. The ISR set consists of replicas that are in sync with the leader."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Leader Election:"})," When a leader election occurs, only replicas in the ISR set are eligible to become the new leader."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Replica Catch-Up:"})," Before a replica becomes the leader, it needs to catch up with the leader's log. If a replica is not in the ISR set or is lagging significantly behind the leader, it won't be eligible to become the new leader."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Out-of-Sync Replica Handling:"})," If a replica is out of sync and cannot catch up within a configured time (controlled by the\xa0",(0,t.jsx)(s.code,{children:"replica.lag.time.max.ms"}),"\xa0property), it may be removed from the ISR set, preventing it from becoming the leader."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"ZooKeeper Information:"})," Kafka uses ZooKeeper to maintain metadata, including information about the ISR set. ZooKeeper keeps track of which replicas are in sync with the leader."]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"By relying on the ISR set and ensuring that only replicas in sync are eligible for leadership, Kafka helps prevent the scenario where an out-of-sync replica becomes a leader. This mechanism contributes to maintaining data consistency and avoiding potential data loss during leader elections."}),"\n",(0,t.jsxs)(s.p,{children:["In summary, Kafka uses the ISR set, replica synchronization, and information stored in ZooKeeper to ensure that only in-sync replicas are considered for leadership when\xa0",(0,t.jsx)(s.code,{children:"unclean.leader.election.enable"}),"\xa0is set to\xa0",(0,t.jsx)(s.code,{children:"false"}),"."]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://stackoverflow.com/questions/77716296/how-kafka-detects-unclean-leader-election",children:"How Kafka detects unclean leader election? - Stack Overflow"})}),"\n",(0,t.jsx)(s.h2,{id:"others",children:"Others"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Idempotent Consumers"}),"\n",(0,t.jsx)(s.li,{children:"Idempotent Producers"}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"http://cloudurable.com/blog/what-is-kafka/index.html",children:"http://cloudurable.com/blog/what-is-kafka/index.html"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"https://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03",children:"https://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"https://www.youtube.com/playlist?list=PLa7VYi0yPIH14oEOfwbcE9_gM5lOZ4ICN",children:"Apache Kafka\xae Architecture | Kafka's Internal Components and How They Work - YouTube"})}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},390281:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/Pasted image 20240213012230-fe58daa64c3bca48d272dfa690a1702d.jpg"},810036:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/Technologies-Kafka-Kafka-Architecture-image1-f9c3459df4492e1f82065aef699896dd.jpg"},704515:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/Technologies-Kafka-Kafka-Architecture-image2-8a664a31a2ddcf0809cc2ee20bbad21e.jpg"},865990:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/Technologies-Kafka-Kafka-Architecture-image3-cd81da28ba7e25b3bf27ebfe34f80266.jpg"},28453:(e,s,n)=>{n.d(s,{R:()=>r,x:()=>o});var i=n(296540);const t={},a=i.createContext(t);function r(e){const s=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:s},e.children)}}}]);