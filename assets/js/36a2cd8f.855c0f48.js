"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[22158],{932666:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>i,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"networking/others/delta-lake-tutorial","title":"Delta Lake Tutorial","description":"Tutorial: Delta Lake | Databricks on AWS","source":"@site/docs/networking/others/delta-lake-tutorial.md","sourceDirName":"networking/others","slug":"/networking/others/delta-lake-tutorial","permalink":"/networking/others/delta-lake-tutorial","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/networking/others/delta-lake-tutorial.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1688412051000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Data formats","permalink":"/networking/others/data-formats"},"next":{"title":"Delta Lake","permalink":"/networking/others/delta-lake"}}');var r=t(474848),l=t(28453);const s={},o="Delta Lake Tutorial",i={},d=[{value:"Tutorial: Delta Lake | Databricks on AWS",id:"tutorial-delta-lake--databricks-on-aws",level:2},{value:"Create a table",id:"create-a-table",level:3},{value:"Upsert to a table",id:"upsert-to-a-table",level:2},{value:"Read a table",id:"read-a-table",level:2},{value:"Write to a table",id:"write-to-a-table",level:2},{value:"Getting Started with Delta Lake",id:"getting-started-with-delta-lake",level:2},{value:"Create a table",id:"create-a-table-1",level:3},{value:"Read a table",id:"read-a-table-1",level:3},{value:"Update table data",id:"update-table-data",level:3},{value:"Overwrite",id:"overwrite",level:4},{value:"Conditional update without overwrite",id:"conditional-update-without-overwrite",level:4},{value:"Read older versions of data using time travel",id:"read-older-versions-of-data-using-time-travel",level:2},{value:"Write a stream of data to a table",id:"write-a-stream-of-data-to-a-table",level:2},{value:"Read a stream of changes from a table",id:"read-a-stream-of-changes-from-a-table",level:2},{value:"Retrieve Delta table history",id:"retrieve-delta-table-history",level:2}];function h(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"delta-lake-tutorial",children:"Delta Lake Tutorial"})}),"\n",(0,r.jsx)(a.h2,{id:"tutorial-delta-lake--databricks-on-aws",children:(0,r.jsx)(a.a,{href:"https://docs.databricks.com/delta/tutorial.html",children:"Tutorial: Delta Lake | Databricks on AWS"})}),"\n",(0,r.jsx)(a.h3,{id:"create-a-table",children:"Create a table"}),"\n",(0,r.jsx)(a.p,{children:"All tables created on Databricks use Delta Lake by default."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:'# Load the data from its source.\ndf = spark.read.load("/databricks-datasets/learning-spark-v2/people/people-10m.delta")\n\n# Write the data to a table.\ntable_name = "people_10m"\ndf.write.saveAsTable(table_name)\n'})}),"\n",(0,r.jsxs)(a.p,{children:["The preceding operations create a new ",(0,r.jsx)(a.a,{href:"https://docs.databricks.com/lakehouse/data-objects.html#managed-table",children:"managed table"})," by using the schema that was inferred from the data. For information about available options when you create a Delta table, see ",(0,r.jsx)(a.a,{href:"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table.html",children:"CREATE TABLE"}),"."]}),"\n",(0,r.jsxs)(a.p,{children:["For managed tables, Databricks determines the location for the data. To get the location, you can use the ",(0,r.jsx)(a.a,{href:"https://docs.databricks.com/delta/table-details.html",children:"DESCRIBE DETAIL"})," statement, for example:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"display(spark.sql('DESCRIBE DETAIL people_10m'))\n"})}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"You can directly use SQL too to create tables"}),"\n",(0,r.jsxs)(a.li,{children:["You can also use the ",(0,r.jsx)(a.code,{children:"DeltaTableBuilder"})," API in Delta Lake to create tables. Compared to the DataFrameWriter APIs, this API makes it easier to specify additional information like column comments, table properties, and ",(0,r.jsx)(a.a,{href:"https://docs.databricks.com/delta/generated-columns.html",children:"generated columns"}),"."]}),"\n"]}),"\n",(0,r.jsx)(a.h2,{id:"upsert-to-a-table",children:"Upsert to a table"}),"\n",(0,r.jsxs)(a.p,{children:["To merge a set of updates and insertions into an existing Delta table, you use the ",(0,r.jsx)(a.a,{href:"https://docs.databricks.com/sql/language-manual/delta-merge-into.html",children:"MERGE INTO"})," statement. For example, the following statement takes data from the source table and merges it into the target Delta table. When there is a matching row in both tables, Delta Lake updates the data column using the given expression. When there is no matching row, Delta Lake adds a new row. This operation is known as an ",(0,r.jsx)(a.em,{children:"upsert"}),"."]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"CREATE OR REPLACE TEMP VIEW people_updates (\n  id, firstName, middleName, lastName, gender, birthDate, ssn, salary\n) AS VALUES\n  (9999998, 'Billy', 'Tommie', 'Luppitt', 'M', '1992-09-17T04:00:00.000+0000', '953-38-9452', 55250),\n  (9999999, 'Elias', 'Cyril', 'Leadbetter', 'M', '1984-05-22T04:00:00.000+0000', '906-51-2137', 48500),\n  (10000000, 'Joshua', 'Chas', 'Broggio', 'M', '1968-07-22T04:00:00.000+0000', '988-61-6247', 90000),\n  (20000001, 'John', '', 'Doe', 'M', '1978-01-14T04:00:00.000+000', '345-67-8901', 55500),\n  (20000002, 'Mary', '', 'Smith', 'F', '1982-10-29T01:00:00.000+000', '456-78-9012', 98250),\n  (20000003, 'Jane', '', 'Doe', 'F', '1981-06-25T04:00:00.000+000', '567-89-0123', 89900);\n\nMERGE INTO people_10m\nUSING people_updates\nON people_10m.id = people_updates.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *;\n"})}),"\n",(0,r.jsxs)(a.p,{children:["If you specify ",(0,r.jsx)(a.code,{children:"*"}),", this updates or inserts all columns in the target table. This assumes that the source table has the same columns as those in the target table, otherwise the query will throw an analysis error."]}),"\n",(0,r.jsxs)(a.p,{children:["You must specify a value for every column in your table when you perform an ",(0,r.jsx)(a.code,{children:"INSERT"})," operation (for example, when there is no matching row in the existing dataset). However, you do not need to update all values."]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.databricks.com/delta/update-schema.html#automatic-schema-evolution-for-delta-lake-merge",children:"Update Delta Lake table schema | Databricks on AWS"})}),"\n",(0,r.jsx)(a.h2,{id:"read-a-table",children:"Read a table"}),"\n",(0,r.jsx)(a.p,{children:"You access data in Delta tables by the table name or the table path, as shown in the following examples:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"people_df = spark.read.table(table_name)\ndisplay(people_df)\n\n-- or\n\npeople_df = spark.read.load(table_path)\ndisplay(people_df)\n"})}),"\n",(0,r.jsx)(a.h2,{id:"write-to-a-table",children:"Write to a table"}),"\n",(0,r.jsx)(a.p,{children:"Delta Lake uses standard syntax for writing data to tables."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- create and insert data\nCREATE TABLE student_copy AS SELECT * FROM student;\n"})}),"\n",(0,r.jsxs)(a.p,{children:["To atomically add new data to an existing Delta table, use ",(0,r.jsx)(a.code,{children:"append"})," mode as in the following examples:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"INSERT INTO people10m SELECT * FROM more_people\n"})}),"\n",(0,r.jsxs)(a.p,{children:["To atomically replace all the data in a table, use ",(0,r.jsx)(a.code,{children:"overwrite"})," mode as in the following examples:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"INSERT OVERWRITE TABLE people10m SELECT * FROM more_people\n"})}),"\n",(0,r.jsx)(a.h2,{id:"getting-started-with-delta-lake",children:(0,r.jsx)(a.a,{href:"https://delta.io/learn/getting-started",children:"Getting Started with Delta Lake"})}),"\n",(0,r.jsx)(a.h3,{id:"create-a-table-1",children:"Create a table"}),"\n",(0,r.jsx)(a.p,{children:"To create a Delta table, write a DataFrame out in the delta format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'data = spark.range(0, 5)\ndata.write.format("delta").save("/tmp/delta-table")\n'})}),"\n",(0,r.jsxs)(a.p,{children:["These operations create a new Delta table using the schema that was ",(0,r.jsx)(a.em,{children:"inferred"})," from your DataFrame."]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-batch.html#-ddlcreatetable",children:"Create a table"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-batch.html#-deltadataframewrites",children:"Write to a table"})}),"\n",(0,r.jsx)(a.h3,{id:"read-a-table-1",children:"Read a table"}),"\n",(0,r.jsxs)(a.p,{children:["You read data in your Delta table by specifying the path to the files ",(0,r.jsx)(a.code,{children:'"/tmp/delta-table"'}),":"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'df = spark.read.format("delta").load("/tmp/delta-table")\ndf.show()\n'})}),"\n",(0,r.jsx)(a.h3,{id:"update-table-data",children:"Update table data"}),"\n",(0,r.jsx)(a.p,{children:"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table:"}),"\n",(0,r.jsx)(a.h4,{id:"overwrite",children:"Overwrite"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'data = spark.range(5, 10)\ndata.write.format("delta").mode("overwrite").save("/tmp/delta-table")\n\n.option("OverwriteSchema", True)\n.option("mergeSchema", True)\n'})}),"\n",(0,r.jsx)(a.p,{children:"If you read this table again, you should see only the values 5-9 you have added because you overwrote the previous data."}),"\n",(0,r.jsx)(a.h4,{id:"conditional-update-without-overwrite",children:"Conditional update without overwrite"}),"\n",(0,r.jsx)(a.p,{children:"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. Here are a few examples:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'from delta.tables import *\nfrom pyspark.sql.functions import *\n\ndeltaTable = DeltaTable.forPath(spark, "/tmp/delta-table")\n\n# Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr("id % 2 == 0"),\n  set = { "id": expr("id + 100") })\n\n# Delete every even value\ndeltaTable.delete(condition = expr("id % 2 == 0"))\n\n# Upsert (merge) new data\nnewData = spark.range(0, 20)\n\ndeltaTable.alias("oldData") \\\n  .merge(\n    newData.alias("newData"),\n    "oldData.id = newData.id") \\\n  .whenMatchedUpdate(set = { "id": col("newData.id") }) \\\n  .whenNotMatchedInsert(values = { "id": col("newData.id") }) \\\n  .execute()\n\ndeltaTable.toDF().show()\n'})}),"\n",(0,r.jsx)(a.p,{children:"You should see that some of the existing rows have been updated and new rows have been inserted."}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-update.html",children:"Table deletes, updates, and merges"})}),"\n",(0,r.jsx)(a.h2,{id:"read-older-versions-of-data-using-time-travel",children:"Read older versions of data using time travel"}),"\n",(0,r.jsx)(a.p,{children:"You can query previous snapshots of your Delta table by using time travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'df = spark.read.format("delta") \\\n  .option("versionAsOf", 0) \\\n  .load("/tmp/delta-table")\n\ndf.show()\n'})}),"\n",(0,r.jsx)(a.p,{children:"You should see the first set of data, from before you overwrote it. Time travel takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again."}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-batch.html#-deltatimetravel",children:"Query an older snapshot of a table (time travel)"})}),"\n",(0,r.jsx)(a.h2,{id:"write-a-stream-of-data-to-a-table",children:"Write a stream of data to a table"}),"\n",(0,r.jsx)(a.p,{children:"You can also write to a Delta table using Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'streamingDf = spark.readStream.format("rate").load()\n\nstream = streamingDf \\\n  .selectExpr("value as id") \\\n  .writeStream.format("delta") \\\n  .option("checkpointLocation", "/tmp/checkpoint") \\\n  .start("/tmp/delta-table")\n'})}),"\n",(0,r.jsx)(a.p,{children:"While the stream is running, you can read the table using the earlier commands."}),"\n",(0,r.jsxs)(a.p,{children:["You can stop the stream by running ",(0,r.jsx)(a.code,{children:"stream.stop()"})," in the same terminal that started the stream."]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-streaming.html",children:"Table streaming reads and writes"})}),"\n",(0,r.jsx)(a.h2,{id:"read-a-stream-of-changes-from-a-table",children:"Read a stream of changes from a table"}),"\n",(0,r.jsxs)(a.p,{children:["While the stream is writing to the Delta table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta table. You can specify which version Structured Streaming should start from by providing the ",(0,r.jsx)(a.code,{children:"startingVersion"})," or ",(0,r.jsx)(a.code,{children:"startingTimestamp"})," option to get changes from that point onwards."]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-streaming.html#-specify-initial-position",children:"Structured Streaming"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'stream2 = spark.readStream.format("delta") \\\n  .load("/tmp/delta-table") \\\n  .writeStream.format("console") \\\n  .start()\n'})}),"\n",(0,r.jsx)(a.h2,{id:"retrieve-delta-table-history",children:"Retrieve Delta table history"}),"\n",(0,r.jsxs)(a.p,{children:["You can retrieve information on the operations, user, timestamp, and so on for each write to a Delta table by running the ",(0,r.jsx)(a.code,{children:"history"})," command. The operations are returned in reverse chronological order. By default table history is retained for 30 days."]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"DESCRIBE HISTORY '/data/events/' -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://docs.databricks.com/delta/history.html",children:"Work with Delta Lake table history | Databricks on AWS"})})]})}function c(e={}){const{wrapper:a}={...(0,l.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},28453:(e,a,t)=>{t.d(a,{R:()=>s,x:()=>o});var n=t(296540);const r={},l=n.createContext(r);function s(e){const a=n.useContext(l);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(l.Provider,{value:a},e.children)}}}]);