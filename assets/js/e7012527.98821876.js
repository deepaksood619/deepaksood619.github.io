"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[71784],{928027:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"ai/llm/tuning","title":"Tuning","description":"The process of adapting a model to a new domain or set of custom use cases by training the model on new data","source":"@site/docs/ai/llm/tuning.md","sourceDirName":"ai/llm","slug":"/ai/llm/tuning","permalink":"/ai/llm/tuning","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/tuning.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1758109499000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Tools","permalink":"/ai/llm/tools"},"next":{"title":"Voice Models","permalink":"/ai/llm/voice-models"}}');var r=i(474848),s=i(28453);const l={},a="Tuning",o={},d=[{value:"Fine Tuning",id:"fine-tuning",level:2},{value:"LoRA",id:"lora",level:3},{value:"LoRA-FA (Frozen-A)",id:"lora-fa-frozen-a",level:3},{value:"VeRA",id:"vera",level:3},{value:"Delta-LoRA",id:"delta-lora",level:3},{value:"LoRA+",id:"lora-1",level:3},{value:"Supervised fine-tuning (SFT)",id:"supervised-fine-tuning-sft",level:2},{value:"Methods for fine-tuning LLMs",id:"methods-for-fine-tuning-llms",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GitHub - microsoft/BitNet: Official inference framework for 1-bit LLMs",id:"github---microsoftbitnet-official-inference-framework-for-1-bit-llms",level:3},{value:"Others",id:"others",level:2},{value:"Links",id:"links",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"tuning",children:"Tuning"})}),"\n",(0,r.jsx)(n.p,{children:"The process of adapting a model to a new domain or set of custom use cases by training the model on new data"}),"\n",(0,r.jsx)(n.h2,{id:"fine-tuning",children:"Fine Tuning"}),"\n",(0,r.jsx)(n.p,{children:"Large language model (LLM) fine-tuning is the process of taking pre-trained models and further training them on smaller, specific datasets to refine their capabilities and improve performance in a particular task or domain. Fine-tuning is about turning general-purpose models and turning them into specialized models. It bridges the gap between generic pre-trained models and the unique requirements of specific applications, ensuring that the language model aligns closely with human expectations."}),"\n",(0,r.jsx)(n.h3,{id:"lora",children:"LoRA"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Introduce two low-rank matrices, A and B, to work alongside the weight matrix W."}),"\n",(0,r.jsx)(n.li,{children:"Adjust these matrices instead of the behemoth W, making updates manageable."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lora-fa-frozen-a",children:"LoRA-FA (Frozen-A)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Takes LoRA a step further by freezing matrix A."}),"\n",(0,r.jsx)(n.li,{children:"Only matrix B is tweaked, reducing the activation memory needed."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vera",children:"VeRA"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"All about efficiency: matrices A and B are fixed and shared across all layers."}),"\n",(0,r.jsx)(n.li,{children:"Focuses on tiny, trainable scaling vectors in each layer, making it super memory-friendly."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"delta-lora",children:"Delta-LoRA"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A twist on LoRA: adds the difference (delta) between products of matrices A and B across training steps to the main weight matrix W."}),"\n",(0,r.jsx)(n.li,{children:"Offers a dynamic yet controlled approach to parameter updates."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lora-1",children:"LoRA+"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"An optimized variant of LoRA where matrix B gets a higher learning rate. This tweak leads to faster and more effective learning."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"5 Techniques to fine-tune LLMs",src:i(900517).A+"",width:"990",height:"1154"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://blog.dailydoseofds.com/p/top-4-llm-fine-tuning-frameworks",children:"Top 4 LLM Fine-tuning Frameworks! - by Avi Chawla"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.dailydoseofds.com/understanding-lora-derived-techniques-for-optimal-llm-fine-tuning/",children:"Understanding LoRA-derived Techniques for Optimal LLM Fine-tuning"})}),"\n",(0,r.jsx)(n.h2,{id:"supervised-fine-tuning-sft",children:"Supervised fine-tuning (SFT)"}),"\n",(0,r.jsx)(n.p,{children:"Supervised fine-tuning means updating a pre-trained language model using labeled data to do a specific task. The data used has been checked earlier. This is different from unsupervised methods, where data isn't checked. Usually, the initial training of the language model is unsupervised, but fine-tuning is supervised."}),"\n",(0,r.jsx)(n.h2,{id:"methods-for-fine-tuning-llms",children:"Methods for fine-tuning LLMs"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Instruction fine-tuning"}),"\n",(0,r.jsx)(n.li,{children:"Full fine-tuning"}),"\n",(0,r.jsxs)(n.li,{children:["Parameter-efficient fine-tuning (PEFT)","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=HcVtpLAGMXo",children:"LLM (Parameter Efficient) Fine Tuning - Explained! - YouTube"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"github---microsoftbitnet-official-inference-framework-for-1-bit-llms",children:(0,r.jsx)(n.a,{href:"https://github.com/microsoft/BitNet",children:"GitHub - microsoft/BitNet: Official inference framework for 1-bit LLMs"})}),"\n",(0,r.jsxs)(n.p,{children:["bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support\xa0",(0,r.jsx)(n.strong,{children:"fast"}),"\xa0and\xa0",(0,r.jsx)(n.strong,{children:"lossless"}),"\xa0inference of 1.58-bit models on CPU (with NPU and GPU support coming next)."]}),"\n",(0,r.jsxs)(n.p,{children:["The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of\xa0",(0,r.jsx)(n.strong,{children:"1.37x"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"5.07x"}),"\xa0on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by\xa0",(0,r.jsx)(n.strong,{children:"55.4%"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"70.0%"}),", further boosting overall efficiency. On x86 CPUs, speedups range from\xa0",(0,r.jsx)(n.strong,{children:"2.37x"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"6.17x"}),"\xa0with energy reductions between\xa0",(0,r.jsx)(n.strong,{children:"71.9%"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"82.2%"}),". Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.reddit.com/r/LocalLLaMA/comments/1g6jmwl/bitnet_inference_framework_for_1bit_llms/",children:"BitNet - Inference framework for 1-bit LLMs : r/LocalLLaMA"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://medium.com/@kldurga999/bitnet-cpp-an-opensource-llm-platform-by-microsoft-8cdeccf272c2",children:"Bitnet.cpp an opensource LLM platform by Microsoft | by Kldurga | Medium"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=C4OYJAs4O60",children:"bitnet.cpp from Microsoft: Run LLMs locally on CPU! (hands-on) - YouTube"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2504.12285",children:"[2504.12285] BitNet b1.58 2B4T Technical Report"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"others",children:"Others"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Instruct tuning / Instruction Tuning"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/ggml-org/ggml",children:"GitHub - ggml-org/ggml: Tensor library for machine learning"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ggml-org/ggml/blob/master/docs/gguf.md",children:"ggml/docs/gguf.md at master \xb7 ggml-org/ggml \xb7 GitHub"})}),"\n",(0,r.jsx)(n.li,{children:"GGUF - A binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.superannotate.com/blog/llm-fine-tuning",children:"Fine-tuning large language models (LLMs) in 2024 | SuperAnnotate"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://mlops.substack.com/p/pruning-aware-trainingpat-in-llms",children:"Pruning Aware Training(PAT) in LLMs - by Bugra Akyildiz"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=t-0s_2uZZU0",children:"Generative AI Fine Tuning LLM Models Crash Course - YouTube"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.dailydoseofds.com/p/kv-caching-in-llms-explained-visually/",children:"KV Caching in LLMs, explained visually"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://jrodthoughts.medium.com/inside-calm-google-deepminds-unique-method-to-augment-llms-with-other-llms-92cb9526e66c",children:"Inside CALM: Google DeepMind\u2019s Unique Method to Augment LLMs with Other LLMs | by Jesus Rodriguez | Medium"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/google-deepmind/calm",children:"GitHub - google-deepmind/calm"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://research.google/blog/accelerating-text-generation-with-confident-adaptive-language-modeling-calm/",children:"Accelerating text generation with Confident Adaptive Language Modeling (CALM)"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2408.13296v1",children:"[2408.13296v1] The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},900517:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/Screenshot 2025-03-10 at 12.37.42 PM-b102673bba25e478b543d310f29fa379.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>a});var t=i(296540);const r={},s=t.createContext(r);function l(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);