"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[79679],{858205:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"ai/libraries/distributed-training","title":"Distributed Training","description":"Architecture of Distributed Training","source":"@site/docs/ai/libraries/distributed-training.md","sourceDirName":"ai/libraries","slug":"/ai/libraries/distributed-training","permalink":"/ai/libraries/distributed-training","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/libraries/distributed-training.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1678191863000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Deep Learning Frameworks","permalink":"/ai/libraries/deep-learning-frameworks"},"next":{"title":"JAX","permalink":"/ai/libraries/jax"}}');var n=i(474848),a=i(28453);const s={},o="Distributed Training",d={},l=[{value:"Architecture of Distributed Training",id:"architecture-of-distributed-training",level:2},{value:"Data-parallel training with asynchronous updates (Using Google Cloud ML)",id:"data-parallel-training-with-asynchronous-updates-using-google-cloud-ml",level:2}];function c(e){const t={h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"distributed-training",children:"Distributed Training"})}),"\n",(0,n.jsx)(t.h2,{id:"architecture-of-distributed-training",children:"Architecture of Distributed Training"}),"\n",(0,n.jsx)(t.p,{children:"There are three basic strategies to train a model with multiple nodes:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Data-parallel training with synchronous updates."}),"\n",(0,n.jsx)(t.li,{children:"Data-parallel training with asynchronous updates."}),"\n",(0,n.jsx)(t.li,{children:"Model-parallel training."}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"data-parallel-training-with-asynchronous-updates-using-google-cloud-ml",children:"Data-parallel training with asynchronous updates (Using Google Cloud ML)"}),"\n",(0,n.jsx)(t.p,{children:"A training job is executed using the following types of nodes:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Parameter server node. Update parameters with gradient vectors from worker and chief work nodes."}),"\n",(0,n.jsx)(t.li,{children:"Worker node. Calculate a gradient vector from the training dataset."}),"\n",(0,n.jsx)(t.li,{children:"Chief worker node. Coordinate the operations of multiple workers, in addition to working as one of the worker nodes."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Because you can use the data-parallel strategy regardless of the model structure, it is a good starting point for applying the distributed training method to your custom model. In data-parallel training, the whole model is shared with all worker nodes. Each node calculates gradient vectors independently from some part of the training dataset in the same manner as the mini-batch processing. The calculated gradient vectors are collected into the parameter server node, and model parameters are updated with the total summation of the gradient vectors. If you distribute 10,000 batches among 10 worker nodes, each node works on roughly 1,000 batches."}),"\n",(0,n.jsx)(t.p,{children:"Data-parallel training can be done with either synchronous or asynchronous updates. When using asynchronous updates, the parameter server applies each gradient vector independently, right after receiving it from one of the worker nodes, as shown in the following diagram."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"image",src:i(888524).A+"",width:"881",height:"410"})}),"\n",(0,n.jsx)(t.p,{children:"In a typical deployment, there are a few parameter server nodes, a single chief worker node, and several worker nodes. When you submit a training job through the service API, these nodes are automatically deployed in your project."}),"\n",(0,n.jsx)(t.p,{children:"The following diagram describes the architecture for running a distributed training job on Cloud ML Engine and using Cloud Datalab to execute predictions with your trained model."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"image",src:i(714971).A+"",width:"999",height:"454"})})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},888524:(e,t,i)=>{i.d(t,{A:()=>r});const r=i.p+"assets/images/ML-Model-Deployment_Distributed-Training-image1-f1da2c36376ca489373be904f45208f4.jpg"},714971:(e,t,i)=>{i.d(t,{A:()=>r});const r=i.p+"assets/images/ML-Model-Deployment_Distributed-Training-image2-59a0fc88b7d2221666450b27247f439a.jpg"},28453:(e,t,i)=>{i.d(t,{R:()=>s,x:()=>o});var r=i(296540);const n={},a=r.createContext(n);function s(e){const t=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),r.createElement(a.Provider,{value:t},e.children)}}}]);