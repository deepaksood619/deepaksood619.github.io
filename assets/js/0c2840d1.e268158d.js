"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[80236],{97533:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var n=a(785893),r=a(511151);const s={},o="Examples",i={id:"technologies/apache/apache-spark/08-examples",title:"Examples",description:"Glue Transformation from Aurora DB to Parquet in s3",source:"@site/docs/technologies/apache/apache-spark/08-examples.md",sourceDirName:"technologies/apache/apache-spark",slug:"/technologies/apache/apache-spark/08-examples",permalink:"/technologies/apache/apache-spark/08-examples",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache/apache-spark/08-examples.md",tags:[],version:"current",lastUpdatedAt:1701793554,formattedLastUpdatedAt:"Dec 5, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Joins",permalink:"/technologies/apache/apache-spark/07-joins"},next:{title:"DataFrame",permalink:"/technologies/apache/apache-spark/09-dataframe"}},p={},l=[{value:"Glue Transformation from Aurora DB to Parquet in s3",id:"glue-transformation-from-aurora-db-to-parquet-in-s3",level:3},{value:"PySpark Tutorial",id:"pyspark-tutorial",level:3},{value:"Queries",id:"queries",level:3},{value:"Load testing data",id:"load-testing-data",level:4},{value:"Timestamps",id:"timestamps",level:4},{value:"Querying postgres db using jdbc",id:"querying-postgres-db-using-jdbc",level:4}];function d(e){const t={a:"a",code:"code",h1:"h1",h3:"h3",h4:"h4",p:"p",pre:"pre",...(0,r.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"examples",children:"Examples"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'# SQL table to parquet\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.jdbc("YOUR_MYSQL_JDBC_CONN_STRING",  "YOUR_TABLE", properties={"user": "YOUR_USER", "password": "YOUR_PASSWORD"})\ndf.write.parquet("YOUR_HDFS_FILE")\n\n# CSV to Parquet\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nif __name__ == "__main__":\n    sc = SparkContext(appName="CSV2Parquet")\n    sqlContext = SQLContext(sc)\n\n    schema = StructType([\n            StructField("col1", IntegerType(), True),\n            StructField("col2", IntegerType(), True),\n            StructField("col3", StringType(), True),\n            StructField("col4", StringType(), True),\n            StructField("col5", StringType(), True),\n            StructField("col6", DoubleType(), True)])\n\n    rdd = sc.textFile("/home/sarvesh/Desktop/input.csv").map(lambda line: line.split(","))\n    df = sqlContext.createDataFrame(rdd, schema)\n    df.write.parquet(\'/home/sarvesh/Desktop/input-parquet\')\n\n# Parquet to csv\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nif __name__ == "__main__":\n    sc = SparkContext(appName="Parquet2CSV")\n    sqlContext = SQLContext(sc)\nreaddf = sqlContext.read.parquet(\'/home/sarvesh/Desktop/submissions-parquet\')\n    readdf.rdd.map(tuple).map(lambda row: str(row[0]) + "," + str(row[1]) + ","+ str(row[2]) + ","+ str(row[3])+ ","+ str(row[4])+","+ str(row[5])).saveAsTextFile("/home/sarvesh/Desktop/parquet-to-csv.csv")\n\n# http://blogs.quovantis.com/how-to-convert-csv-to-parquet-files/\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName("Python Spark SQL basic example") \\\n    .config("spark.some.config.option", "some-value") \\\n    .getOrCreate()\n\ndf = spark.read.csv("/Users/deepak/Repositories/Python-Competitive-Programming/Experiments/creditcard.csv", header=True, sep=",", inferSchema=True)\n\ndf.write.parquet(\'/Users/deepak/Repositories/Python-Competitive-Programming/Experiments/output-parquet\')\n\ndata.count(), len(data.columns)\ndata.show(5)\ndata.printSchema()\ndata.select("Name","Platform","User_Score","User_Count").show(15, truncate=False)\ndata.describe(["User_Score","User_Count"]).show()\ndata.groupBy("Platform").count().orderBy("count", ascending=False).show(10)\n\ncondition1 = (data.User_Score.isNotNull()) | (data.User_Count.isNotNull())\ncondition2 = data.User_Score != "tbd"\ndata = data.filter(condition1).filter(condition2)\n\n# https://towardsdatascience.com/pyspark-import-any-data-f2856cda45fd\n\n# Check if table exists in Spark Catlogue\nspark.catalog.tableExists("schema.dev.table_name")\n'})}),"\n",(0,n.jsx)(t.h3,{id:"glue-transformation-from-aurora-db-to-parquet-in-s3",children:"Glue Transformation from Aurora DB to Parquet in s3"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:' import sys\n from awsglue.transforms import *\n from awsglue.utils import getResolvedOptions\n from pyspark.context import SparkContext\n from awsglue.context import GlueContext\n from awsglue.job import Job\n\n args = getResolvedOptions(sys.argv, [\'JOB_NAME\'])\n\n sc = SparkContext()\n glueContext = GlueContext(sc)\n spark = glueContext.spark_session\n job = Job(glueContext)\n job.init(args [\'JOB_NAME\'], args)\n\n datasource0 = glueContext.create_dynamic_frame.from_catalog(database = "aurora", table_name = "aurorasttash_website_live_userdevicesms_old", transformation_ctx = "datasource0")\n # datasource0 = glueContext.create_dynamic_frame_from_options("s3", {\'paths\': ["s3://example-migration-data/test"], \'recurse\':True, \'groupFiles\': \'inPartition\', \'groupSize\': \'104857600\'}, format="json")\n\n applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("device_id", "string", "device_id", "string"), ("sender", "string", "sender", "string"), ("message_type", "string", "message_type", "string"), ("id", "int", "id", "int"), ("customer_id", "int", "customer_id", "int"), ("message", "string", "message", "string"), ("create_date", "timestamp", "create_date", "timestamp"), ("sms_time", "timestamp", "sms_time", "timestamp")], transformation_ctx = "applymapping1")\n\n resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = "make_struct", transformation_ctx = "resolvechoice2")\n\n dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = "dropnullfields3")\n # datasource_df = dropnullfields3.repartition(2)\n datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = "s3", connection_options = {"path": "s3://example-migration-data/glue/user_device_sms_old"}, format = "parquet", transformation_ctx = "datasink4")\n job.commit()\n'})}),"\n",(0,n.jsx)(t.h3,{id:"pyspark-tutorial",children:"PySpark Tutorial"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:' # File location and type\n file_location = "/FileStore/tables/game_skater_stats.csv"\n file_type = "csv"\n\n # CSV options\n infer_schema = "true"\n first_row_is_header = "true"\n delimiter = ","\n\n # The applied options are for CSV files. For other file types, these will be ignored.\n df = spark.read.format(file_type) \\\n   .option("inferSchema", infer_schema) \\\n   .option("header", first_row_is_header) \\\n   .option("sep", delimiter) \\\n   .load(file_location)\n\n display(df)\n\n # how to take the dataframe from the past snippet and save it as a parquet file on DBFS, and then reload the dataframe from the saved parquet file.\n df.write.save(\'/FileStore/parquet/game_skater_stats\',\n                format=\'parquet\')\n df = spark.read.load("/FileStore/parquet/game_skater_stats")\n display(df)\n\n'})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.a,{href:"https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873",children:"https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873"})}),"\n",(0,n.jsx)(t.h3,{id:"queries",children:"Queries"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"table_df = table_df.union(table_df)\n\ntable_df.display()\ntable_df.show()\ndisplay(table_df)\n\ntable_df.count()\n\ntable_df.limit(10).display()\n\ntable_df_2.replace('Revenue Fee', 'Maintanence Fee').display()\n\n# check column types\ntable_df.schema\ntable_df.dtypes\n\n# Add a new column\ntable_df_2 = table_df.withColumn(\"bonus_amount\", table_df.salary*0.3)\n"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.a,{href:"https://medium.com/@harun.raseed093/show-vs-display-7a7185f3ef65",children:"Show() Vs Display(). To Display the dataframe in a tabular\u2026 | by Harun Raseed Basheer | Medium"})}),"\n",(0,n.jsx)(t.h4,{id:"load-testing-data",children:"Load testing data"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"# Load the data from its source.\ndf = spark.read.load(\"/databricks-datasets/learning-spark-v2/people/people-10m.delta\")\n\n# Write the data to a table.\n\ntable_name = \"people_10m\"\n\ndf.write.saveAsTable(table_name)\n\n# Load a table in spark dataframe\npeople_df = spark.read.table('people_10m')\ndisplay(people_df)\n\ndf = spark.sql('SELECT * FROM people_10m WHERE id >= 9999998')\n"})}),"\n",(0,n.jsx)(t.h4,{id:"timestamps",children:"Timestamps"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'from pyspark.sql.functions import to_timestamp, lit\n\n# add a new column with a specific date\nspecific_date = "2022-03-13 10:30:00"\ndf3 = df.withColumn("specific_date", to_timestamp(lit(specific_date)))\n\ndf2 = df.withColumn("pipelineAt", F.current_timestamp())\n'})}),"\n",(0,n.jsx)(t.h4,{id:"querying-postgres-db-using-jdbc",children:"Querying postgres db using jdbc"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'tablename = \'xyz\'\n\n# Load Table\nsource = f\'"{tablename}"\'\n\ndf = (spark.read.format("jdbc")\n .option("url", URL)\n .option("dbtable", f"(select * from {source} limit 1000)")\n .option("user", user)\n .option("password", password)\n .option("ssl", True)\n .option("sslmode", "require")\n .option("sslrootcert", ssl_path)\n .load())\n\nprint(df.count())\n'})})]})}function c(e={}){const{wrapper:t}={...(0,r.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},511151:(e,t,a)=>{a.d(t,{Z:()=>i,a:()=>o});var n=a(667294);const r={},s=n.createContext(r);function o(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);