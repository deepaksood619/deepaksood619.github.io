"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[32191],{543426:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"databases/data-warehouses/bigquery/queries","title":"Queries","description":"SQL comparison","source":"@site/docs/databases/data-warehouses/bigquery/queries.md","sourceDirName":"databases/data-warehouses/bigquery","slug":"/databases/data-warehouses/bigquery/queries","permalink":"/databases/data-warehouses/bigquery/queries","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/databases/data-warehouses/bigquery/queries.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1739996367000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Optimizations","permalink":"/databases/data-warehouses/bigquery/optimizations"},"next":{"title":"Query Optimizations","permalink":"/databases/data-warehouses/bigquery/query-optimizations"}}');var i=t(474848),s=t(28453);const r={},o="Queries",l={},d=[{value:"SQL comparison",id:"sql-comparison",level:3},{value:"BigQuery Table usage in a Query - Information Schema",id:"bigquery-table-usage-in-a-query---information-schema",level:2},{value:"Best Practice: Utilize Expiration Settings",id:"best-practice-utilize-expiration-settings",level:3},{value:"Identifying Tables That Are Not Being Used for a Long Time",id:"identifying-tables-that-are-not-being-used-for-a-long-time",level:3},{value:"Optimizing Streaming Insert Cost",id:"optimizing-streaming-insert-cost",level:3},{value:"Data scanning analysis",id:"data-scanning-analysis",level:3},{value:"SQL",id:"sql",level:2},{value:"Commands",id:"commands",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"queries",children:"Queries"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SELECT * REPLACE\nSELECT * EXCEPT\n"})}),"\n",(0,i.jsx)(n.h3,{id:"sql-comparison",children:"SQL comparison"}),"\n",(0,i.jsxs)(n.p,{children:["BigQuery ",(0,i.jsx)(n.a,{href:"https://cloud.google.com/bigquery/docs/reference/standard-sql/",children:"standard SQL"})," supports compliance with the SQL 2011 standard and has extensions that support querying nested and repeated data. Redshift SQL is based on PostgreSQL but has several differences which are detailed in the ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html",children:"Redshift documentation"}),". For a detailed comparison between Redshift and BigQuery SQL syntax and functions, see the ",(0,i.jsx)(n.a,{href:"https://cloud.google.com/solutions/migration/dw2bq/redshift/redshift-bq-sql-translation-reference",children:"Redshift to BigQuery SQL translation reference"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"bigquery-table-usage-in-a-query---information-schema",children:"BigQuery Table usage in a Query - Information Schema"}),"\n",(0,i.jsx)(n.h3,{id:"best-practice-utilize-expiration-settings",children:"Best Practice: Utilize Expiration Settings"}),"\n",(0,i.jsx)(n.p,{children:"To optimize your BigQuery usage, it is crucial to manage the lifecycle of your tables and partitions effectively. By configuring expiration settings, you can automatically remove unneeded tables and partitions, thus saving storage costs and maintaining a clean dataset environment."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://cloud.google.com/bigquery/docs/best-practices-storage#use_the_expiration_settings_to_remove_unneeded_tables_and_partitions",children:"Default Table Expiration for Datasets"}),": Set a default expiration time for all new tables created in a dataset. This ensures that any table not explicitly configured with an expiration date will be automatically deleted after the specified period."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://cloud.google.com/bigquery/docs/updating-datasets#table-expiration",children:"Table Expiration:"})," Define specific expiration times for individual tables. This allows you to control the retention period for each table based on its relevance and usage patterns."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration",children:"Partition Expiration for Partitioned Tables"}),": For partitioned tables, configure expiration times at the partition level. This helps in managing data retention for specific time periods within the table, such as removing older data while retaining recent data."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Implementing these expiration settings helps in automating data lifecycle management, reducing manual efforts, and optimizing storage costs."}),"\n",(0,i.jsx)(n.h3,{id:"identifying-tables-that-are-not-being-used-for-a-long-time",children:"Identifying Tables That Are Not Being Used for a Long Time"}),"\n",(0,i.jsx)(n.p,{children:"The following query is essential for customers using Google BigQuery because it identifies and records which tables are utilized in each SQL query. By extracting dataset and table names from the queries, it offers a clear understanding of which tables are accessed during query executions. This information is invaluable for data governance, query optimization, and resource allocation."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Governance:"})," Understanding which tables are accessed ensures that you can maintain data integrity and security. You can manage access controls more effectively and ensure sensitive data is only accessed by authorized users."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Query Optimization:"})," By identifying frequently accessed tables, you can prioritize optimization efforts, such as indexing or restructuring data, to improve query performance."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Allocation:"})," Analyzing table usage patterns allows for informed decisions about dataset organization and resource allocation, ensuring efficient use of the BigQuery environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Historical Context:"})," Tracking changes in table usage patterns over time provides insights into data usage evolution. This helps predict future trends and plan capacity accordingly."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Overall, this query enhances transparency and control over table dependencies within BigQuery, facilitating better data management and query performance optimization."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"WITH QueryTableInfo AS (\n  SELECT\n    project_id,\n    job_id,\n    start_time,\n    end_time,\n    query,\n    COUNT(*) TOTAL_QUERIES,\n    SUM(\n      total_slot_ms / TIMESTAMP_DIFF(\n        end_time, creation_time, MILLISECOND\n      )\n    ) AVG_SLOT_USAGE,\n    SUM(\n      TIMESTAMP_DIFF(end_time, creation_time, SECOND)\n    ) TOTAL_DURATION_IN_SECONDS,\n    AVG(\n      TIMESTAMP_DIFF(end_time, creation_time, SECOND)\n    ) AVG_DURATION_IN_SECONDS,\n    MAX(\n      TIMESTAMP_DIFF(end_time, creation_time, SECOND)\n    ) Max_DURATION_IN_SECONDS,\n    SUM(total_bytes_processed * 10e - 12) TOTAL_PROCESSED_TB,\n    EXTRACT(\n      DATE\n      FROM\n        creation_time\n    ) AS EXECUTION_DATE,\n    EXTRACT(\n      HOUR\n      FROM\n        creation_time\n    ) AS EXECUTION_TIME,\n    user_email AS USER,\n    -- Extract dataset and table names separately using regular expression\n    REGEXP_EXTRACT_ALL(query, r '`([^`]+)`') AS TABLE_INFO\n  FROM\n    `dealshare-d82f7.region-asia-south1.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n  WHERE\n    state = 'DONE'\n    AND statement_type = 'SELECT'\n    AND creation_time BETWEEN TIMESTAMP_SUB(\n      CURRENT_TIMESTAMP(),\n      INTERVAL 1 DAY\n    )\n    AND CURRENT_TIMESTAMP()\n  GROUP BY\n    EXECUTION_DATE,\n    EXECUTION_TIME,\n    USER,\n    project_id,\n    job_id,\n    start_time,\n    end_time,\n    query\n)\nSELECT\n  *,\n  ARRAY < STRUCT < dataset_name STRING,\n  table_name STRING >> [ STRUCT(\n    SPLIT(\n      TABLE_INFO[SAFE_OFFSET(0) ],\n      '.'\n    ) [SAFE_OFFSET(0) ] AS dataset_name,\n    SPLIT(\n      TABLE_INFO[SAFE_OFFSET(0) ],\n      '.'\n    ) [SAFE_OFFSET(1) ] AS table_name\n  ) ] AS USED_TABLES_STR\nFROM\n  QueryTableInfo\nORDER BY\n  EXECUTION_DATE,\n  EXECUTION_TIME,\n  AVG_SLOT_USAGE;\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image",src:t(770154).A+"",width:"1238",height:"748"})}),"\n",(0,i.jsx)(n.p,{children:"With the changes above that can be implemented for BigQuery storage costs, one could have potential savings of around 20-30% by switching from logical storage to physical storage, though actual savings may vary based on the data type."}),"\n",(0,i.jsx)(n.h3,{id:"optimizing-streaming-insert-cost",children:"Optimizing Streaming Insert Cost"}),"\n",(0,i.jsx)(n.p,{children:"Recommendation: Optimize Streaming Insert Operations"}),"\n",(0,i.jsx)(n.p,{children:"Streaming inserts can contribute significantly to BigQuery costs, especially at scale. Consider the following strategies to optimize streaming insert costs:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Batch Inserts:"})," Instead of streaming data continuously, batch your inserts where possible. This reduces the frequency of insert operations and can lead to lower overall costs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use of Cloud Pub/Sub:"})," If feasible, use Cloud Pub/Sub to batch and deliver messages to BigQuery in larger chunks rather than individually. This reduces the number of insert operations and can optimize costs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control Data Volume:"})," Implement data filtering and validation before data reaches BigQuery to reduce unnecessary data ingestion, thus lowering costs."]}),"\n",(0,i.jsxs)(n.li,{children:["If you're using Google Analytics for Firebase or Google Analytics to stream data into BigQuery, optimizing streaming costs involves understanding how data flows and making strategic adjustments. Here are several ways to reduce streaming costs in this context:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Google Analytics allows you to adjust the data sampling rate, which affects the volume of data streamed into BigQuery. By lowering the sampling rate, especially for less critical or less frequently analyzed data, you can reduce the volume of data being processed and thereby lower costs."}),"\n",(0,i.jsx)(n.li,{children:"Adjust sampling settings in Google Analytics to strike a balance between data accuracy and cost efficiency. Focus high sampling on critical data and lower it for less critical data."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Event Filtering:"})," Configure Firebase or Google Analytics to filter out events that are not necessary for your analysis, reducing the volume of data being ingested."]}),"\n",(0,i.jsx)(n.li,{children:"Data Transformation: Use Firebase functions or Google Cloud Functions to pre-process and filter data before it enters BigQuery, ensuring only relevant data is streamed."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-scanning-analysis",children:"Data scanning analysis"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- find how much data is being scanned by month\nSELECT\n  FORMAT_TIMESTAMP('%Y-%m', creation_time) AS month,\n  ROUND(SUM(total_bytes_processed) / POW(2, 30), 2) AS total_gb_scanned\nFROM\n  `region-asia-south1.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\nWHERE\n  state = 'DONE'\n  AND job_type = 'QUERY'\n  AND creation_time BETWEEN TIMESTAMP('2024-01-01') AND CURRENT_TIMESTAMP()\nGROUP BY\n  month\nORDER BY\n  month;\n\n-- find how much data is being scanned by day\nSELECT\n  FORMAT_TIMESTAMP('%Y-%m-%d', creation_time) AS day,\n  ROUND(SUM(total_bytes_processed) / POW(2, 30), 2) AS total_gb_scanned\nFROM\n  `region-REGION_NAME.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\nWHERE\n  state = 'DONE'\n  AND job_type = 'QUERY'\n  AND creation_time BETWEEN TIMESTAMP('2025-01-01') AND CURRENT_TIMESTAMP()\nGROUP BY\n  day\nORDER BY\n  day;\n\n-- find the top queries that scanned the most amount of data\nSELECT\n  query,\n  user_email,\n  COUNT(*) AS query_count,\n  ROUND(SUM(total_bytes_processed) / POW(2, 30), 2) AS total_gb_scanned,\n  ROUND(SUM(total_bytes_processed) / COUNT(*) / POW(2, 30), 2) AS avg_gb_scanned_per_query,\n  ARRAY_AGG(FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', creation_time) ORDER BY creation_time) AS query_run_timestamps\nFROM\n  `region-asia-south1.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\nWHERE\n  state = 'DONE'\n  AND job_type = 'QUERY'\n  AND creation_time BETWEEN TIMESTAMP('2025-01-01') AND CURRENT_TIMESTAMP()\nGROUP BY\n  query,\n  user_email\nORDER BY\n  total_gb_scanned DESC\nLIMIT 10;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"sql",children:"SQL"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- standardSQL\nSELECT\n    departure_airport,\n    arrival_airport,\n    COUNT(1) AS num_flights\nFROM\n    `bigquery-samples.airline_ontime_data.flights`\nGROUP BY\n    departure_airport,\n    arrival_airport\nORDER BY\n    num_flights DESC\nLIMIT\n    10\n\n-- standardSQL\nSELECT\n    departure_delay,\n    COUNT(1) AS num_flights,\n    APPROX_QUANTILES(arrival_delay, 5) AS arrival_delay_quantiles\nFROM\n    `bigquery-samples.airline_ontime_data.flights`\nGROUP BY\n    departure_delay\nHAVING\n    num_flights > 100\nORDER BY\n    departure_delay ASC\n\n-- get all datasets\nSELECT\n  schema_name AS dataset_id\nFROM\n  `project_id.region-asia-south1.INFORMATION_SCHEMA.SCHEMATA`;\n\n-- size of each dataset\nSELECT SUM(size_bytes) AS bytes FROM dataset.__TABLES__;\n\n-- size of tables desc for a dataset\nSELECT\n  table_id,\n  size_bytes / (1024 * 1024 * 1024) AS table_size_gb\nFROM\n  `dataset_id.__TABLES__`\nORDER BY\n  table_size_gb DESC;\n\n\n-- get count of tables\nSELECT\n  table_schema,\n  COUNT(*) AS number_of_tables\nFROM\n  `dealshare-d82f7.region-asia-south1.INFORMATION_SCHEMA.TABLES`\nGROUP BY\n  table_schema\norder by number_of_tables desc;\n\n-- delete data\nTRUNCATE TABLE `dataset-name.database_name.table_name`;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"commands",children:"Commands"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from google.cloud import bigquery\nclient = bigquery.Client()\ndataset_ref = client.dataset("hacker_news", project="bigquery-public-data")\ndataset_ref = client.dataset("chicago_crime", project="bigquery-public-data")\ndataset = client.get_dataset(dataset_ref)\n'})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},770154:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/Screenshot 2025-01-27 at 9.19.29 PM-7aef5c61f33d8da999cabd6451a793e0.jpg"},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(296540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);