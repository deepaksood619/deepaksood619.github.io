"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[68413],{128181:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"ai/move-37/q-learning-algorithms","title":"Q-Learning Algorithms","description":"Q-Learning algorithms are a family of Reinforcement Learning algorithms.","source":"@site/docs/ai/move-37/q-learning-algorithms.md","sourceDirName":"ai/move-37","slug":"/ai/move-37/q-learning-algorithms","permalink":"/ai/move-37/q-learning-algorithms","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/q-learning-algorithms.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1678191863000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Others","permalink":"/ai/move-37/others"},"next":{"title":"Quizzes","permalink":"/ai/move-37/quizzes"}}');var n=a(474848),o=a(28453);const r={},s="Q-Learning Algorithms",l={},c=[{value:"Policy Gradient Method - Attempts to learn functions which directly map an observation to an action",id:"policy-gradient-method---attempts-to-learn-functions-which-directly-map-an-observation-to-an-action",level:2},{value:"Q-Learning - Attempts to learn the value of being in a given state, and taking a specific action there",id:"q-learning---attempts-to-learn-the-value-of-being-in-a-given-state-and-taking-a-specific-action-there",level:2},{value:"Policy - Policy is a simple lookup table: state -&gt; best action",id:"policy---policy-is-a-simple-lookup-table-state---best-action",level:2},{value:"Reward - the reward from our immediate action, plus all discounted future rewards from applying the current policy (Denoted by capital G)",id:"reward---the-reward-from-our-immediate-action-plus-all-discounted-future-rewards-from-applying-the-current-policy-denoted-by-capital-g",level:2}];function d(e){const t={h1:"h1",h2:"h2",header:"header",img:"img",p:"p",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"q-learning-algorithms",children:"Q-Learning Algorithms"})}),"\n",(0,n.jsx)(t.p,{children:"Q-Learning algorithms are a family of Reinforcement Learning algorithms."}),"\n",(0,n.jsx)(t.p,{children:"Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."}),"\n",(0,n.jsx)(t.h2,{id:"policy-gradient-method---attempts-to-learn-functions-which-directly-map-an-observation-to-an-action",children:"Policy Gradient Method - Attempts to learn functions which directly map an observation to an action"}),"\n",(0,n.jsx)(t.h2,{id:"q-learning---attempts-to-learn-the-value-of-being-in-a-given-state-and-taking-a-specific-action-there",children:"Q-Learning - Attempts to learn the value of being in a given state, and taking a specific action there"}),"\n",(0,n.jsx)(t.p,{children:"Q - Quality"}),"\n",(0,n.jsx)(t.p,{children:"Q - Long term discounted reward we expect from taking action a in state s"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"image",src:a(51147).A+"",width:"998",height:"376"})}),"\n",(0,n.jsx)(t.p,{children:"The policy for state s is to choose the actual bias Q value."}),"\n",(0,n.jsx)(t.h2,{id:"policy---policy-is-a-simple-lookup-table-state---best-action",children:"Policy - Policy is a simple lookup table: state -> best action"}),"\n",(0,n.jsx)(t.h2,{id:"reward---the-reward-from-our-immediate-action-plus-all-discounted-future-rewards-from-applying-the-current-policy-denoted-by-capital-g",children:"Reward - the reward from our immediate action, plus all discounted future rewards from applying the current policy (Denoted by capital G)"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"image",src:a(536783).A+"",width:"999",height:"527"})})]})}function p(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},51147:(e,t,a)=>{a.d(t,{A:()=>i});const i=a.p+"assets/images/Q-Learning-Algorithms-image1-b644a6eca2c7420bb64ce0f749245256.jpg"},536783:(e,t,a)=>{a.d(t,{A:()=>i});const i=a.p+"assets/images/Q-Learning-Algorithms-image2-f78aa5093c814d82fdedda9702a8455b.jpg"},28453:(e,t,a)=>{a.d(t,{R:()=>r,x:()=>s});var i=a(296540);const n={},o=i.createContext(n);function r(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),i.createElement(o.Provider,{value:t},e.children)}}}]);