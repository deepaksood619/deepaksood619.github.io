"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[97851],{629477:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"ai/llm/design-patterns","title":"Design patterns","description":"In-context learning","source":"@site/docs/ai/llm/design-patterns.md","sourceDirName":"ai/llm","slug":"/ai/llm/design-patterns","permalink":"/ai/llm/design-patterns","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/design-patterns.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1750702471000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Code Generators","permalink":"/ai/llm/code-generators"},"next":{"title":"Ethics","permalink":"/ai/llm/ethics"}}');var o=n(474848),i=n(28453);const s={},r="Design patterns",l={},d=[{value:"In-context learning",id:"in-context-learning",level:2},{value:"LoRA (Low Rank Adaptation)",id:"lora-low-rank-adaptation",level:2},{value:"Context Window / Tokens",id:"context-window--tokens",level:2},{value:"Links",id:"links",level:2}];function c(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"design-patterns",children:"Design patterns"})}),"\n",(0,o.jsx)(t.h2,{id:"in-context-learning",children:"In-context learning"}),"\n",(0,o.jsx)(t.p,{children:'The core idea of in-context learning is to use LLMs off the shelf (i.e., without any fine-tuning), then control their behavior through clever prompting and conditioning on private "contextual" data.'}),"\n",(0,o.jsxs)(t.p,{children:["For example, say you\u2019re building a chatbot to answer questions about a set of legal documents. Taking a naive approach, you could paste all the documents into a ChatGPT or GPT-4 prompt, then ask a question about them at the end. This may work for very small datasets, but it doesn\u2019t scale. The biggest GPT-4 model can only process ~50 pages of input text, and performance (measured by inference time and accuracy) degrades badly as you approach this limit, called a ",(0,o.jsx)(t.strong,{children:"context window."})]}),"\n",(0,o.jsx)(t.p,{children:"In-context learning solves this problem with a clever trick: instead of sending all the documents with each LLM prompt, it sends only a handful of the most relevant documents. And the most relevant documents are determined with the help of . . . you guessed it . . . LLMs."}),"\n",(0,o.jsx)(t.p,{children:"At a very high level, the workflow can be divided into three stages:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Data preprocessing / embedding:"})," This stage involves storing private data (legal documents, in our example) to be retrieved later. Typically, the documents are broken into chunks, passed through an embedding model, then stored in a specialized database called a vector database."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Prompt construction / retrieval:"})," When a user submits a query (a legal question, in this case), the application constructs a series of prompts to submit to the language model. A compiled prompt typically combines a prompt template hard-coded by the developer; examples of valid outputs called few-shot examples; any necessary information retrieved from external APIs; and a set of relevant documents retrieved from the vector database."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Prompt execution / inference:"})," Once the prompts have been compiled, they are submitted to a pre-trained LLM for inference-including both proprietary model APIs and open-source or self-trained models. Some developers also add operational systems like logging, caching, and validation at this stage."]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"This looks like a lot of work, but it\u2019s usually easier than the alternative: training or fine-tuning the LLM itself. You don\u2019t need a specialized team of ML engineers to do in-context learning. You also don\u2019t need to host your own infrastructure or buy an expensive dedicated instance from OpenAI. This pattern effectively reduces an AI problem to a data engineering problem that most startups and big companies already know how to solve. It also tends to outperform fine-tuning for relatively small datasets-since a specific piece of information needs to occur at least ~10 times in the training set before an LLM will remember it through fine-tuning-and can incorporate new data in near real time."}),"\n",(0,o.jsxs)(t.p,{children:["One of the biggest questions around in-context learning is: What happens if we just change the underlying model to increase the context window? This is indeed possible, and it is an active area of research (e.g., see the ",(0,o.jsx)(t.a,{href:"https://arxiv.org/abs/2302.10866",children:"Hyena paper"})," or this ",(0,o.jsx)(t.a,{href:"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c",children:"recent post"}),"). But this comes with a number of tradeoffs-primarily that cost and time of inference scale quadratically with the length of the prompt. Today, even linear scaling (the best theoretical outcome) would be cost-prohibitive for many applications. A single GPT-4 query over 10,000 pages would cost hundreds of dollars at current API rates. So, we don\u2019t expect wholesale changes to the stack based on expanded context windows"]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.a,{href:"https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/",children:"Emerging Architectures for LLM Applications | Andreessen Horowitz"})}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.a,{href:"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c",children:"The Secret Sauce behind 100K context window in LLMs: all tricks in one place | by Galina Alperovich | GoPenAI"})}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.a,{href:"https://aws.amazon.com/blogs/big-data/exploring-real-time-streaming-for-generative-ai-applications/",children:"Exploring real-time streaming for generative AI Applications | AWS Big Data Blog"})}),"\n",(0,o.jsx)(t.h2,{id:"lora-low-rank-adaptation",children:"LoRA (Low Rank Adaptation)"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation",children:"Mastering Low-Rank Adaptation (LoRA): Enhancing Large Language Models for Efficient Adaptation | DataCamp"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/docs/diffusers/main/en/training/lora",children:"LoRA"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://github.com/microsoft/LoRA",children:'GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"'})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://www.youtube.com/watch?v=Bq9zqTJDsjg",children:"LoRA - Explained! - YouTube"})}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"context-window--tokens",children:"Context Window / Tokens"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.a,{href:"https://github.com/NVIDIA/RULER",children:"GitHub - NVIDIA/RULER: This repo contains the source code for RULER: What\u2019s the Real Context Size of Your Long-Context Language Models?"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"RULER - Real Context Size of Your Long-Context Language Models"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://www.reddit.com/r/LocalLLaMA/comments/1eplndh/what_is_the_current_largest_context_window_for_an/",children:"What is the current largest context window for an open LLM? : r/LocalLLaMA"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://codingscape.com/blog/llms-with-largest-context-windows",children:"LLMs with largest context windows"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://www.thecloudgirl.dev/blog/rag-vs-large-context-window",children:"RAG vs Large Context Window LLMs: When to use which one? \u2014 The Cloud Girl"})}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"links",children:"Links"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsxs)(t.a,{href:"https://www.youtube.com/watch?v=aEA6X_IElpc",children:["AWS re",":Invent"," 2023 - Generative AI: Architectures and applications in depth (BOA308) - YouTube"]})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsxs)(t.a,{href:"https://www.youtube.com/watch?v=oBhP44wowoY",children:["AWS re",":Invent"," 2023 - SaaS meets AI/ML & generative AI: Multi-tenant patterns & strategies (SAS306) - YouTube"]})}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>r});var a=n(296540);const o={},i=a.createContext(o);function s(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);