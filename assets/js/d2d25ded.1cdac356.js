"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[63943],{469673:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"technologies/flink/processing-model","title":"Processing Model","description":"APIs","source":"@site/docs/technologies/flink/processing-model.md","sourceDirName":"technologies/flink","slug":"/technologies/flink/processing-model","permalink":"/technologies/flink/processing-model","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/flink/processing-model.md","tags":[],"version":"current","lastUpdatedBy":"Deeapak Sood","lastUpdatedAt":1765827773000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Others","permalink":"/technologies/flink/others"},"next":{"title":"PyFlink","permalink":"/technologies/flink/pyflink"}}');var a=s(474848),i=s(28453);const o={},r="Processing Model",l={},c=[{value:"APIs",id:"apis",level:2},{value:"Stream Processing Model",id:"stream-processing-model",level:2},{value:"Batch Processing Model",id:"batch-processing-model",level:2},{value:"Advantages and Limitations",id:"advantages-and-limitations",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"processing-model",children:"Processing Model"})}),"\n",(0,a.jsx)(t.h2,{id:"apis",children:"APIs"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"SQL API (Flink SQL - High-level language)"}),"\n",(0,a.jsx)(t.li,{children:"Table API (Java SQL APIs - Declarative DSL)"}),"\n",(0,a.jsx)(t.li,{children:"DataStream API (Low Level API - Core APIs)"}),"\n",(0,a.jsx)(t.li,{children:"Process Functions (Low-level building blocks (streams, state, etc.))"}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/overview/",children:"Overview | Apache Flink"})}),"\n",(0,a.jsx)(t.h2,{id:"stream-processing-model",children:"Stream Processing Model"}),"\n",(0,a.jsx)(t.p,{children:"Flink's stream processing model handles incoming data on an item-by-item basis as a true stream. Flink provides its DataStream API to work with unbounded streams of data. The basic components that Flink works with are:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Streams"})," are immutable, unbounded datasets that flow through the system"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Operators"})," are functions that operate on data streams to produce other streams"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Sources"})," are the entry point for streams entering the system"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Sinks"})," are the place where streams flow out of the Flink system. They might represent a database or a connector to another system"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Stream processing tasks take snapshots at set points during their computation to use for recovery in case of problems. For storing state, Flink can work with a number of state backends depending with varying levels of complexity and persistence."}),"\n",(0,a.jsx)(t.p,{children:'Additionally, Flink\'s stream processing is able to understand the concept of "event time", meaning the time that the event actually occurred, and can handle sessions as well. This means that it can guarantee ordering and grouping in some interesting ways.'}),"\n",(0,a.jsx)(t.h2,{id:"batch-processing-model",children:"Batch Processing Model"}),"\n",(0,a.jsx)(t.p,{children:"Flink's batch processing model in many ways is just an extension of the stream processing model. Instead of reading from a continuous stream, it reads a bounded dataset off of persistent storage as a stream. Flink uses the exact same runtime for both of these processing models."}),"\n",(0,a.jsx)(t.p,{children:"Flink offers some optimizations for batch workloads. For instance, since batch operations are backed by persistent storage, Flink removes snapshotting from batch loads. Data is still recoverable, but normal processing completes faster."}),"\n",(0,a.jsx)(t.p,{children:"Another optimization involves breaking up batch tasks so that stages and components are only involved when needed. This helps Flink play well with other users of the cluster. Preemptive analysis of the tasks gives Flink the ability to also optimize by seeing the entire set of operations, the size of the data set, and the requirements of steps coming down the line."}),"\n",(0,a.jsx)(t.h2,{id:"advantages-and-limitations",children:"Advantages and Limitations"}),"\n",(0,a.jsx)(t.p,{children:"Flink is currently a unique option in the processing framework world. While Spark performs batch and stream processing, its streaming is not appropriate for many use cases because of its micro-batch architecture. Flink's stream-first approach offers low latency, high throughput, and real entry-by-entry processing."}),"\n",(0,a.jsx)(t.p,{children:"Flink manages many things by itself. Somewhat unconventionally, it manages its own memory instead of relying on the native Java garbage collection mechanisms for performance reasons. Unlike Spark, Flink does not require manual optimization and adjustment when the characteristics of the data it processes change. It handles data partitioning and caching automatically as well."}),"\n",(0,a.jsx)(t.p,{children:'Flink analyzes its work and optimizes tasks in a number of ways. Part of this analysis is similar to what SQL query planners do within relationship databases, mapping out the most effective way to implement a given task. It is able to parallelize stages that can be completed in parallel, while bringing data together for blocking tasks. For iterative tasks, Flink attempts to do computation on the nodes where the data is stored for performance reasons. It can also do "delta iteration", or iteration on only the portions of data that have changes.'}),"\n",(0,a.jsx)(t.p,{children:"In terms of user tooling, Flink offers a web-based scheduling view to easily manage tasks and view the system. Users can also display the optimization plan for submitted tasks to see how it will actually be implemented on the cluster. For analysis tasks, Flink offers SQL-style querying, graph processing and machine learning libraries, and in-memory computation."}),"\n",(0,a.jsx)(t.p,{children:"Flink operates well with other components. It is written to be a good neighbor if used within a Hadoop stack, taking up only the necessary resources at any given time. It integrates with YARN, HDFS, and Kafka easily. Flink can run tasks written for other processing frameworks like Hadoop and Storm with compatibility packages."}),"\n",(0,a.jsx)(t.p,{children:"One of the largest drawbacks of Flink at the moment is that it is still a very young project. Large scale deployments in the wild are still not as common as other processing frameworks and there hasn't been much research into Flink's scaling limitations. With the rapid development cycle and features like the compatibility packages, there may begin to be more Flink deployments as organizations get the chance to experiment with it."}),"\n",(0,a.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(t.p,{children:"Flink offers both low latency stream processing with support for traditional batch tasks. Flink is probably best suited for organizations that have heavy stream processing requirements and some batch-oriented tasks. Its compatibility with native Storm and Hadoop programs, and its ability to run on a YARN-managed cluster can make it easy to evaluate. Its rapid development makes it worth keeping an eye on."})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},28453:(e,t,s)=>{s.d(t,{R:()=>o,x:()=>r});var n=s(296540);const a={},i=n.createContext(a);function o(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);