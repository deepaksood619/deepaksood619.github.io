"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[34704],{129125:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"technologies/kafka/others","title":"Others","description":"Storage formats: Serialization and deserialization of events","source":"@site/docs/technologies/kafka/others.md","sourceDirName":"technologies/kafka","slug":"/technologies/kafka/others","permalink":"/technologies/kafka/others","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/kafka/others.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1769339984000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Monitoring","permalink":"/technologies/kafka/monitoring"},"next":{"title":"Releases / Upgrades / Change logs","permalink":"/technologies/kafka/releases-upgrades-changelogs"}}');var s=a(474848),o=a(28453);const i={},r="Others",c={},l=[{value:"Storage formats: Serialization and deserialization of events",id:"storage-formats-serialization-and-deserialization-of-events",level:2},{value:"Data contracts, schema on read, and schema on write",id:"data-contracts-schema-on-read-and-schema-on-write",level:2},{value:"Other Stream Processing Brokers",id:"other-stream-processing-brokers",level:2},{value:"Kafka Connect Dead Letter Queues",id:"kafka-connect-dead-letter-queues",level:2},{value:"Kafka Edge Computing",id:"kafka-edge-computing",level:2},{value:"Kafka Gotchas",id:"kafka-gotchas",level:2},{value:"Compression",id:"compression",level:2},{value:"Secor",id:"secor",level:2},{value:"Conduktor",id:"conduktor",level:2},{value:"Tools",id:"tools",level:2},{value:"Others",id:"others-1",level:2}];function h(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"others",children:"Others"})}),"\n",(0,s.jsx)(t.h2,{id:"storage-formats-serialization-and-deserialization-of-events",children:"Storage formats: Serialization and deserialization of events"}),"\n",(0,s.jsxs)(t.p,{children:['Events are serialized when they are written to a topic and deserialized when they are read. These operations turn binary data into the forms you and I understand, and vice versa. Importantly, these operations are done solely by the Kafka clients, i.e., producing and consuming applications such as ksqlDB, Kafka Streams, or a microservice using the Go client for Kafka, for example. As such, there is no single "storage format" in Kafka. Common serialization formats used by Kafka clients include Apache Avro\u2122 (with the ',(0,s.jsx)(t.a,{href:"https://docs.confluent.io/current/schema-registry/index.html",children:"Confluent Schema Registry"}),"), Protobuf, and JSON."]}),"\n",(0,s.jsx)(t.p,{children:'Kafka brokers, on the other hand, are agnostic to the serialization format or "type" of a stored event. All they see is a pair of raw bytes for event key and event value coming in when being written, and going out when being read. Brokers thus have no idea what\'s in the data they serve - it\'s a black box to them. Being this "dumb" is actually pretty smart, because this design decision allows brokers to scale much better than traditional messaging systems.'}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://www.confluent.io/blog/avro-kafka-data",children:"https://www.confluent.io/blog/avro-kafka-data"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage",children:"KIP-405: Kafka Tiered Storage - Apache Kafka - Apache Software Foundation"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://2minutestreaming.beehiiv.com/p/apache-kafka-kip-405-tiered-storage",children:"KIP-405: Kafka Tiered Storage"})}),"\n",(0,s.jsx)(t.h2,{id:"data-contracts-schema-on-read-and-schema-on-write",children:"Data contracts, schema on read, and schema on write"}),"\n",(0,s.jsx)(t.p,{children:"As already mentioned, it is the responsibility of the consuming client (whether it's ksqlDB, Kafka Connect, a custom Kafka consumer, etc.) to deserialize the raw bytes of a Kafka message into the original event by applying some kind of schema, be it a formalized schema in Avro or Protobuf, or an informal JSON format scribbled on the back of a napkin in the company canteen. This means it is, generally speaking, a schema-on-read setup."}),"\n",(0,s.jsxs)(t.p,{children:["But how does a consuming client know how to deserialize stored events, given that most likely a different client produced them? The answer is that producers and consumers must agree on a data contract in some way. Gwen Shapira covered the important subject of ",(0,s.jsx)(t.a,{href:"https://www.confluent.io/blog/schemas-contracts-compatibility",children:"data contracts and schema management"})," in an earlier blog post, so I'll skip over the details here. But in summary, the easiest option is to use Avro and ",(0,s.jsx)(t.a,{href:"https://www.confluent.io/confluent-schema-registry/",children:"Confluent Schema Registry"}),". With a schema registry and a formalized schema (including but not limited to Avro), we are moving from schema on read into ",(0,s.jsx)(t.a,{href:"https://www.oreilly.com/ideas/data-governance-and-the-death-of-schema-on-read",children:"schema-on-write territory"}),', which is a boon for pretty much everyone who is working with data, not just the few poor souls of us tasked to "go and do data governance."']}),"\n",(0,s.jsxs)(t.p,{children:["And with Confluent Platform 5.4 or newer, you have the additional option to ",(0,s.jsx)(t.a,{href:"https://www.confluent.io/blog/data-governance-with-schema-validation",children:"centrally enforce broker-side Schema Validation"})," so that no misbehaving client can violate the data contract: incoming events are validated server side before they are stored in Kafka topics. This feature is a huge benefit for any Kafka user and especially for larger, regulated organizations."]}),"\n",(0,s.jsx)(t.h2,{id:"other-stream-processing-brokers",children:"Other Stream Processing Brokers"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Apache Pulsar"}),"\n",(0,s.jsx)(t.li,{children:"AWS Kinesis"}),"\n",(0,s.jsx)(t.li,{children:"AWS SQS"}),"\n",(0,s.jsx)(t.li,{children:"Google Cloud Pub/Sub"}),"\n",(0,s.jsx)(t.li,{children:"Azure Event Hubs"}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Redpanda"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Redpanda is the real-time engine for modern apps. Kafka API Compatible; 10x faster"}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://redpanda.com",children:"https://redpanda.com"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/redpanda-data/redpanda",children:"GitHub - redpanda-data/redpanda: Redpanda is a streaming data platform for developers. Kafka API compatible. 10x faster. No ZooKeeper. No JVM!"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/technologies/confluent/warpstream",children:(0,s.jsx)(t.strong,{children:"warpstream"})})}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://kafscale.io/",children:"KafScale - Stateless Kafka on S3 | KafScale"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Apache 2.0 licensed. No vendor lock-in. Self-hosted."}),"\n",(0,s.jsx)(t.li,{children:"One endpoint. Infinite scale."}),"\n",(0,s.jsx)(t.li,{children:"Kafka-compatible streaming platform. Scale streaming and analytics cloud-native on S3. Automated."}),"\n",(0,s.jsxs)(t.li,{children:["KafScale is a Kafka-protocol compatible streaming platform built around a simple premise: ",(0,s.jsx)(t.strong,{children:"durable logs belong in object storage, not in stateful brokers"}),"."]}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/KafScale/platform",children:"GitHub - KafScale/platform: KafScale delivers Kafka-compatible data streaming on S3. Scale infinitely, stateless brokers and processors."})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://www.novatechflow.com/p/kafscale.html",children:"KafScale Architecture Decisions"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://kafscale.io/comparison/",children:"Comparison | KafScale"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/automq/automq",children:"GitHub - AutoMQ/automq: AutoMQ is a cloud-native alternative to Kafka by decoupling durability to cloud storage services like S3. 10x Cost-Effective. No Cross-AZ Traffic Cost. Autoscale in seconds. Single-digit ms latency. Multi-AZ Availability."})}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://youtu.be/kz7R1mGrN9Q",children:"Co-Designing Raft + Thread-per-Core Execution Model for the Kafka-API"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://bravenewgeek.com/benchmarking-message-queue-latency",children:"https://bravenewgeek.com/benchmarking-message-queue-latency"})}),"\n",(0,s.jsx)(t.h2,{id:"kafka-connect-dead-letter-queues",children:"Kafka Connect Dead Letter Queues"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues",children:"https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues"})}),"\n",(0,s.jsx)(t.h2,{id:"kafka-edge-computing",children:"Kafka Edge Computing"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://www.kai-waehner.de/blog/2020/01/01/apache-kafka-edge-computing-industrial-iot-retailing-logistics",children:"https://www.kai-waehner.de/blog/2020/01/01/apache-kafka-edge-computing-industrial-iot-retailing-logistics"})}),"\n",(0,s.jsx)(t.h2,{id:"kafka-gotchas",children:"Kafka Gotchas"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Too many tunable knobs"}),"\n",(0,s.jsx)(t.li,{children:"Unsafe defaults"}),"\n",(0,s.jsx)(t.li,{children:"enable.auto.commit"}),"\n",(0,s.jsx)(t.li,{children:"max.in.flight.requests.per.connection"}),"\n",(0,s.jsx)(t.li,{children:"Appalling tooling"}),"\n",(0,s.jsx)(t.li,{children:"Complicated bootstrapping process"}),"\n",(0,s.jsx)(t.li,{children:"Shaky client libraries"}),"\n",(0,s.jsx)(t.li,{children:"Lack of true multitenancy"}),"\n",(0,s.jsx)(t.li,{children:"Lack of geo-awareness"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://itnext.io/kafka-gotchas-24b51cc8d44e",children:"https://itnext.io/kafka-gotchas-24b51cc8d44e"})}),"\n",(0,s.jsx)(t.h2,{id:"compression",children:"Compression"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://cwiki.apache.org/confluence/display/KAFKA/Compression#app-switcher",children:"https://cwiki.apache.org/confluence/display/KAFKA/Compression#app-switcher"})}),"\n",(0,s.jsx)(t.p,{children:"When the broker receives a compressed batch of messages from a producer:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"it always decompresses the data in order to validate it"}),"\n",(0,s.jsxs)(t.li,{children:["it considers the compression codec of the destination topic","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"if the compression codec of the destination topic is producer, or if the codecs of the batch and destination topic are the same, the broker takes the compressed batch from the client and writes it directly to the topic's log file without recompressing the data."}),"\n",(0,s.jsx)(t.li,{children:"Otherwise, the broker needs to re-compress the data to match the codec of the destination topic."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Decompression and re-compression can also happen if producers are running a version prior to 0.10 because offsets need to be overwritten, or if any other message format conversion is required."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://stackoverflow.com/questions/59902385/if-i-set-compression-type-at-topic-level-and-producer-level-which-takes-prece",children:"https://stackoverflow.com/questions/59902385/if-i-set-compression-type-at-topic-level-and-producer-level-which-takes-prece"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://stackoverflow.com/questions/48670584/kafka-set-compression-type-at-producer-vs-topic",children:"https://stackoverflow.com/questions/48670584/kafka-set-compression-type-at-producer-vs-topic"})}),"\n",(0,s.jsx)(t.h2,{id:"secor",children:"Secor"}),"\n",(0,s.jsxs)(t.p,{children:["Secor is a service persisting ",(0,s.jsx)(t.a,{href:"http://kafka.apache.org/",children:"Kafka"})," logs to ",(0,s.jsx)(t.a,{href:"http://aws.amazon.com/s3/",children:"Amazon S3"}),", ",(0,s.jsx)(t.a,{href:"https://cloud.google.com/storage/",children:"Google Cloud Storage"}),", ",(0,s.jsx)(t.a,{href:"https://azure.microsoft.com/en-us/services/storage/blobs/",children:"Microsoft Azure Blob Storage"})," and ",(0,s.jsx)(t.a,{href:"http://swift.openstack.org/",children:"Openstack Swift"}),"."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://github.com/pinterest/secor",children:"https://github.com/pinterest/secor"})}),"\n",(0,s.jsx)(t.h2,{id:"conduktor",children:"Conduktor"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://github.com/conduktor/conduktor-platform",children:"GitHub - conduktor/conduktor-platform: Streamline Apache Kafka with Conduktor Platform. \ud83d\ude80"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"We take the complexity out of Kafka. Console gives you visibility into your Kafka ecosystem and concentrates all of the Kafka APIs into a single interface. Troubleshoot and debug Kafka, drill-down into topic data, and continuously monitor your streaming applications."}),"\n",(0,s.jsx)(t.li,{children:"Conduktor supports all Kafka providers (Apache Kafka, MSK, Confluent, Aiven, Redpanda, Strimzi etc.)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://conduktor.io/",children:"Conduktor - The Streaming Data Hub"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"The Governance Platform for Kafka"}),"\n",(0,s.jsx)(t.li,{children:"The intelligent backbone for all your data streaming. Operate, govern, secure. Built for teams and AI."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://www.youtube.com/watch?v=11JcsITzZ_g",children:"Conduktor - Getting Started - YouTube"})}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"tools",children:"Tools"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://github.com/osodevops/k2i",children:"GitHub - osodevops/k2i: K2I - Kafka to Iceberg streaming ingestion engine. A Rust CLI tool inspired by Moonlink architecture that consumes from Kafka, buffers with Apache Arrow for sub-second query freshness, and writes to Apache Iceberg tables."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://k2i.dev/",children:"K2I - Real-time Streaming Ingestion for Apache Iceberg"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"others-1",children:"Others"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://kafka-tutorials.confluent.io",children:"https://kafka-tutorials.confluent.io"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://www.confluent.io/blog/kafka-streams-tables-part-4-elasticity-fault-tolerance-advanced-concepts",children:"https://www.confluent.io/blog/kafka-streams-tables-part-4-elasticity-fault-tolerance-advanced-concepts"})})]})}function d(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},28453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>r});var n=a(296540);const s={},o=n.createContext(s);function i(e){const t=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(o.Provider,{value:t},e.children)}}}]);