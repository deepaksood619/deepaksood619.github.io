"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[27320],{306603:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>f,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"technologies/confluent/confluent-kafka","title":"confluent-kafka","description":"Streaming Data Governance \\\\| Understanding the Confluent Cloud Stream Governance Platform - YouTube","source":"@site/docs/technologies/confluent/confluent-kafka.md","sourceDirName":"technologies/confluent","slug":"/technologies/confluent/confluent-kafka","permalink":"/technologies/confluent/confluent-kafka","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/confluent/confluent-kafka.md","tags":[],"version":"current","lastUpdatedBy":"Deeapak Sood","lastUpdatedAt":1764577313000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Confluent Intelligence","permalink":"/technologies/confluent/confluent-intelligence"},"next":{"title":"Confluent Pitch","permalink":"/technologies/confluent/confluent-pitch"}}');var r=t(474848),i=t(28453);const s={},a="confluent-kafka",l={},c=[{value:"Difference between open source Kafka vs Confluent",id:"difference-between-open-source-kafka-vs-confluent",level:2},{value:"Confluent kafka-python",id:"confluent-kafka-python",level:2},{value:"Consumer",id:"consumer",level:2},{value:"Producer",id:"producer",level:2},{value:"Resources",id:"resources",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"confluent-kafka",children:"confluent-kafka"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/playlist?list=PLa7VYi0yPIH1sKNegs6Y8m92PRoYrcQzm",children:"Streaming Data Governance | Understanding the Confluent Cloud Stream Governance Platform - YouTube"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Streaming at Scale"}),"\n",(0,r.jsxs)(n.li,{children:["Stream Governance","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Availability"}),"\n",(0,r.jsx)(n.li,{children:"Usability"}),"\n",(0,r.jsx)(n.li,{children:"Integrity"}),"\n",(0,r.jsx)(n.li,{children:"Security"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Stream Quality"})}),"\n",(0,r.jsx)(n.li,{children:"Schema Registry"}),"\n",(0,r.jsx)(n.li,{children:"Stream Discoverability"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Stream Catalog"})}),"\n",(0,r.jsx)(n.li,{children:"Visualizing Streams"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Stream Lineage"})}),"\n",(0,r.jsx)(n.li,{children:"Security"}),"\n",(0,r.jsx)(n.li,{children:"Data as a product"}),"\n",(0,r.jsx)(n.li,{children:"Flink + Tableflow"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"difference-between-open-source-kafka-vs-confluent",children:"Difference between open source Kafka vs Confluent"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"hot vs cold storage in partitions"}),"\n",(0,r.jsx)(n.li,{children:"fetch from replica instead of leader"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"confluent-kafka-python",children:"Confluent kafka-python"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'pip install confluent-kafka\npip install "confluent-kafka [avro]"\n'})}),"\n",(0,r.jsx)(n.h2,{id:"consumer",children:"Consumer"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from confluent_kafka import Consumer, KafkaError\n\nconsumer_config = {\n    'bootstrap.servers': 'my-cluster-kafka-brokers.kafka:9092',\n    'partition.assignment.strategy': 'roundrobin',\n    'group.id': 'test_bank_data_consumer',\n    'auto.offset.reset': 'earliest',  # earliest/latest\n    'enable.auto.commit': 'false',\n    # for limiting the amount of messages pre-fetched by librdkafka\n    'queued.max.messages.kbytes': '32000',\n    'fetch.message.max.bytes': '15728640',\n}\n\nc = Consumer(consumer_config)\n\n# callbacks\ndef print_on_assign(consumer, partitions):\n    logging.info(f'Assignment: {partitions}')\n\n    for partition in partitions:\n        logging.info(f'watermark: {c.get_watermark_offsets(partition=partition)}')\n\n    logging.info(f'committed offsets for all partitions: {c.committed(partitions=partitions)}')\n\n    logging.info(f'position: {c.position(partitions=partitions)}')\n\ndef print_on_revoke(consumer, partitions):\n    logging.info(f'Revoke Assignment: {partitions}')\n\nc.subscribe(['bank_data'], on_assign=print_on_assign, on_revoke=print_on_revoke)\n\ntimeout_seconds = 1\n\nwhile True:\n    msg = c.poll(1.0)\n\n    # initial error handling\n    if msg is None:\n        continue\n\n    if msg.error():\n        if msg.error().code() == KafkaError._PARTITION_EOF:\n            continue\n        else:\n            logging.error(f'druid consumer error: {msg.error()}')\n            break\n\n    logging.debug(f'{msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n\n    try:\n        # get value from message and convert bytes\n        final_data = msg.value()\n        final_data = json.loads(final_data.decode('utf-8'))\n        c.commit()\n\n    except Exception as e:\n        try:\n            logging.error(f'data/msg: {msg.value()}')\n        except Exception:\n            logging.exception(f'cannot print data')\n        logging.exception(\n            f'global exception occurred, Will not attempt for another {timeout_seconds} seconds.')\n    else:\n        continue\n\n    # exponential back-off if exception occurred\n    time.sleep(timeout_seconds)\n    timeout_seconds *= 2\n"})}),"\n",(0,r.jsx)(n.h2,{id:"producer",children:"Producer"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from confluent_kafka import Producer\n\np = Producer({\n    'bootstrap.servers': 'my-cluster-kafka-brokers.kafka:9092',\n    'queue.buffering.max.messages': '1000000',\n    'queue.buffering.max.kbytes': '1048576',\n    'message.max.bytes': '15728640',\n    'delivery.timeout.ms': '10000',\n    'request.timeout.ms': '5000'\n})\n\ndef delivery_report(err, msg):\n    \"\"\" Called once for each message produced to indicate delivery result.\n        Triggered by poll() or flush(). \"\"\"\n    if err is not None:\n        # raise error and handle using exception\n        logging.exception(f'kafka deliver_report error: {err}')\n    else:\n        logging.debug(f'Message delivered topic: {msg.topic()} partition: {msg.partition()} offset: {msg.offset()}')\n\np.produce('bank_data', json.dumps(payload), callback=delivery_report)\np.flush()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/@ConfluentDeveloper",children:"Confluent Developer - YouTube"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/@Confluent",children:"Confluent - YouTube"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/confluentinc/confluent-kafka-python",children:"https://github.com/confluentinc/confluent-kafka-python"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.confluent.io/current/clients/confluent-kafka-python",children:"https://docs.confluent.io/current/clients/confluent-kafka-python"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION",children:"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://towardsdatascience.com/3-libraries-you-should-know-to-master-apache-kafka-in-python-c95fdf8700f2",children:"https://towardsdatascience.com/3-libraries-you-should-know-to-master-apache-kafka-in-python-c95fdf8700f2"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.confluent.io/platform/current/security/authentication/ldap/client-authentication-ldap.html",children:"Configure Kafka clients for LDAP Authentication in Confluent Platform | Confluent Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.confluent.io/cloud/current/clusters/cluster-types.html",children:"Confluent Cloud Cluster Types | Confluent Documentation"})}),"\n"]})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(296540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);