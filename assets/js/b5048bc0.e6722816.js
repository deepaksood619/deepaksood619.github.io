"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[74864],{589474:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ai/llm/limitations-problems","title":"Model Limitations / Problems","description":"Hallucinations","source":"@site/docs/ai/llm/limitations-problems.md","sourceDirName":"ai/llm","slug":"/ai/llm/limitations-problems","permalink":"/ai/llm/limitations-problems","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/limitations-problems.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1736275047000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Libraries","permalink":"/ai/llm/libraries"},"next":{"title":"Lovable Prompt Portfolio Website","permalink":"/ai/llm/lovable-prompt-portfolio-website"}}');var s=n(474848),a=n(28453);const o={},r="Model Limitations / Problems",l={},c=[{value:"Hallucinations",id:"hallucinations",level:2},{value:"Reducing hallucinations",id:"reducing-hallucinations",level:3},{value:"Problems with LLM",id:"problems-with-llm",level:2},{value:"Reversal Curse",id:"reversal-curse",level:2},{value:"Links",id:"links",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"model-limitations--problems",children:"Model Limitations / Problems"})}),"\n",(0,s.jsx)(i.h2,{id:"hallucinations",children:"Hallucinations"}),"\n",(0,s.jsx)(i.p,{children:"Makes statements that sound plausible but are not true"}),"\n",(0,s.jsx)(i.h3,{id:"reducing-hallucinations",children:"Reducing hallucinations"}),"\n",(0,s.jsx)(i.p,{children:"First find relevant information, then answer the question based on the relevant information"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)",children:"Hallucination (artificial intelligence) - Wikipedia"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Researchers have recognized this issue, and by 2023, analysts estimated that chatbots hallucinate as much as 27% of the time, with factual errors present in 46% of their responses."}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"problems-with-llm",children:"Problems with LLM"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Not Updated to the latest information:"})," Generative Al uses large language models to generate texts and these models have information only to date they are trained. If data is requested beyond that date, accuracy/output may be compromised."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Hallucinations:"})," Hallucinations refer to the output which is factually incorrect or nonsensical. However, the output looks coherent and grammatically correct. This information could be misleading and could have a major impact on business decision-making."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Domain-specific most accurate information:"})," LLMs' output lacks accurate information many times when specificity is more important than generalized output. For instance, organizational HR policies tailored to specific employees may not be accurately addressed by LLM-based Al due to its tendency towards generic responses."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Source Citations:"})," In Generative Al responses, we don't know what source it is referring to generate a particular response. So citations become difficult and sometimes it is not ethically correct to not cite the source of information and give due credit."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Updates take Long training time:"})," information is changing very frequently and if you think to re-train those models with new information it requires huge resources and long training time which is a computationally intensive task."]}),"\n",(0,s.jsx)(i.li,{children:"Presenting false information when it does not have the answer."}),"\n",(0,s.jsx)(i.li,{children:"Non-deterministic - same request can give different response/solution/output"}),"\n",(0,s.jsx)(i.li,{children:"Confidence is low because of hallucination"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"reversal-curse",children:"Reversal Curse"}),"\n",(0,s.jsx)(i.p,{children:'If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Valentina Tereshkova was the first woman to travel to space", it will not automatically be able to answer the question, "Who was the first woman to travel to space?". Moreover, the likelihood of the correct answer ("Valentina Tershkova") will not be higher than for a random name.'}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://www.strangeloopcanon.com/p/what-can-llms-never-do",children:"What can LLMs never do? - by Rohit Krishnan"})}),"\n",(0,s.jsx)(i.h2,{id:"links",children:"Links"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.a,{href:"https://ai.stanford.edu/~kzliu/blog/unlearning",children:"Machine Unlearning in 2024 - Ken Ziyu Liu - Stanford Computer Science"})}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,i,n)=>{n.d(i,{R:()=>o,x:()=>r});var t=n(296540);const s={},a=t.createContext(s);function o(e){const i=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);