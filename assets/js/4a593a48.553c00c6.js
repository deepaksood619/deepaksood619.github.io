"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[90428],{612103:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"ai/move-37/reinforcement-learning","title":"Reinforcement Learning","description":"Reinforcement learning","source":"@site/docs/ai/move-37/reinforcement-learning.md","sourceDirName":"ai/move-37","slug":"/ai/move-37/reinforcement-learning","permalink":"/ai/move-37/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/reinforcement-learning.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1701793554000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Quizzes","permalink":"/ai/move-37/quizzes"},"next":{"title":"Syllabus","permalink":"/ai/move-37/syllabus"}}');var r=t(474848),a=t(28453);const o={},s="Reinforcement Learning",l={},c=[{value:"Reinforcement learning",id:"reinforcement-learning-1",level:2},{value:"Categorizing Reinforcement Learning Agents",id:"categorizing-reinforcement-learning-agents",level:2},{value:"Two Types of Environments",id:"two-types-of-environments",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"reinforcement-learning",children:"Reinforcement Learning"})}),"\n",(0,r.jsx)(n.h2,{id:"reinforcement-learning-1",children:"Reinforcement learning"}),"\n",(0,r.jsxs)(n.p,{children:["RL is an area of ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Machine_learning",children:"machine learning"})," concerned with how ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Software_agent",children:"software agents"})," ought to take ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Action_selection",children:(0,r.jsx)(n.em,{children:"actions"})})," in an ",(0,r.jsx)(n.em,{children:"environment"}),"so as to maximize some notion of cumulative ",(0,r.jsx)(n.em,{children:"reward"})]}),"\n",(0,r.jsxs)(n.p,{children:["In machine learning, the environment is typically formulated as a ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Markov_Decision_Process",children:"Markov Decision Process"}),"(MDP), as many reinforcement learning algorithms for this context utilize ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Dynamic_programming",children:"dynamic programming"})," techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible."]}),"\n",(0,r.jsxs)(n.p,{children:["When the environment is fully observed, we call the reinforcement learning problem a",(0,r.jsx)(n.strong,{children:"Markov decision process"}),". When the state does not depend on the previous actions, we call the problem a",(0,r.jsx)(n.strong,{children:"contextual bandit problem"}),". When there is no state, just a set of available actions with initially unknown rewards, this problem is the classic",(0,r.jsx)(n.strong,{children:"multi-armed bandit problem."})]}),"\n",(0,r.jsx)(n.h2,{id:"categorizing-reinforcement-learning-agents",children:"Categorizing Reinforcement Learning Agents"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value Based Agent,"})," the agent will evaluate all the states in the state space, and the policy will be kind of implicit, i.e. the value function tells the agent how good is each action in a particular state and the agent will choose the best one."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Policy Based Agent,"})," instead of representing the value function inside the agent, we explicitly represent the policy. The agent searches for the optimal action-value function which in turn will enable it to act optimally."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Actor-Critic Agent,"})," this agent is a value-based and policy-based agent. It's an agent that stores both of the policy, and how much reward it is getting from each state."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model-Based Agent,"})," the agent tries to build a model of how the environment works, and then plan to get the best possible behavior."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model-Free Agent,"})," here the agent doesn't try to understand the environment, i.e. it doesn't try to build the dynamics. Instead we go directly to the policy and/or value function. We just see experience and try to figure out a policy of how to behave optimally to get the most possible rewards."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"two-types-of-environments",children:"Two Types of Environments"}),"\n",(0,r.jsx)(n.p,{children:"Deterministic environment: deterministic environment means that both state transition model and reward model are deterministic functions. If the agent while in a given state repeats a given action, it will always go the same next state and receive the same reward value"}),"\n",(0,r.jsx)(n.p,{children:"Stochastic environment: In a stochastic environment there is uncertainity about the actions effect. When the agent repeats doing the same action in a given state, the new state and received reward may not be the same each time. For example, a robot which tries to move forward but because of the imperfection in the robot operation or other factors in the environment (e.g. slippery floor), sometimes the actionforwardwill make it move forward but in sometimes, it will move toleftorright"}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Reinforcement_learning",children:"https://en.wikipedia.org/wiki/Reinforcement_learning"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14",children:"https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690",children:"https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html",children:"https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial",children:"https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.freecodecamp.org/news/train-an-ai-to-play-a-snake-game-using-python",children:"https://www.freecodecamp.org/news/train-an-ai-to-play-a-snake-game-using-python"})})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(296540);const r={},a=i.createContext(r);function o(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);