"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[2895],{477966:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>l,frontMatter:()=>r,metadata:()=>n,toc:()=>h});const n=JSON.parse('{"id":"technologies/apache/apache-hive","title":"Apache Hive","description":"- Hive is a distributed data management for Hadoop","source":"@site/docs/technologies/apache/apache-hive.md","sourceDirName":"technologies/apache","slug":"/technologies/apache/apache-hive","permalink":"/technologies/apache/apache-hive","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache/apache-hive.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1701793554000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Apache HBase","permalink":"/technologies/apache/apache-hbase"},"next":{"title":"Hudi","permalink":"/technologies/apache/apache-hudi"}}');var a=i(474848),s=i(28453);const r={},o="Apache Hive",d={},h=[{value:"Hive is not",id:"hive-is-not",level:2},{value:"Features of Hive",id:"features-of-hive",level:2},{value:"Partitioning of table",id:"partitioning-of-table",level:2},{value:"Built-in Operators",id:"built-in-operators",level:2},{value:"Built-in Functions",id:"built-in-functions",level:2},{value:"Hive LLAP",id:"hive-llap",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Working of Hive",id:"working-of-hive",level:2},{value:"Others - Presto",id:"others---presto",level:2},{value:"References",id:"references",level:2}];function c(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"apache-hive",children:"Apache Hive"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Hive is a distributed data management for Hadoop"}),"\n",(0,a.jsx)(t.li,{children:"It supports SQL-like query option HiveSQL (HSQL) to access big data"}),"\n",(0,a.jsx)(t.li,{children:"It can be primarily used for Data mining purpose"}),"\n",(0,a.jsx)(t.li,{children:"It runs on top of Hadoop"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"hive-is-not",children:"Hive is not"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"A relational database"}),"\n",(0,a.jsx)(t.li,{children:"A design for OnLine Transaction Processing (OLTP)"}),"\n",(0,a.jsx)(t.li,{children:"A language for real-time queries and row-level updates"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"features-of-hive",children:"Features of Hive"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"It stores schema in a database and processed data into HDFS."}),"\n",(0,a.jsx)(t.li,{children:"It is designed for OLAP."}),"\n",(0,a.jsx)(t.li,{children:"It provides SQL type language for querying called HiveQL or HQL."}),"\n",(0,a.jsx)(t.li,{children:"It is familiar, fast, scalable, and extensible."}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:["Apache Hive is a ",(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Data_warehouse",children:"data warehouse"})," software project built on top of ",(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Apache_Hadoop",children:"Apache Hadoop"})," for providing data query and analysis.Hive gives an ",(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/SQL",children:"SQL"}),"-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the ",(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/MapReduce",children:"MapReduce"})," Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (",(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Apache_Hive#HiveQL",children:"HiveQL"}),") into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids portability of SQL-based applications to Hadoop"]}),"\n",(0,a.jsx)(t.h2,{id:"partitioning-of-table",children:"Partitioning of table"}),"\n",(0,a.jsx)(t.p,{children:"Hive stores tables in partitions. Partitions are used to divide the table into related parts. Partitions make data querying more efficient. For example in the above weather table the data can be partitioned on the basis of year and month and when query is fired on weather table this partition can be used as one of the column."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-sql",children:"CREATE EXTERNAL TABLE IF NOT EXSISTS weatherext ( wban INT, date STRING)\nPARTITIONED BY (year INT, month STRING)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLOCATION ' /hive/data/weatherext';\n"})}),"\n",(0,a.jsx)(t.p,{children:"Loading data in partitioned tables is different than non-partitioned one. There is little manual work of mentioning the partition data. Data can be loaded in partition, year 2012 and month 01 and 02 as follows:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-sql",children:"LOAD DATA INPATH 'hdfs:/data/2012.txt' INTO TABLE weatherext PARTITION (year=2012, month='01');\nLOAD DATA INPATH 'hdfs:/data/2012.txt' INTO TABLE weatherext PARTITION (year=2012, month='02');\n"})}),"\n",(0,a.jsx)(t.p,{children:"This creates the partitioned table and makes different folder for each partition which helps in querying data."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://www.dezyre.com/hadoop-tutorial/apache-hive-tutorial-tables",children:"https://www.dezyre.com/hadoop-tutorial/apache-hive-tutorial-tables"})}),"\n",(0,a.jsx)(t.p,{children:"Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data."}),"\n",(0,a.jsx)(t.p,{children:"Tables or partitions are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying. Bucketing works based on the value of hash function of some column of a table."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://www.tutorialspoint.com/hive/hive_partitioning.htm",children:"https://www.tutorialspoint.com/hive/hive_partitioning.htm"})}),"\n",(0,a.jsx)(t.h2,{id:"built-in-operators",children:"Built-in Operators"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://www.tutorialspoint.com/hive/hive_built_in_operators.htm",children:"https://www.tutorialspoint.com/hive/hive_built_in_operators.htm"})}),"\n",(0,a.jsx)(t.h2,{id:"built-in-functions",children:"Built-in Functions"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://www.tutorialspoint.com/hive/hive_built_in_functions.htm",children:"https://www.tutorialspoint.com/hive/hive_built_in_functions.htm"})}),"\n",(0,a.jsx)(t.h2,{id:"hive-llap",children:"Hive LLAP"}),"\n",(0,a.jsx)(t.p,{children:"Hive LLAP stands for Long Live and Process that is part of HortonWorks distribution. It is being said that it was launched to compete with impala of cloudera."}),"\n",(0,a.jsx)(t.p,{children:"It consists of daemons that runs hive queries so worker tasks are being run inside daemons only."}),"\n",(0,a.jsx)(t.p,{children:"And all small queries run by these daemons rather than YARN container. It helps in pre fetching and caching of columns before query runs."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://cwiki.apache.org/confluence/display/Hive/LLAP",children:"https://cwiki.apache.org/confluence/display/Hive/LLAP"})}),"\n",(0,a.jsx)(t.h2,{id:"architecture",children:"Architecture"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:i(807043).A+"",width:"600",height:"330"})}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:(0,a.jsx)(t.strong,{children:"Unit Name"})}),(0,a.jsx)(t.th,{children:(0,a.jsx)(t.strong,{children:"Operation"})})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"User Interface"}),(0,a.jsx)(t.td,{children:"Hive is a data warehouse infrastructure software that can create interaction between user and HDFS. The user interfaces that Hive supports are Hive Web UI, Hive command line, and Hive HD Insight (In Windows server)."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"Meta Store"}),(0,a.jsx)(t.td,{children:"Hive chooses respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"HiveQL Process Engine"}),(0,a.jsx)(t.td,{children:"HiveQL is similar to SQL for querying on schema info on the Metastore. It is one of the replacements of traditional approach for MapReduce program. Instead of writing MapReduce program in Java, we can write a query for MapReduce job and process it."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"Execution Engine"}),(0,a.jsx)(t.td,{children:"The conjunction part of HiveQL process Engine and MapReduce is Hive Execution Engine. Execution engine processes the query and generates results as same as MapReduce results. It uses the flavor of MapReduce."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"HDFS or HBASE"}),(0,a.jsx)(t.td,{children:"Hadoop distributed file system or HBASE are the data storage techniques to store data into file system."})]})]})]}),"\n",(0,a.jsx)(t.h2,{id:"working-of-hive",children:"Working of Hive"}),"\n",(0,a.jsx)(t.p,{children:"The following diagram depicts the workflow between Hive and Hadoop."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:i(912564).A+"",width:"601",height:"359"})}),"\n",(0,a.jsx)(t.p,{children:"The following table defines how Hive interacts with Hadoop framework:"}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:(0,a.jsx)(t.strong,{children:"Step No."})}),(0,a.jsx)(t.th,{children:(0,a.jsx)(t.strong,{children:"Operation"})}),(0,a.jsx)(t.th,{children:(0,a.jsx)(t.strong,{children:"Description"})})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"1"}),(0,a.jsx)(t.td,{children:"Execute Query"}),(0,a.jsx)(t.td,{children:"The Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"2"}),(0,a.jsx)(t.td,{children:"Get Plan"}),(0,a.jsx)(t.td,{children:"The driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"3"}),(0,a.jsx)(t.td,{children:"Get Metadata"}),(0,a.jsx)(t.td,{children:"The compiler sends metadata request to Metastore (any database)."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"4"}),(0,a.jsx)(t.td,{children:"Send Metadata"}),(0,a.jsx)(t.td,{children:"Metastore sends metadata as a response to the compiler."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"5"}),(0,a.jsx)(t.td,{children:"Send Plan"}),(0,a.jsx)(t.td,{children:"The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"6"}),(0,a.jsx)(t.td,{children:"Execute Plan"}),(0,a.jsx)(t.td,{children:"The driver sends the execute plan to the execution engine."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"7"}),(0,a.jsx)(t.td,{children:"Execute Job"}),(0,a.jsx)(t.td,{children:"Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"7.1"}),(0,a.jsx)(t.td,{children:"Metadata Ops"}),(0,a.jsx)(t.td,{children:"Meanwhile in execution, the execution engine can execute metadata operations with Metastore."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"8"}),(0,a.jsx)(t.td,{children:"Fetch Result"}),(0,a.jsx)(t.td,{children:"The execution engine receives the results from Data nodes."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"9"}),(0,a.jsx)(t.td,{children:"Send Results"}),(0,a.jsx)(t.td,{children:"The execution engine sends those resultant values to the driver."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"10"}),(0,a.jsx)(t.td,{children:"Send Results"}),(0,a.jsx)(t.td,{children:"The driver sends the results to Hive Interfaces."})]})]})]}),"\n",(0,a.jsx)(t.h2,{id:"others---presto",children:"Others - Presto"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://blog.treasuredata.com/blog/2015/03/20/presto-versus-hive",children:"https://blog.treasuredata.com/blog/2015/03/20/presto-versus-hive"})}),"\n",(0,a.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Apache_Hive",children:"https://en.wikipedia.org/wiki/Apache_Hive"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://www.tutorialspoint.com/hive/hive_introduction.htm",children:"https://www.tutorialspoint.com/hive/hive_introduction.htm"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://analyticshut.com/hive-collect-set-vs-collect-list/",children:"Difference between COLLECT_SET and COLLECT_LIST in Hive"})})]})}function l(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},807043:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Technologies-Apache-Apache-Hive-image1-0f7ecdc55a5ea6d6f8a4f6b0f0fc672d.jpg"},912564:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Technologies-Apache-Apache-Hive-image2-6b1635938bad123ab1c4235109108e33.jpg"},28453:(e,t,i)=>{i.d(t,{R:()=>r,x:()=>o});var n=i(296540);const a={},s=n.createContext(a);function r(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);