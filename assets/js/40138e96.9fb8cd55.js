"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[69943],{949988:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>d,frontMatter:()=>l,metadata:()=>a,toc:()=>h});const a=JSON.parse('{"id":"ai/llm/langchain","title":"Langchain","description":"Welcome to LangChain - \ud83e\udd9c\ud83d\udd17 LangChain 0.0.180","source":"@site/docs/ai/llm/langchain.md","sourceDirName":"ai/llm","slug":"/ai/llm/langchain","permalink":"/ai/llm/langchain","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/langchain.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1737994431000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Intro","permalink":"/ai/llm/intro"},"next":{"title":"Libraries","permalink":"/ai/llm/libraries"}}');var t=i(474848),s=i(28453);const l={},r="Langchain",o={},h=[{value:"Langchain vs LlamaIndex",id:"langchain-vs-llamaindex",level:4},{value:"LangGraph",id:"langgraph",level:4},{value:"Courses",id:"courses",level:5},{value:"LangSmith",id:"langsmith",level:4},{value:"SmolAgent - Agents",id:"smolagent---agents",level:2},{value:"Building your agent",id:"building-your-agent",level:3},{value:"Links",id:"links",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"langchain",children:"Langchain"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://python.langchain.com/en/latest/index.html",children:"Welcome to LangChain - \ud83e\udd9c\ud83d\udd17 LangChain 0.0.180"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://huyenchip.com/2023/04/11/llm-engineering.html",children:"Building LLM applications for production"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.makeuseof.com/langchain-llm-introduction/",children:"Introduction to LangChain LLM: A Beginner\u2019s Guide"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain",children:"How to Build LLM Applications with LangChain | DataCamp"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Langchain Modules",src:i(938157).A+"",width:"999",height:"433"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python -m pip install --upgrade langchain[llm]\npip install chromadb\npip install pypdf\n\npip install chainlit\nchainlit hello\n\nchainlit run document_qa.py\n"})}),"\n",(0,t.jsx)(n.h4,{id:"langchain-vs-llamaindex",children:"Langchain vs LlamaIndex"}),"\n",(0,t.jsx)(n.p,{children:"Both LangChain & LlamaIndex offer distinct approaches to implementing RAG workflows."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LangChain"})," follows a modular pipeline starting with Document Loaders that handle various file formats, followed by Text Splitters for chunk management, and Embeddings for vector creation."]}),"\n",(0,t.jsx)(n.p,{children:"It then utilizes Vector Stores like SingleStore, FAISS or Chroma for storage, a Retriever for similarity search, and finally, an LLM Chain for response generation. This framework emphasizes composability and flexibility in pipeline construction."}),"\n",(0,t.jsxs)(n.p,{children:["On the other hand, ",(0,t.jsx)(n.strong,{children:"LlamaIndex"})," begins with Data Connectors for multi-source loading, employs a Node Parser for sophisticated document processing, and features diverse Index Construction options including vector, list, and tree structures."]}),"\n",(0,t.jsx)(n.p,{children:"It implements a Storage Context for persistent storage, an advanced Query Engine for retrieval, and Response Synthesis for context integration. LlamaIndex specializes in data indexing and retrieval, offering more sophisticated indexing structures out of the box, while maintaining a focus on ease of use with structured data."}),"\n",(0,t.jsx)(n.p,{children:"The key distinction lies in their approaches: LangChain prioritizes customization and pipeline flexibility, while LlamaIndex emphasizes structured data handling and advanced indexing capabilities, making each framework suitable for different use cases in RAG implementations."}),"\n",(0,t.jsx)(n.p,{children:"No matter what AI framework you pick, I always recommend using a robust data platform like SingleStore that supports not just vector storage but also hybrid search, low latency, fast data ingestion, all data types, AI frameworks integration, and much more."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:i(107044).A+"",width:"1000",height:"750"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://dev.to/pavanbelagatti/a-beginners-guide-to-building-llm-powered-applications-with-langchain-2d6e",children:"A Beginner\u2019s Guide to Building LLM-Powered Applications with LangChain! - DEV Community"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=lOic_3bcxT8",children:"Understanding LlamaIndex in 9 Minutes! - YouTube"})}),"\n",(0,t.jsx)(n.h4,{id:"langgraph",children:"LangGraph"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=QblpBsipCwM",children:"Build Agentic Workflows Using LangGraph! - YouTube"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.curotec.com/insights/langchain-vs-langgraph-framework-comparison/",children:"LangChain vs. LangGraph: Which AI Framwork Is Right for You?"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://towardsdatascience.com/ai-agent-workflows-a-complete-guide-on-whether-to-build-with-langgraph-or-langchain-117025509fa0",children:"AI Agent Workflows: A Complete Guide on Whether to Build With LangGraph or LangChain | by Sandi Besen | Towards Data Science"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/neo4j-labs/llm-graph-builder",children:"GitHub - neo4j-labs/llm-graph-builder: Neo4j graph construction from unstructured data using LLMs"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://neo4j.com/labs/genai-ecosystem/llm-graph-builder/",children:"Neo4j LLM Knowledge Graph Builder - Extract Nodes and Relationships from Unstructured Text"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://neo4j.com/blog/graphrag-python-package/",children:"GraphRAG Python Package: Accelerating GenAI With Knowledge Graphs"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html#retriever-configuration",children:"User Guide: RAG \u2014 neo4j-graphrag-python documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://neo4j.com/labs/genai-ecosystem/genai-stack/",children:"Site Unreachable"})}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"courses",children:"Courses"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://graphacademy.neo4j.com/",children:"GraphAcademy \u2014 Free, Self-Paced, Hands-on Online Training"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://graphacademy.neo4j.com/courses/modeling-fundamentals/",children:"Graph Data Modeling Fundamentals"})}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"langsmith",children:"LangSmith"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.langchain.com/langsmith",children:"LangSmith"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://medium.com/around-the-prompt/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5",children:"What is LangSmith and why should I care as a developer? | by Logan Kilpatrick | Around the Prompt | Medium"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"smolagent---agents",children:"SmolAgent - Agents"}),"\n",(0,t.jsx)(n.h3,{id:"building-your-agent",children:"Building your agent"}),"\n",(0,t.jsx)(n.p,{children:"To initialize a minimal agent, you need at least these two arguments:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"model"}),", a text-generation model to power your agent - because the agent is different from a simple LLM, it is a system that uses a LLM as its engine. You can use any of these options:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/smolagents/v1.5.1/en/reference/agents#smolagents.TransformersModel",children:"TransformersModel"}),"\xa0takes a pre-initialized\xa0",(0,t.jsx)(n.code,{children:"transformers"}),"\xa0pipeline to run inference on your local machine using\xa0",(0,t.jsx)(n.code,{children:"transformers"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/smolagents/v1.5.1/en/reference/agents#smolagents.HfApiModel",children:"HfApiModel"}),"\xa0leverages a\xa0",(0,t.jsx)(n.code,{children:"huggingface_hub.InferenceClient"}),"\xa0under the hood."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/smolagents/v1.5.1/en/reference/agents#smolagents.LiteLLMModel",children:"LiteLLMModel"}),"\xa0lets you call 100+ different models through\xa0",(0,t.jsx)(n.a,{href:"https://docs.litellm.ai/",children:"LiteLLM"}),"!"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/smolagents/v1.5.1/en/reference/agents#smolagents.AzureOpenAIServerModel",children:"AzureOpenAIServerModel"}),"\xa0allows you to use OpenAI models deployed in\xa0",(0,t.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/products/ai-services/openai-service",children:"Azure"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"tools"}),", a list of\xa0",(0,t.jsx)(n.code,{children:"Tools"}),"\xa0that the agent can use to solve the task. It can be an empty list. You can also add the default toolbox on top of your\xa0",(0,t.jsx)(n.code,{children:"tools"}),"\xa0list by defining the optional argument\xa0",(0,t.jsx)(n.code,{children:"add_base_tools=True"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"links",children:"Links"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/huggingface/smolagents",children:"GitHub - huggingface/smolagents: \ud83e\udd17 smolagents: a barebones library for agents. Agents write python code to call tools and orchestrate other agents."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/smolagents/en/index",children:"smolagents"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=uzskhpH5fvo",children:"Build Multi-Agent Systems with SmolAgents - YouTube"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=VSm5-CX4QaM",children:"Build AI Agents using HuggingFace's SmolAgents | Agentic AI - YouTube"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=VSm5-CX4QaM",children:"Build AI Agents using HuggingFace's SmolAgents | Agentic AI - YouTube"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.datacamp.com/tutorial/smolagents",children:"Hugging Face's Smolagents: A Guide With Examples"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},107044:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/Pasted image 20241118181518-2aefbe281b2f1d9308743143c5ef0874.jpg"},938157:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/Screenshot 2024-04-16 at 7.02.28 PM-e80d354345abda939879a940d9c0eb4b.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>r});var a=i(296540);const t={},s=a.createContext(t);function l(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);