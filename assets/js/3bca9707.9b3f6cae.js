"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[9595],{321681:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>d,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ai/ml-algorithms/vector-embeddings","title":"Vector Embeddings","description":"Vector embeddings are a way to convert words and sentences and other data into numbers that capture their meaning and relationships. They represent different data types as points in a multidimensional space, where similar data points are clustered closer together. These numerical representations help machines understand and process this data more effectively.","source":"@site/docs/ai/ml-algorithms/vector-embeddings.md","sourceDirName":"ai/ml-algorithms","slug":"/ai/ml-algorithms/vector-embeddings","permalink":"/ai/ml-algorithms/vector-embeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/ml-algorithms/vector-embeddings.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1757661758000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Support Vector Machine (SVM)","permalink":"/ai/ml-algorithms/support-vector-machine-svm"},"next":{"title":"ML Fundamentals","permalink":"/ai/ml-fundamentals/"}}');var i=s(474848),r=s(28453);const d={},a="Vector Embeddings",o={},c=[{value:"Types of vector embeddings",id:"types-of-vector-embeddings",level:3},{value:"Word embeddings",id:"word-embeddings",level:4},{value:"Sentence embeddings",id:"sentence-embeddings",level:4},{value:"Document embeddings",id:"document-embeddings",level:4},{value:"Image embeddings",id:"image-embeddings",level:4},{value:"User embeddings",id:"user-embeddings",level:4},{value:"Product embeddings",id:"product-embeddings",level:4},{value:"Are embeddings and vectors the same thing?",id:"are-embeddings-and-vectors-the-same-thing",level:3},{value:"Use Cases",id:"use-cases",level:3},{value:"Text Embeddings / Transformers",id:"text-embeddings--transformers",level:2},{value:"Links",id:"links",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"vector-embeddings",children:"Vector Embeddings"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Vector embeddings are a way to convert words and sentences and other data into numbers that capture their meaning and relationships."})," They represent different data types as points in a multidimensional space, where similar data points are clustered closer together. These numerical representations help machines understand and process this data more effectively."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://www.elastic.co/what-is/word-embedding",children:"Word"})," and sentence embeddings are two of the most common subtypes of vector embeddings, but there are others. Some vector embeddings can represent entire documents, as well as image vectors designed to match up visual content, user profile vectors to determine a user\u2019s preferences, product vectors that help identify similar products and many others. Vector embeddings help ",(0,i.jsx)(n.a,{href:"https://www.elastic.co/what-is/machine-learning",children:"machine learning"})," algorithms find patterns in data and perform tasks such as ",(0,i.jsx)(n.a,{href:"https://www.elastic.co/what-is/sentiment-analysis",children:"sentiment analysis"}),", language translation, recommendation systems, and many more."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"vector-embeddings",src:s(164435).A+"",width:"999",height:"453"})}),"\n",(0,i.jsx)(n.h3,{id:"types-of-vector-embeddings",children:"Types of vector embeddings"}),"\n",(0,i.jsx)(n.h4,{id:"word-embeddings",children:(0,i.jsx)(n.a,{href:"https://www.elastic.co/what-is/word-embedding",children:"Word embeddings"})}),"\n",(0,i.jsx)(n.p,{children:"Represent individual words as vectors. Techniques like Word2Vec, GloVe, and FastText learn word embeddings by capturing semantic relationships and contextual information from large text corpora."}),"\n",(0,i.jsx)(n.h4,{id:"sentence-embeddings",children:"Sentence embeddings"}),"\n",(0,i.jsx)(n.p,{children:"Represent entire sentences as vectors. Models like Universal Sentence Encoder (USE) and SkipThought generate embeddings that capture the overall meaning and context of the sentences."}),"\n",(0,i.jsx)(n.h4,{id:"document-embeddings",children:"Document embeddings"}),"\n",(0,i.jsx)(n.p,{children:"Represent documents (anything from newspaper articles and academic papers to books) as vectors. They capture the semantic information and context of the entire document. Techniques like Doc2Vec and Paragraph Vectors are designed to learn document embeddings."}),"\n",(0,i.jsx)(n.h4,{id:"image-embeddings",children:"Image embeddings"}),"\n",(0,i.jsx)(n.p,{children:"Represent images as vectors by capturing different visual features. Techniques like convolutional neural networks (CNNs) and pre-trained models like ResNet and VGG generate image embeddings for tasks like image classification, object detection, and image similarity."}),"\n",(0,i.jsx)(n.h4,{id:"user-embeddings",children:"User embeddings"}),"\n",(0,i.jsxs)(n.p,{children:["Represent users in a system or platform as vectors. They capture user preferences, ",(0,i.jsx)(n.a,{href:"https://www.elastic.co/what-is/user-behavior-analytics",children:"behaviors"}),", and characteristics. User embeddings can be used in everything from recommendation systems to personalized marketing as well as user segmentation."]}),"\n",(0,i.jsx)(n.h4,{id:"product-embeddings",children:"Product embeddings"}),"\n",(0,i.jsx)(n.p,{children:"Represent products in ecommerce or recommendation systems as vectors. They capture a product\u2019s attributes, features, and any other semantic information available. Algorithms can then use these embeddings to compare, recommend, and analyze products based on their vector representations."}),"\n",(0,i.jsx)(n.h3,{id:"are-embeddings-and-vectors-the-same-thing",children:"Are embeddings and vectors the same thing?"}),"\n",(0,i.jsx)(n.p,{children:"In the context of vector embeddings, yes, embeddings and vectors are the same thing. Both refer to numerical representations of data, where each data point is represented by a vector in a high-dimensional space."}),"\n",(0,i.jsx)(n.h3,{id:"use-cases",children:"Use Cases"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Recommendation systems (i.e. Netflix-style if-you-like-these-movies-you\u2019ll-like-this-one-too)"}),"\n",(0,i.jsx)(n.li,{children:"All kinds of search"}),"\n",(0,i.jsx)(n.li,{children:"Text search (like Google Search)"}),"\n",(0,i.jsx)(n.li,{children:"Image search (like Google Reverse Image Search)"}),"\n",(0,i.jsx)(n.li,{children:"Chatbots and question-answering systems"}),"\n",(0,i.jsx)(n.li,{children:"Data preprocessing (preparing data to be fed into a machine learning model)"}),"\n",(0,i.jsx)(n.li,{children:"One-shot/zero-shot learning (i.e. machine learning models that learn from almost no training data)"}),"\n",(0,i.jsx)(n.li,{children:"Fraud detection/outlier detection"}),"\n",(0,i.jsx)(n.li,{children:'Typo detection and all manners of "fuzzy matching"'}),"\n",(0,i.jsx)(n.li,{children:"Detecting when ML models go stale (drift)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://www.elastic.co/what-is/vector-embedding",children:"What are vector embeddings? | A Comprehensive Vector Embeddings Guide | Elastic"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings",children:"Meet AI\u2019s multitool: Vector embeddings | Google Cloud Blog"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://www.pinecone.io/learn/vector-embeddings/",children:"What are Vector Embeddings | Pinecone"})}),"\n",(0,i.jsx)(n.h2,{id:"text-embeddings--transformers",children:"Text Embeddings / Transformers"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/SeanLee97/AnglE",children:"GitHub - SeanLee97/AnglE: Angle-optimized Text Embeddings | \ud83d\udd25 SOTA on STS and MTEB Leaderboard"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://huggingface.co/spaces/mteb/leaderboard",children:"MTEB Leaderboard - a Hugging Face Space by mteb"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"MTEB - Massive Text Embeddings Benchmark"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://platform.openai.com/tokenizer",children:(0,i.jsx)(n.strong,{children:"OpenAI Platform - Calculate Tokens for text"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://gptforwork.com/tools/tokenizer",children:"Tokenizer and token counter (GPT, Claude, Gemini, Grok) | GPT for Work"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"sentence-transformers/all-MiniLM-L6-v2 \xb7 Hugging Face"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.sbert.net/docs/pretrained_models.html",children:"Pretrained Models - Sentence-Transformers documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers",children:"sentence-transformers (Sentence Transformers)"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"gemini-embedding-001"}),"\xa0- ",(0,i.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs/embeddings",children:"Embeddings \xa0|\xa0 Gemini API \xa0|\xa0 Google AI for Developers"})]}),"\n",(0,i.jsxs)(n.li,{children:["voyage-context-3","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"It is a contextualized chunk embedding model that produces vectors for chunks that capture the full document context without any manual metadata and context augmentation."}),"\n",(0,i.jsx)(n.li,{children:"This is unlike common chunk embedding models that embed chunks independently."}),"\n",(0,i.jsx)(n.li,{children:"This makes your embeddings semantically rich and context-aware, without the overhead of dealing with metadata and hence the speed."}),"\n",(0,i.jsx)(n.li,{children:"This embedding model cuts vectorDB costs by 200x."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/ai/nlp/word-embedding-to-transformers",children:"word-embedding-to-transformers"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.youtube.com/watch?v=yfHHvmaMkcA&ab_channel=freeCodeCamp.org",children:(0,i.jsx)(n.strong,{children:"Vector Embeddings Tutorial - Code Your Own AI Assistant with GPT-4 API + LangChain + NLP - YouTube"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.youtube.com/watch?v=QdDoFfkVkcw&ab_channel=RabbitHoleSyndrome",children:"$0 Embeddings (OpenAI vs. free & open source) - YouTube"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://blog.codewithdan.com/the-abcs-of-ai-transformers-tokens-and-embeddings-a-lego-story/#:~:text=The%20embeddings%20serve%20as%20the,an%20encoder%20and%20a%20decoder.",children:"The ABCs of AI Transformers, Tokens, and Embeddings: A LEGO Story - Code with Dan Blog"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/BAAI/bge-large-en",children:"BAAI/bge-large-en \xb7 Hugging Face"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},164435:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Pasted image 20231216192551-f1ad577865a78f6dffacbafa8055ed3c.jpg"},28453:(e,n,s)=>{s.d(n,{R:()=>d,x:()=>a});var t=s(296540);const i={},r=t.createContext(i);function d(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);