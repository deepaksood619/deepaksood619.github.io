"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[40561],{354832:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ai/llm/prompt-engineering","title":"Prompt Engineering","description":"Prompt Engineering \\\\| Kaggle","source":"@site/docs/ai/llm/prompt-engineering.md","sourceDirName":"ai/llm","slug":"/ai/llm/prompt-engineering","permalink":"/ai/llm/prompt-engineering","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/prompt-engineering.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1747376238000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Pricing / Costs","permalink":"/ai/llm/pricing-costs"},"next":{"title":"Prompt Examples","permalink":"/ai/llm/prompt-examples"}}');var s=i(474848),r=i(28453);const o={},a="Prompt Engineering",l={},c=[{value:"Prompting Principles",id:"prompting-principles",level:2},{value:"Principle 1: Write clear and specific instructions",id:"principle-1-write-clear-and-specific-instructions",level:3},{value:"Principle 2: Give the model time to &quot;think&quot;",id:"principle-2-give-the-model-time-to-think",level:3},{value:"Imitating",id:"imitating",level:3},{value:"Prompting Techniques",id:"prompting-techniques",level:2},{value:"Chain-of-thought",id:"chain-of-thought",level:3},{value:"Least to most prompting",id:"least-to-most-prompting",level:3},{value:"Other techniques",id:"other-techniques",level:3},{value:"Parameters",id:"parameters",level:2},{value:"Temperature",id:"temperature",level:3},{value:"Other Topics",id:"other-topics",level:2},{value:"Summarization",id:"summarization",level:3},{value:"Assistant APIs",id:"assistant-apis",level:2},{value:"Learning",id:"learning",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"prompt-engineering",children:"Prompt Engineering"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.kaggle.com/whitepaper-prompt-engineering",children:"Prompt Engineering | Kaggle"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt design"})," is the process of creating a prompt that is tailored to the specific task that the system is being asked to perform."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt engineering"})," is the process of creating a prompt that is designed to improve performance."]}),"\n",(0,s.jsx)(n.h2,{id:"prompting-principles",children:"Prompting Principles"}),"\n",(0,s.jsx)(n.h3,{id:"principle-1-write-clear-and-specific-instructions",children:"Principle 1: Write clear and specific instructions"}),"\n",(0,s.jsx)(n.p,{children:"Tactic 1: Use delimiters to clearly indicate distinct parts of the input"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Delimiters can be anything like: , ",(0,s.jsx)(n.code,{children:'"""'}),", ",(0,s.jsx)(n.code,{children:"< >"}),", ",(0,s.jsx)(n.code,{children:"<tag> </tag>"}),", ",(0,s.jsx)(n.code,{children:":"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Tactic 2: Ask for a structured output"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"JSON, HTML"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Tactic 3: Ask the model to check whether conditions are satisfied"}),"\n",(0,s.jsx)(n.p,{children:'Tactic 4: "Few-shot" prompting'}),"\n",(0,s.jsx)(n.h3,{id:"principle-2-give-the-model-time-to-think",children:'Principle 2: Give the model time to "think"'}),"\n",(0,s.jsx)(n.p,{children:"Tactic 1: Specify the steps required to complete a task"}),"\n",(0,s.jsx)(n.p,{children:"Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"}),"\n",(0,s.jsx)(n.h3,{id:"imitating",children:"Imitating"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"In the style of x write about y"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prompting-techniques",children:"Prompting Techniques"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"prompt-techniques",src:i(927711).A+"",width:"999",height:"477"})}),"\n",(0,s.jsx)(n.h3,{id:"chain-of-thought",children:"Chain-of-thought"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.em,{children:"Chain-of-thought"})," (CoT) prompting is a technique that allows ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Large_language_models",title:"Large language models",children:"large language models"})," (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. Chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Train_of_thought",title:"Train of thought",children:"train of thought"}),". It allows large language models to overcome difficulties with some reasoning tasks that require ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Logical_reasoning",title:"Logical reasoning",children:"logical thinking"})," and multiple steps to solve, such as ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Arithmetic",title:"Arithmetic",children:"arithmetic"})," or ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Commonsense_reasoning",title:"Commonsense reasoning",children:"commonsense reasoning"})," questions."]}),"\n",(0,s.jsx)(n.h3,{id:"least-to-most-prompting",children:"Least to most prompting"}),"\n",(0,s.jsx)(n.p,{children:"Least-to-most prompting is a prompt engineering technique where complex problems are broken down into smaller, simpler subproblems, and then solved sequentially.\xa0This approach is particularly effective in tasks involving symbolic manipulation, compositional generalization, and mathematical reasoning, often exceeding the performance of Chain-of-Thought prompting on more difficult problems."}),"\n",(0,s.jsx)(n.p,{children:"Here's a breakdown of how it works:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Problem Decomposition:"})," The initial prompt guides the Large Language Model (LLM) to decompose a complex problem into a series of simpler subproblems."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sequential Solving:"})," The LLM then solves each subproblem sequentially, utilizing the solutions to previous subproblems to guide the next step."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enhanced Reasoning:"})," By breaking down complex tasks into simpler components, least-to-most prompting allows LLMs to leverage their reasoning capabilities more effectively, leading to improved performance, especially on challenging problems."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://learnprompting.org/docs/intermediate/least_to_most",children:"Least-to-Most Prompting"})}),"\n",(0,s.jsx)(n.h3,{id:"other-techniques",children:"Other techniques"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Generated knowledge prompting"}),"\n",(0,s.jsx)(n.li,{children:"Least-to-most prompting"}),"\n",(0,s.jsx)(n.li,{children:"Self-consistency decoding"}),"\n",(0,s.jsx)(n.li,{children:"Complexity-based prompting"}),"\n",(0,s.jsx)(n.li,{children:"Self-refine"}),"\n",(0,s.jsx)(n.li,{children:"Tree-of-thought"}),"\n",(0,s.jsx)(n.li,{children:"Maieutic prompting"}),"\n",(0,s.jsx)(n.li,{children:"Directional-stimulus prompting"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Prompt_engineering",children:"Prompt engineering - Wikipedia"})}),"\n",(0,s.jsx)(n.h2,{id:"parameters",children:"Parameters"}),"\n",(0,s.jsx)(n.h3,{id:"temperature",children:"Temperature"}),"\n",(0,s.jsx)(n.p,{children:"Controls the randomness of the model's output. A higher temperature makes the output more random, while a lower temperature makes it more deterministic."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.coltsteele.com/tips/understanding-openai-s-temperature-parameter",children:"Understanding OpenAI's Temperature Parameter | Colt Steele"})}),"\n",(0,s.jsx)(n.h2,{id:"other-topics",children:"Other Topics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Iterative"}),"\n",(0,s.jsx)(n.li,{children:"Summarizing"}),"\n",(0,s.jsx)(n.li,{children:"Inferring"}),"\n",(0,s.jsx)(n.li,{children:"Transforming"}),"\n",(0,s.jsx)(n.li,{children:"Expanding"}),"\n",(0,s.jsx)(n.li,{children:"Chatbot"}),"\n",(0,s.jsx)(n.li,{children:"Conclusion"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/",children:"ChatGPT Prompt Engineering for Developers - DeepLearning.AI"})}),"\n",(0,s.jsx)(n.h3,{id:"summarization",children:"Summarization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"{\n    \"anthropic_version\": \"bedrock-2023-05-31\",\n    \"messages\": [\n        {\n            \"role\" : \"user\",\n            \"content\" : \"You will be given a conversation between a user and an AI assistant.\n             When available, in order to have more context, you will also be give summaries you previously generated.\n             Your goal is to summarize the input conversation.\n\n             When you generate summaries you ALWAYS follow the below guidelines:\n             <guidelines>\n             - Each summary MUST be formatted in XML format.\n             - Each summary must contain at least the following topics: 'user goals', 'assistant actions'.\n             - Each summary, whenever applicable, MUST cover every topic and be place between <topic name='$TOPIC_NAME'></topic>.\n             - You AlWAYS output all applicable topics within <summary></summary>\n             - If nothing about a topic is mentioned, DO NOT produce a summary for that topic.\n             - You summarize in <topic name='user goals'></topic> ONLY what is related to User, e.g., user goals.\n             - You summarize in <topic name='assistant actions'></topic> ONLY what is related to Assistant, e.g., assistant actions.\n             - NEVER start with phrases like 'Here's the summary...', provide directly the summary in the format described below.\n             </guidelines>\n\n             The XML format of each summary is as it follows:\n            <summary>\n                <topic name='$TOPIC_NAME'>\n                    ...\n                </topic>\n                ...\n            </summary>\n\n            Here is the list of summaries you previously generated.\n\n            <previous_summaries>\n            $past_conversation_summary$\n            </previous_summaries>\n\n            And here is the current conversation session between a user and an AI assistant:\n\n            <conversation>\n            $conversation$\n            </conversation>\n\n            Please summarize the input conversation following above guidelines plus below additional guidelines:\n            <additional_guidelines>\n            - ALWAYS strictly follow above XML schema and ALWAYS generate well-formatted XML.\n            - NEVER forget any detail from the input conversation.\n            - You also ALWAYS follow below special guidelines for some of the topics.\n            <special_guidelines>\n                <user_goals>\n                    - You ALWAYS report in <topic name='user goals'></topic> all details the user provided in formulating their request.\n                </user_goals>\n                <assistant_actions>\n                    - You ALWAYS report in <topic name='assistant actions'></topic> all details about action taken by the assistant, e.g., parameters used to invoke actions.\n                </assistant_actions>\n            </special_guidelines>\n            </additional_guidelines>\n            \"\n        }\n    ]\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"assistant-apis",children:"Assistant APIs"}),"\n",(0,s.jsxs)(n.p,{children:["The Assistants API allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries. The Assistants API currently supports three types of ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/assistants/tools",children:"tools"}),": Code Interpreter, Retrieval, and Function calling."]}),"\n",(0,s.jsx)(n.p,{children:"At a high level, a typical integration of the Assistants API has the following flow:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create an ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/assistants/createAssistant",children:"Assistant"})," in the API by defining its custom instructions and picking a model. If helpful, enable tools like Code Interpreter, Retrieval, and Function calling."]}),"\n",(0,s.jsxs)(n.li,{children:["Create a ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/threads",children:"Thread"})," when a user starts a conversation."]}),"\n",(0,s.jsxs)(n.li,{children:["Add ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/messages",children:"Messages"})," to the Thread as the user ask questions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/runs",children:"Run"})," the Assistant on the Thread to trigger responses. This automatically calls the relevant tools."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.freecodecamp.org/news/create-ai-assistants-with-openais-assistants-api/",children:"Create AI Assistants with OpenAI's Assistants API"})}),"\n",(0,s.jsx)(n.p,{children:"Knowledge based retrieval tool -"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/assistants/overview",children:"platform.openai.com/docs/assistants/overview"})}),"\n",(0,s.jsx)(n.h2,{id:"learning",children:"Learning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.freecodecamp.org/news/large-language-models-and-cybersecurity/",children:"Large Language Models and Cybersecurity - What You Should Know"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://magazine.sebastianraschka.com/p/understanding-large-language-models?utm_source=substack&utm_medium=email",children:"Understanding Large Language Models - by Sebastian Raschka"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5",children:"The Art of Prompt Design: Use Clear Syntax | by Scott Lundberg | May, 2023 | Towards Data Science"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.google.com/presentation/d/1wNm1uQs5JnmnxR7es2pb4koEELZ9k_CeTdjvTa38cT8/edit?usp=sharing",children:"Prompt Engineering - Google Slides"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=_ZvnD73m40o",children:"Prompt Engineering Tutorial - Master ChatGPT and LLM Responses - YouTube"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.freecodecamp.org/news/advanced-prompt-engineering-handbook/",children:"Advanced Prompt Engineering for Content Creators - Full Handbook"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2",children:"Prompt Engineering with Llama 2 - DeepLearning.AI"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},927711:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/Screenshot 2024-04-16 at 7.00.28 PM-1475b7ad0d7d9bbd8af1de017fbcb728.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(296540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);