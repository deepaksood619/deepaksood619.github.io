"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[47193],{361137:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>o,contentTitle:()=>i,default:()=>c,frontMatter:()=>l,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"technologies/apache-spark/09-dataframe","title":"DataFrame","description":"- PySpark - Create an empty DataFrame","source":"@site/docs/technologies/apache-spark/09-dataframe.md","sourceDirName":"technologies/apache-spark","slug":"/technologies/apache-spark/09-dataframe","permalink":"/technologies/apache-spark/09-dataframe","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache-spark/09-dataframe.md","tags":[],"version":"current","lastUpdatedBy":"Deeapak Sood","lastUpdatedAt":1765917138000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Examples","permalink":"/technologies/apache-spark/08-examples"},"next":{"title":"SQL Functions / Datasources","permalink":"/technologies/apache-spark/10-sql-functions-datasources"}}');var s=r(474848),n=r(28453);const l={},i="DataFrame",o={},p=[{value:"CAST",id:"cast",level:3},{value:"Spark DataFrame vs Pandas DataFrame",id:"spark-dataframe-vs-pandas-dataframe",level:2},{value:"Spark collect",id:"spark-collect",level:2},{value:"When to avoid Collect()",id:"when-to-avoid-collect",level:4},{value:"collect () vs select ()",id:"collect--vs-select-",level:4},{value:"Writing Data - .saveAsTable() vs .writeTo()",id:"writing-data---saveastable-vs-writeto",level:2},{value:"Using .saveAsTable()",id:"using-saveastable",level:3},{value:"The Workaround \u2014 Dynamic Partition Overwrite",id:"the-workaround--dynamic-partition-overwrite",level:3},{value:"Enter .writeTo(): The Modern API",id:"enter-writeto-the-modern-api",level:3},{value:"Final Thoughts",id:"final-thoughts",level:3},{value:"Use .writeTo() when",id:"use-writeto-when",level:4},{value:"Use .saveAsTable() when",id:"use-saveastable-when",level:4}];function d(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"dataframe",children:"DataFrame"})}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-create-an-empty-dataframe/",children:"PySpark - Create an empty DataFrame"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/convert-pyspark-rdd-to-dataframe/",children:"PySpark - Convert RDD to DataFrame"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pandas/convert-pyspark-dataframe-to-pandas/",children:"PySpark - Convert DataFrame to Pandas"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-show-display-dataframe-contents-in-table/",children:"PySpark - show()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/",children:"PySpark - StructType & StructField"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-column-functions/",children:"PySpark - Column Class"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/",children:"PySpark - select()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-collect/",children:"PySpark - collect()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-withcolumn/",children:"PySpark - withColumn()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-rename-dataframe-column/",children:"PySpark - withColumnRenamed()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-where-filter/",children:"PySpark - where() & filter()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-distinct-to-drop-duplicates/",children:"PySpark - drop() & dropDuplicates()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/",children:"PySpark - orderBy() and sort()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/",children:"PySpark - groupBy()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/",children:"PySpark - join()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-union-and-unionall/",children:"PySpark - union() & unionAll()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-unionbyname/",children:"PySpark - unionByName()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/",children:"PySpark - UDF (User Defined Function)"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-transform-function/",children:"PySpark - transform()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-apply-function-to-column/",children:"PySpark - apply()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-map-transformation/",children:"PySpark - map()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-flatmap-transformation/",children:"PySpark - flatMap()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-foreach-usage-with-examples/",children:"PySpark - foreach()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-sampling-example/",children:"PySpark - sample() vs sampleBy()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/",children:"PySpark - fillna() & fill()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-pivot-and-unpivot-dataframe/",children:"PySpark - pivot() (Row to Column)"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/",children:"PySpark - partitionBy()"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/",children:"PySpark - MapType (Map/Dict)"})}),"\n"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"# Copy pyspark.sql.dataframe.DataFrame to another dataframe\ndf_copy = df.alias('df')\n\n# Subtract two pyspark.sql.dataframe.DataFrame\ndiff_df = df.subtract(df_copy)\ndiff_df.display()\n\nresult_df = df.where(df[\"id\"].isin(['5edc8f7d-0036-4910-84c4-48d46f7eeb04']))\nresult_df.display()\nresult_df.head()\n\n# group by\ndf1.groupBy(F.date_format('updatedAt','yyyy-MM-dd').alias('day')).count().display()\n\n# filter\ndf2 = df1.filter((df1.updatedAt >= \"2023-04-05\"))\ndf3 = df2.filter(df.amount.isNotNull())\n\n# select\ndf1.select('amount').groupby('amount').count().display()\n\n# group by count, order by count desc\nfrom pyspark.sql.functions import desc\n\ndfFilter.sort(desc(df.groupby('DepositTransactionId').count())).display()\n\n# dropDuplicates\ndf = df.dropDuplicates()\n"})}),"\n",(0,s.jsx)(a.h3,{id:"cast",children:"CAST"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'# cast columns to different types\nfrom pyspark.sql.functions import col\n\nfrom pyspark.sql.types import DateType, LongType, DoubleType, IntegerType, BooleanType\n\ndf = df.withColumn("col_name", col("col_name").cast(IntegerType())) \\\n.withColumn("col_name2", col("col_name2").cast(IntegerType())) \\\n.withColumn("col_name3", col("col_name3").cast(BooleanType()))\n'})}),"\n",(0,s.jsx)(a.h2,{id:"spark-dataframe-vs-pandas-dataframe",children:"Spark DataFrame vs Pandas DataFrame"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{style:{textAlign:"center"},children:"Spark DataFrame"}),(0,s.jsx)(a.th,{style:{textAlign:"center"},children:"Pandas DataFrame"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame supports parallelization."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame does not support parallelization."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame has Multiple Nodes."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame has a Single  Node."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"It follows Lazy Execution which means that a task is not executed until an action is performed."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"It follows Eager Execution, which means task is executed immediately."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame is Immutable."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame is Mutable."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Complex operations are difficult to perform as compared to Pandas DataFrame."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Complex operations are easier to perform as compared to Spark DataFrame."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame is distributed and hence processing in the Spark DataFrame is faster for a large amount of data."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame is not distributed and hence processing in the Pandas DataFrame will be slower for a large amount of data."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"sparkDataFrame.count() returns the number of rows."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"pandasDataFrame.count() returns the number of non NA/null observations for each column."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrames are excellent for building a scalable application."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrames can\u2019t be used to build a scalable application."})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame assures fault tolerance."}),(0,s.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame does not assure fault tolerance. We need to implement our own framework to assure it."})]})]})]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://www.geeksforgeeks.org/difference-between-spark-dataframe-and-pandas-dataframe/",children:"Difference Between Spark DataFrame and Pandas DataFrame - GeeksforGeeks"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pandas-vs-pyspark-dataframe-with-examples/",children:"Pandas vs PySpark DataFrame With Examples - Spark By Examples"})}),"\n",(0,s.jsx)(a.h2,{id:"spark-collect",children:"Spark collect"}),"\n",(0,s.jsxs)(a.p,{children:["PySpark RDD/DataFrame ",(0,s.jsx)(a.code,{children:"collect()"})," is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after ",(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-where-filter/",children:"filter()"}),", ",(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/",children:"group()"})," e.t.c. Retrieving larger datasets results in ",(0,s.jsx)(a.code,{children:"OutOfMemory"})," error."]}),"\n",(0,s.jsx)(a.h4,{id:"when-to-avoid-collect",children:"When to avoid Collect()"}),"\n",(0,s.jsxs)(a.p,{children:["Usually, collect() is used to retrieve the action output when you have very small result set and calling ",(0,s.jsx)(a.code,{children:"collect()"})," on an RDD/DataFrame with a bigger result set causes out of memory as it returns the entire dataset (from all workers) to the driver hence we should avoid calling collect() on a larger dataset."]}),"\n",(0,s.jsx)(a.h4,{id:"collect--vs-select-",children:"collect () vs select ()"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.code,{children:"select()"})," is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-collect/",children:"PySpark Collect() - Retrieve data from DataFrame - Spark By Examples"})}),"\n",(0,s.jsx)(a.h2,{id:"writing-data---saveastable-vs-writeto",children:"Writing Data - .saveAsTable() vs .writeTo()"}),"\n",(0,s.jsx)(a.h3,{id:"using-saveastable",children:"Using .saveAsTable()"}),"\n",(0,s.jsxs)(a.p,{children:["This is the ",(0,s.jsx)(a.strong,{children:"classic Spark SQL method"}),". It\u2019s been around for a long time and works with Hive tables, Parquet tables, and modern catalogs like Iceberg."]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:'df.write.mode("overwrite").format("iceberg").saveAsTable("iceberg_jdbc.default.people_data_iceberg")'})}),"\n",(0,s.jsx)(a.p,{children:"This line of code:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Writes a DataFrame to the table people_data_iceberg"}),"\n",(0,s.jsx)(a.li,{children:'Uses the mode "overwrite" to replace existing data'}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Supports Iceberg, Hive, or any table with a registered catalog"}),"\n",(0,s.jsxs)(a.p,{children:["But there\u2019s a catch \u2014 ",(0,s.jsx)(a.strong,{children:'if you use mode("overwrite") without caution'}),", Spark will ",(0,s.jsx)(a.strong,{children:"drop and recreate the entire table"}),", deleting all partitions."]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.strong,{children:"The Partition Overwrite Problem"})}),"\n",(0,s.jsx)(a.p,{children:"Imagine you have this Iceberg table:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{src:"https://media.licdn.com/dms/image/v2/D5612AQH4bpNFVznggQ/article-inline_image-shrink_1000_1488/B56Zpid68mG0AQ-/0/1762588602799?e=1766620800&v=beta&t=Iq6wAq2jJGA2ZfkqhCHTiPbR2-sXPq13d81AimcLoVc",alt:"Article content"})}),"\n",(0,s.jsx)(a.p,{children:"Now, you only want to replace records for India."}),"\n",(0,s.jsx)(a.p,{children:"If you do this:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'india_df = df.filter("country = \'India\'")\nindia_df.write.mode("overwrite").format("iceberg").saveAsTable("iceberg_jdbc.default.people_data_iceberg")\n'})}),"\n",(0,s.jsxs)(a.p,{children:["Boom \u2014 Spark overwrites the entire table unless you ",(0,s.jsx)(a.strong,{children:"enable dynamic partition overwrite"}),"."]}),"\n",(0,s.jsx)(a.h3,{id:"the-workaround--dynamic-partition-overwrite",children:"The Workaround \u2014 Dynamic Partition Overwrite"}),"\n",(0,s.jsx)(a.p,{children:"To safely overwrite only specific partitions, you can enable a Spark config:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")\n'})}),"\n",(0,s.jsx)(a.p,{children:"Then run:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'india_df.write.mode("overwrite").format("iceberg").saveAsTable("iceberg_jdbc.default.people_data_iceberg")\n'})}),"\n",(0,s.jsx)(a.h3,{id:"enter-writeto-the-modern-api",children:"Enter .writeTo(): The Modern API"}),"\n",(0,s.jsxs)(a.p,{children:["Starting from Spark 3.1+, a ",(0,s.jsx)(a.strong,{children:"new, safer, and more expressive API"})," was introduced \u2014 .writeTo()."]}),"\n",(0,s.jsx)(a.p,{children:"Here\u2019s how it looks:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'india_df.writeTo("iceberg_jdbc.default.people_data_iceberg").overwritePartitions()\n'})}),"\n",(0,s.jsxs)(a.p,{children:["This achieves the same goal \u2014 only the India partition is replaced \u2014 but with ",(0,s.jsx)(a.strong,{children:"no extra configuration needed"}),"."]}),"\n",(0,s.jsx)(a.p,{children:"Now Spark will:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Detect the partitions present in the incoming DataFrame (country='India')"}),"\n",(0,s.jsxs)(a.li,{children:["Overwrite ",(0,s.jsx)(a.strong,{children:"only those partitions"})]}),"\n",(0,s.jsx)(a.li,{children:"Leave others (USA, Brazil) untouched"}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Without this config, you risk erasing your whole table."}),"\n",(0,s.jsx)(a.h3,{id:"final-thoughts",children:"Final Thoughts"}),"\n",(0,s.jsxs)(a.p,{children:["While .saveAsTable() remains useful and backward-compatible, it\u2019s a ",(0,s.jsx)(a.strong,{children:"legacy approach"})," designed before modern table formats existed. On the other hand, .writeTo() is built for ",(0,s.jsx)(a.strong,{children:"atomic, schema-aware, and partition-safe writes"})," \u2014 especially for ",(0,s.jsx)(a.strong,{children:"Iceberg"}),", ",(0,s.jsx)(a.strong,{children:"Delta"}),", and ",(0,s.jsx)(a.strong,{children:"Hudi"})," tables."]}),"\n",(0,s.jsx)(a.h4,{id:"use-writeto-when",children:"Use .writeTo() when"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Working with Iceberg or Delta"}),"\n",(0,s.jsx)(a.li,{children:"You want fine-grained control (create, replace, overwritePartitions)"}),"\n",(0,s.jsx)(a.li,{children:"You care about explicit and safe write semantics"}),"\n"]}),"\n",(0,s.jsx)(a.h4,{id:"use-saveastable-when",children:"Use .saveAsTable() when"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"You\u2019re integrating with legacy Hive tables"}),"\n",(0,s.jsx)(a.li,{children:"You already have a global partitionOverwriteMode set"}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://www.linkedin.com/pulse/saveastable-vs-writeto-apache-spark-subtle-powerful-difference-bf9ic/",children:"saveAsTable() vs .writeTo() in Apache Spark: The Subtle but Powerful Difference"})})]})}function c(e={}){const{wrapper:a}={...(0,n.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,a,r)=>{r.d(a,{R:()=>l,x:()=>i});var t=r(296540);const s={},n=t.createContext(s);function l(e){const a=t.useContext(n);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(n.Provider,{value:a},e.children)}}}]);