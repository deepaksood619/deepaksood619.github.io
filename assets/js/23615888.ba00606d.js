"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[25358],{515235:(e,s,a)=>{a.r(s),a.d(s,{assets:()=>o,contentTitle:()=>t,default:()=>p,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"computer-science/operating-system/parallel-processing","title":"Parallel Processing","description":"Parallel Processing, MPP (Massive Parallel Processing)","source":"@site/docs/computer-science/operating-system/parallel-processing.md","sourceDirName":"computer-science/operating-system","slug":"/computer-science/operating-system/parallel-processing","permalink":"/computer-science/operating-system/parallel-processing","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/computer-science/operating-system/parallel-processing.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1734554726000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Others","permalink":"/computer-science/operating-system/others"},"next":{"title":"RAID","permalink":"/computer-science/operating-system/raid"}}');var r=a(474848),n=a(28453);const l={},t="Parallel Processing",o={},c=[{value:"Parallel Processing, MPP (Massive Parallel Processing)",id:"parallel-processing-mpp-massive-parallel-processing",level:2},{value:"Message Passing Interface (MPI)",id:"message-passing-interface-mpi",level:2},{value:"Concurrency vs Parallelism",id:"concurrency-vs-parallelism",level:2},{value:"Embarrassingly Parallel",id:"embarrassingly-parallel",level:2},{value:"Characteristics",id:"characteristics",level:3},{value:"Examples",id:"examples",level:3},{value:"Term origin",id:"term-origin",level:3}];function d(e){const s={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"parallel-processing",children:"Parallel Processing"})}),"\n",(0,r.jsx)(s.h2,{id:"parallel-processing-mpp-massive-parallel-processing",children:"Parallel Processing, MPP (Massive Parallel Processing)"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Symmetric Multi-Processing (SMP)"})," is a tightly coupled multiprocessor system where processors share resources -- single instances of the Operating System (OS), memory, I/O devices and connected using a common bus. SMP is the primary parallel architecture employed in servers and is depicted in the following image"]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"image",src:a(299015).A+"",width:"581",height:"428"})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Massively Parallel Processing (MPP)"})," is the coordinated processing of a single task by multiple processors, each processor using its own OS and memory and communicating with each other using some form of messaging interface. MPP can be setup with a shared nothing or shared disk architecture"]}),"\n",(0,r.jsx)(s.p,{children:"In a shared nothing architecture, there is no single point of contention across the system and nodes do not share memory or disk storage. Data is horizontally partitioned across nodes, such that each node has a subset of rows from each table in the database. Each node then processes only the rows on its own disks. Systems based on this architecture can achieve massive scale as there is no single bottleneck to slow down the system. This is what Emma is looking for."}),"\n",(0,r.jsx)(s.p,{children:"MPP with shared-nothing architecture is depicted in the following image."}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"image",src:a(561560).A+"",width:"623",height:"433"})}),"\n",(0,r.jsx)(s.h2,{id:"message-passing-interface-mpi",children:"Message Passing Interface (MPI)"}),"\n",(0,r.jsxs)(s.p,{children:["Message Passing Interface(MPI) is a standardized and portable ",(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Message-passing",children:"message-passing"})," standard designed by a group of researchers from academia and industry to function on a wide variety of ",(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Parallel_computing",children:"parallel computing"})," architectures. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in ",(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/C_(programming_language)",children:"C"}),", ",(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/C%2B%2B",children:"C++"}),", and ",(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Fortran",children:"Fortran"}),". There are several well-tested and efficient implementations of MPI, many of which are open-source or in the public domain. These fostered the development of a parallel software industry, and encouraged development of portable and scalable large-scale parallel applications."]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Message_Passing_Interface",children:"https://en.wikipedia.org/wiki/Message_Passing_Interface"})}),"\n",(0,r.jsx)(s.h2,{id:"concurrency-vs-parallelism",children:"Concurrency vs Parallelism"}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"Concurrency vs Parallelism",src:a(540966).A+"",width:"999",height:"1300"})}),"\n",(0,r.jsx)(s.h2,{id:"embarrassingly-parallel",children:"Embarrassingly Parallel"}),"\n",(0,r.jsx)(s.p,{children:"In parallel computing, an embarrassingly parallel problem is one that can be easily split into multiple parallel tasks with little to no communication between the tasks:"}),"\n",(0,r.jsx)(s.h3,{id:"characteristics",children:"Characteristics"}),"\n",(0,r.jsx)(s.p,{children:"These problems are considered relatively simple to parallelize, and are well suited to large, internet-based distributed platforms."}),"\n",(0,r.jsx)(s.h3,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(s.p,{children:"Some examples of embarrassingly parallel jobs include:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Rendering 3D graphics"}),"\n",(0,r.jsx)(s.li,{children:"Monte Carlo simulations"}),"\n",(0,r.jsx)(s.li,{children:"Image or video processing"}),"\n",(0,r.jsx)(s.li,{children:"Parameter sweeps"}),"\n",(0,r.jsx)(s.li,{children:"Data mining"}),"\n",(0,r.jsx)(s.li,{children:"Brute-force search"}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"term-origin",children:"Term origin"}),"\n",(0,r.jsx)(s.p,{children:'The term "embarrassingly" refers to parallelization problems that are "embarrassingly easy".'}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/Embarrassingly_parallel",children:"Embarrassingly parallel - Wikipedia"})}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.a,{href:"https://www.freecodecamp.org/news/embarrassingly-parallel-algorithms-explained-with-examples/",children:"Embarrassingly Parallel Algorithms Explained"})})]})}function p(e={}){const{wrapper:s}={...(0,n.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},299015:(e,s,a)=>{a.d(s,{A:()=>i});const i=a.p+"assets/images/Parallel-Processing-image1-3405b69157829f860432e778edcd31e8.jpg"},561560:(e,s,a)=>{a.d(s,{A:()=>i});const i=a.p+"assets/images/Parallel-Processing-image2-9026874e194e5a28753f9606e2115549.jpg"},540966:(e,s,a)=>{a.d(s,{A:()=>i});const i=a.p+"assets/images/Pasted image 20240607132644-067980d08d0479a889a71ae9dc725e30.jpg"},28453:(e,s,a)=>{a.d(s,{R:()=>l,x:()=>t});var i=a(296540);const r={},n=i.createContext(r);function l(e){const s=i.useContext(n);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function t(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(n.Provider,{value:s},e.children)}}}]);