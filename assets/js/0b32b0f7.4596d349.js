"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[63957],{376119:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"ai/nlp/nlp-concepts","title":"NLP Concepts","description":"A token is the technical name for a sequence of characters - such as hairy, his, or :) - that we want to treat as a group. When we count the number of tokens in a text, say, the phrase to be or not to be, we are counting occurrences of these sequences","source":"@site/docs/ai/nlp/nlp-concepts.md","sourceDirName":"ai/nlp","slug":"/ai/nlp/nlp-concepts","permalink":"/ai/nlp/nlp-concepts","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/nlp/nlp-concepts.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1691775957000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"NLP","permalink":"/ai/nlp/intro"},"next":{"title":"NLTK","permalink":"/ai/nlp/nltk"}}');var o=t(474848),i=t(28453);const r={},a="NLP Concepts",c={},l=[{value:"Frequency Distribution",id:"frequency-distribution",level:2},{value:"hapaxes - the words that occur once only",id:"hapaxes---the-words-that-occur-once-only",level:2},{value:"Pronoun Resolution",id:"pronoun-resolution",level:2},{value:"Spoken Dialog Systems",id:"spoken-dialog-systems",level:2},{value:"Recognizing Textual Entailment (RTE)",id:"recognizing-textual-entailment-rte",level:2},{value:"Links",id:"links",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"nlp-concepts",children:"NLP Concepts"})}),"\n",(0,o.jsxs)(n.p,{children:["A ",(0,o.jsx)(n.strong,{children:"token"})," is the technical name for a sequence of characters - such as hairy, his, or :) - that we want to treat as a group. When we count the number of tokens in a text, say, the phrase ",(0,o.jsx)(n.em,{children:"to be or not to be"}),", we are counting occurrences of these sequences"]}),"\n",(0,o.jsxs)(n.p,{children:["A ",(0,o.jsx)(n.strong,{children:"word type"})," is the form or spelling of the word independently of its specific occurrences in a text - that is, the word considered as a unique item of vocabulary."]}),"\n",(0,o.jsx)(n.p,{children:"Now, let's calculate a measure of the lexical richness of the text. The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.code,{children:"len(set(text3)) / len(text3)"})}),"\n",(0,o.jsx)(n.h2,{id:"frequency-distribution",children:"Frequency Distribution"}),"\n",(0,o.jsx)(n.p,{children:'It is a "distribution" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items.'}),"\n",(0,o.jsx)(n.h2,{id:"hapaxes---the-words-that-occur-once-only",children:"hapaxes - the words that occur once only"}),"\n",(0,o.jsxs)(n.p,{children:["A ",(0,o.jsx)(n.strong,{children:"collocation"})," is a sequence of words that occur together unusually often. Thus ",(0,o.jsx)(n.em,{children:"red wine"})," is a collocation, whereas ",(0,o.jsx)(n.em,{children:"the wine"})," is not. A characteristic of collocations is that they are resistant to substitution with words that have similar senses; for example, ",(0,o.jsx)(n.em,{children:"maroon wine"})," sounds definitely odd."]}),"\n",(0,o.jsxs)(n.p,{children:["To get a handle on collocations, we start off by extracting from a text a list of word pairs, also known as ",(0,o.jsx)(n.strong,{children:"bigrams"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.code,{children:"list(bigrams(['more', 'is', 'said', 'than', 'done']))"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.code,{children:"[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]"})}),"\n",(0,o.jsx)(n.p,{children:"word sense disambiguation we want to work out which sense of a word was intended in a given context. we automatically disambiguate words using context, exploiting the simple fact that nearby words have closely related meanings"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.em,{children:"serve"}),": help with food or drink; hold an office; put ball into play"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.em,{children:"dish"}),": plate; course of a meal; communications device"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["The lost children were found by the ",(0,o.jsx)(n.em,{children:"searchers"})," (",(0,o.jsx)(n.strong,{children:"agentive"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["The lost children were found by the ",(0,o.jsx)(n.em,{children:"mountain"})," (",(0,o.jsx)(n.strong,{children:"locative"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["The lost children were found by the ",(0,o.jsx)(n.em,{children:"afternoon"})," (",(0,o.jsx)(n.strong,{children:"temporal"}),")"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"pronoun-resolution",children:"Pronoun Resolution"}),"\n",(0,o.jsx)(n.p,{children:'A deeper kind of language understanding is to work out "who did what to whom" - i.e., to detect the subjects and objects of verbs. You learnt to do this in elementary school, but it\'s harder than you might think. In the sentence the thieves stole the paintings it is easy to tell who performed the stealing action. Consider three possible following sentences in (4c), and try to determine what was sold, caught, and found (one case is ambiguous).'}),"\n",(0,o.jsx)(n.p,{children:"The thieves stole the paintings. They were subsequently sold."}),"\n",(0,o.jsx)(n.p,{children:"The thieves stole the paintings. They were subsequently caught."}),"\n",(0,o.jsx)(n.p,{children:"The thieves stole the paintings. They were subsequently found."}),"\n",(0,o.jsxs)(n.p,{children:["Answering this question involves finding the ",(0,o.jsx)(n.strong,{children:"antecedent"})," of the pronoun they, either thieves or paintings. Computational techniques for tackling this problem include ",(0,o.jsx)(n.strong,{children:"anaphora resolution"})," - identifying what a pronoun or noun phrase refers to - and ",(0,o.jsx)(n.strong,{children:"semantic role labeling"})," - identifying how a noun phrase relates to the verb (as agent, patient, instrument, and so on)."]}),"\n",(0,o.jsx)(n.h2,{id:"spoken-dialog-systems",children:"Spoken Dialog Systems"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"image",src:t(470405).A+"",width:"998",height:"526"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Simple Pipeline Architecture for a Spoken Dialogue System: Spoken input (top left) is analyzed, words are recognized, sentences are parsed and interpreted in context, application-specific actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage of the process."})}),"\n",(0,o.jsx)(n.h2,{id:"recognizing-textual-entailment-rte",children:"Recognizing Textual Entailment (RTE)"}),"\n",(0,o.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=VzS8hrOSSAs",children:"Sentence Tokenization in Transformer Code from scratch! - YouTube"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=Nw_PJdmydZY",children:"The complete guide to Transformer neural Networks! - YouTube"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=cUsqFx4Sij8&ab_channel=CodeEmporium",children:"Ngram: The 1st Language Model - YouTube"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=EcuVZzUiHMY",children:"How AI (like ChatGPT) understands word sequences. - YouTube"})})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},470405:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/NLP_NLP-Concepts-image1-08227e0c2629c3721f86e2c5be08215f.jpg"},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(296540);const o={},i=s.createContext(o);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);