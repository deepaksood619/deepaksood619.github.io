"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[46273],{341288:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ai/llm/evaluation-benchmarking","title":"Evaluation / Benchmarking","description":"LLM Evaluation / Monitoring","source":"@site/docs/ai/llm/evaluation-benchmarking.md","sourceDirName":"ai/llm","slug":"/ai/llm/evaluation-benchmarking","permalink":"/ai/llm/evaluation-benchmarking","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/evaluation-benchmarking.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1751484655000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Ethics","permalink":"/ai/llm/ethics"},"next":{"title":"Fintech Use Cases","permalink":"/ai/llm/fintech-use-cases"}}');var s=i(474848),r=i(28453);const a={},o="Evaluation / Benchmarking",l={},c=[{value:"LLM Evaluation / Monitoring",id:"llm-evaluation--monitoring",level:2},{value:"Key Aspects of LLM Evaluation",id:"key-aspects-of-llm-evaluation",level:2},{value:"Metrics",id:"metrics",level:2},{value:"Time to first token (TTFT)",id:"time-to-first-token-ttft",level:3},{value:"Token Per Second (TPS)",id:"token-per-second-tps",level:3},{value:"GPU Usage",id:"gpu-usage",level:3},{value:"Links",id:"links",level:2}];function h(e){const n={a:"a",blockquote:"blockquote",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"evaluation--benchmarking",children:"Evaluation / Benchmarking"})}),"\n",(0,s.jsx)(n.h2,{id:"llm-evaluation--monitoring",children:"LLM Evaluation / Monitoring"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/langfuse/langfuse",children:"GitHub - langfuse/langfuse: \ud83e\udea2 Open source LLM engineering platform: LLM Observability, metrics, evals, prompt management, playground, datasets. Integrates with OpenTelemetry, Langchain, OpenAI SDK, LiteLLM, and more. \ud83c\udf4aYC W23"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://langfuse.com/",children:"Langfuse"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/confident-ai/deepeval",children:(0,s.jsx)(n.strong,{children:"DeepEval"})})," - a simple-to-use, open-source evaluation framework for LLM applications."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.adaline.ai/",children:"Adaline - Iterate, evaluate, deploy, and monitor prompts"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.langchain.com/langsmith",children:"LangSmith"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://galileo.ai/",children:"Galileo AI: The Generative AI Evaluation Company"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.helicone.ai/",children:"Helicone / LLM-Observability for Developers"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/confident-ai/deepteam",children:"GitHub - confident-ai/deepteam: The LLM Red Teaming Framework"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://medium.com/@abhghoshsgp/evaluating-llm-models-a-guide-to-popular-frameworks-59f236542d46",children:"Evaluating LLM Models: A Guide to Popular Frameworks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/llm-evaluation",children:"MLflow LLM Evaluation (Legacy) | MLflow"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/explodinggradients/ragas",children:"GitHub - explodinggradients/ragas: Supercharge Your LLM Application Evaluations \ud83d\ude80"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.deepchecks.com/en/stable/tabular/auto_tutorials/quickstarts/plot_quick_model_evaluation.html",children:"Model Evaluation Suite Quickstart \u2014 Deepchecks Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/Arize-ai/phoenix",children:"GitHub - Arize-ai/phoenix: AI Observability & Evaluation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.trulens.org/",children:"TruLens for LLMs"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-aspects-of-llm-evaluation",children:"Key Aspects of LLM Evaluation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy and Quality"}),": Measures how well the model predicts or generates correct outputs for tasks like text generation, translation, or summarization."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bias and Fairness"}),": Assesses the presence of biases related to gender, race, or other demographics to ensure ethical AI."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": Tests the model\u2019s resilience to noisy or adversarial inputs, ensuring consistent performance across diverse contexts."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generalization"}),": Evaluates how well the model can adapt to new, unseen data outside its training set."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficiency"}),": Checks the computational cost, memory usage, and inference time, ensuring scalability."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interpretability"}),": Examines whether the model\u2019s decision-making process is understandable, allowing developers to trace and reason about its outputs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Alignment"}),": Verifies whether the model\u2019s outputs align with human values and expected behaviors, particularly important for applications like conversational AI."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"metrics",children:"Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Token Per Second (TPS)"}),"\n",(0,s.jsx)(n.li,{children:"Time to first token (TTFT)"}),"\n",(0,s.jsx)(n.li,{children:"GPU Usage"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"time-to-first-token-ttft",children:"Time to first token (TTFT)"}),"\n",(0,s.jsx)(n.p,{children:"It refers to the amount of time an LLM takes to generate the first token in its response after receiving an input or prompt. It is typically measured in seconds or milliseconds, and a lower TTFT indicates faster model responsiveness."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why TTFT Matters for Performance Benchmarking?"})}),"\n",(0,s.jsx)(n.p,{children:"TTFT is a key metric for understanding a model\u2019s responsiveness, especially when input complexity varies. It helps benchmark how efficiently the model handles different types of inputs."}),"\n",(0,s.jsx)(n.h3,{id:"token-per-second-tps",children:"Token Per Second (TPS)"}),"\n",(0,s.jsx)(n.p,{children:"TPS refers to the number of tokens\xa0that a LLM can generate or process in one second. A higher TPS indicates faster model responses."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"TPS is generally calculated using the formula:"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"TPS = (Input Tokens + Output Tokens) / Total Turnaround Time (TAT in seconds)"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This value represents the\xa0",(0,s.jsx)(n.strong,{children:"average TPS"}),", accounting for both the input and output tokens over the total time taken."]}),"\n",(0,s.jsxs)(n.p,{children:["However, it\u2019s also important to evaluate\xa0",(0,s.jsx)(n.strong,{children:"Output TPS"}),", which specifically measures how many tokens the model generates per second, independent of the input tokens."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Output TPS can be calculated as:"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Output TPS = Output Tokens / Time to Generate Output Tokens (TAT in seconds)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Output TPS is a more focused metric that excludes input token processing. It provides a clear measure of the model\u2019s raw generation speed, offering insights into how efficiently the model produces output regardless of the complexity or size of the input."}),"\n",(0,s.jsx)(n.h3,{id:"gpu-usage",children:"GPU Usage"}),"\n",(0,s.jsx)(n.p,{children:"GPU usage benchmarking is critical for two main reasons: it helps you avoid performance bottlenecks in production and ensures you\u2019re not overpaying for unused resources. Without proper GPU benchmarking, you might end up with a model that either crashes due to insufficient resources or wastes money on underutilized hardware."}),"\n",(0,s.jsx)(n.p,{children:"When measuring GPU usage, we look at two key metrics:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Volatile GPU Utilization (0\u2013100%) :"})}),"\n",(0,s.jsx)(n.p,{children:"This shows how hard your GPU is working during model inference. Think of it like your GPU\u2019s \u2018effort level\u2019 \u2014 it sits at 0% when idle and ramps up as the model generates output. We need to know:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"What percentage of GPU power the model typically uses"}),"\n",(0,s.jsx)(n.li,{children:"How utilization varies with different batch sizes and input lengths"}),"\n",(0,s.jsx)(n.li,{children:"Peak utilization during heavy loads"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"GPU Memory Usage:"})}),"\n",(0,s.jsx)(n.p,{children:"This tells us how much VRAM your model needs. It\u2019s crucial to measure:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Base memory required just to load the model (idle state)"}),"\n",(0,s.jsx)(n.li,{children:"Peak memory usage during generation"}),"\n",(0,s.jsx)(n.li,{children:"Memory patterns with different input sizes"}),"\n",(0,s.jsx)(n.li,{children:"How memory usage scales with batch processing"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Understanding both metrics helps you right-size your infrastructure and avoid nasty surprises in production like out-of-memory errors or performance degradation."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Using these metrics (TTFT, TPS, and GPU usage) together helps us make a smart comparison between different GPUs and infrastructure options. This way, we can pick the setup that gives us the best performance for our specific use case without overspending."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://rumn.medium.com/benchmarking-llm-performance-token-per-second-tps-time-to-first-token-ttft-and-gpu-usage-8c50ee8387fa",children:"Benchmarking LLMs: TPS, TTFT, GPU Usage | Medium"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.ibm.com/think/topics/ai-agent-evaluation",children:"What is AI Agent Evaluation? | IBM"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(296540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);