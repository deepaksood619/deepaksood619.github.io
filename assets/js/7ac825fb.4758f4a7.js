"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[46597],{267846:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"technologies/kafka/kafka-schema-registry-compatibility-rules","title":"Kafka Schema Registry Compatibility Rules","description":"The Schema Registry supports\xa0compatibility rules\xa0that help manage schema changes gracefully:","source":"@site/docs/technologies/kafka/kafka-schema-registry-compatibility-rules.md","sourceDirName":"technologies/kafka","slug":"/technologies/kafka/kafka-schema-registry-compatibility-rules","permalink":"/technologies/kafka/kafka-schema-registry-compatibility-rules","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/kafka/kafka-schema-registry-compatibility-rules.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1771180085000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"kafka-rest proxy","permalink":"/technologies/kafka/kafka-rest-proxy"},"next":{"title":"Kafka Schema Registry","permalink":"/technologies/kafka/kafka-schema-registry"}}');var s=t(474848),i=t(28453);const r={},o="Kafka Schema Registry Compatibility Rules",c={},l=[{value:"Compatibility Settings",id:"compatibility-settings",level:2},{value:"Prefer backward compatibility",id:"prefer-backward-compatibility",level:2},{value:"Understand compatibility groups",id:"understand-compatibility-groups",level:2},{value:"Use schema migration rules",id:"use-schema-migration-rules",level:2},{value:"Example Migration Rule",id:"example-migration-rule",level:3},{value:"Data Quality Rules",id:"data-quality-rules",level:2},{value:"Data Contracts",id:"data-contracts",level:2},{value:"Limitations",id:"limitations",level:3},{value:"Understanding the scope of a data contract",id:"understanding-the-scope-of-a-data-contract",level:3},{value:"Links",id:"links",level:2}];function d(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"kafka-schema-registry-compatibility-rules",children:"Kafka Schema Registry Compatibility Rules"})}),"\n",(0,s.jsxs)(a.p,{children:["The Schema Registry supports\xa0",(0,s.jsx)(a.strong,{children:"compatibility rules"}),"\xa0that help manage schema changes gracefully:"]}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Forward Compatibility"}),": Old consumers can read messages produced with newer schemas."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Backward Compatibility"}),": New consumers can read messages produced with older schemas."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Full Compatibility"}),": Both forward and backward compatibility are maintained."]}),"\n"]}),"\n",(0,s.jsxs)(a.p,{children:["These rules are critical for deciding how changes are rolled out. For instance, if you control the\xa0",(0,s.jsx)(a.strong,{children:"producers"}),"\xa0and update them first, you might choose\xa0",(0,s.jsx)(a.strong,{children:"forward compatibility"}),", consumers you don't necessarily have control over can be updated at a later date. If you are in control of the consumers instead,\xa0",(0,s.jsx)(a.strong,{children:"backward compatibility"}),"\xa0ensures they can still process older message versions, as producers are updated progressively (or maybe not at all) by other teams to use the newer schema."]}),"\n",(0,s.jsxs)(a.p,{children:["A forward compatibility check can also be thought of as a backward compatibility check with the arguments switched. This implies that a full compatibility check is both a backward compatibility check of the new schema against the old schema\xa0",(0,s.jsx)(a.strong,{children:"and"}),"\xa0a backward compatibility check of the old schema against the new schema."]}),"\n",(0,s.jsxs)(a.p,{children:["When registering a new schema version, compatibility checks can be performed against the previous version, or against all versions. In the latter case, the checks are said to be performed\xa0",(0,s.jsx)(a.em,{children:"transitively"}),"."]}),"\n",(0,s.jsxs)(a.p,{children:["The guarantee provided by a compatibility level can be thought of as a\xa0",(0,s.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Safety_and_liveness_properties",children:"safety property"}),". It tries to establish that nothing bad will happen to a client application, depending on how the schema (and therefore the data) evolves over time."]}),"\n",(0,s.jsxs)(a.p,{children:["Compatibility checks know nothing of what actual data exists in a system, so may appear more strict than necessary. For example, in JSON Schema, a schema that allows additional properties in the payload that are not defined in the schema is referred to as an\xa0",(0,s.jsx)(a.a,{href:"https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/",children:"open content model"}),". Adding a new property definition to an open content model is a backward incompatible change. That's because undefined properties that appear in old data may conflict with the new property definition, such as having a different type. However, you may not have any data with undefined properties. To get around this issue, the compatibility level can be temporarily set to NONE while the new schema is registered to the subject. Alternatively, compatibility groups can be used, which are explained later."]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"Schema Registry Compatibility Rules",src:t(588293).A+"",width:"1671",height:"549"})}),"\n",(0,s.jsx)(a.h2,{id:"compatibility-settings",children:"Compatibility Settings"}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Backward -"})," (default) Consumers using the new schema can read data written by producers using the latest registered schema."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Transitive backward -"})," Consumers using the new schema can read data written by producers using all previously registered schemas."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Forward -"})," Consumers using the latest registered schema can read data written by producers using the new schema."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Transitive forward -"})," Consumers using all the previously registered schemas can read data written by producers using the new schema."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Full -"})," The new schema is forward and backward compatible with the latest registered schema."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Transitive full -"})," The new schema is forward and backward compatible with all previously registered schemas."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"None -"})," Schema compatibility checks are disabled."]}),"\n"]}),"\n",(0,s.jsx)(a.h2,{id:"prefer-backward-compatibility",children:"Prefer backward compatibility"}),"\n",(0,s.jsx)(a.p,{children:"Backward compatibility, which is the default setting, should be preferred over the other compatibility levels. Backward compatibility allows a consumer to read old messages. Kafka Streams requires at least backward compatibility, because it may need to read old messages in the changelog topic."}),"\n",(0,s.jsx)(a.p,{children:"Full compatibility also allows reading older messages but can be overly restrictive, as a full compatibility check is a backward compatibility check of the new schema against the old schema, as well as the old schema against the new schema."}),"\n",(0,s.jsx)(a.p,{children:"Some argue that full compatibility should be preferred because it allows both new consumers to use old schemas (via backward compatibility) and old consumers to use new schemas (via forward compatibility). With backward compatibility, consumers need to be upgraded before producers, while with forward compatibility, producers need to be upgraded before consumers.\xa0 Full compatibility allows you to upgrade (or downgrade) consumers and producers independently in any order. However, the actual rules for full compatibility make it restrictive and difficult to use in practice, due to it being a backward compatibility check in both directions.\xa0\xa0"}),"\n",(0,s.jsx)(a.p,{children:"In general, working with backward compatibility is a more natural means of evolving a system.\xa0 When making backward compatible changes, we first upgrade consumers to handle the changes, then we upgrade producers. This allows both producers and consumers to evolve over time. When making forward-compatible changes, we first upgrade the producers, but we can choose to never upgrade consumers! This means that the only changes that we can make for forward compatibility are those that won't break existing consumers, which is fairly restrictive."}),"\n",(0,s.jsx)(a.p,{children:"A better alternative to the use of full compatibility is to use schema migration rules, as described later, which also allows new consumers to use old schemas, and old consumers to use new schemas. Schema migration rules also allow consumers and producers to be upgraded (or downgraded) independently. Furthermore, schema migration rules can handle many more scenarios than a full compatibility setting and were designed for arbitrarily complex schema evolutions, including ones that would normally be breaking changes."}),"\n",(0,s.jsxs)(a.p,{children:["If a schema evolves more than once, you can use\xa0",(0,s.jsx)(a.em,{children:"transitive"}),"\xa0backward compatibility to ensure that old messages corresponding to the different schemas can be read by the same consumer."]}),"\n",(0,s.jsx)(a.h2,{id:"understand-compatibility-groups",children:"Understand compatibility groups"}),"\n",(0,s.jsx)(a.p,{children:"Within a subject, typically each schema is compatible with the previous schema. If you want to introduce a breaking change, similar to bumping a major version when using semantic versioning, then you can add a metadata property to your schema as follows:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-json",children:'{\n  "schema": "...",\n  "metadata": {\n    "properties": {\n      "major_version": "2"\n    }\n  },\n  "ruleSet": ...\n}\n'})}),"\n",(0,s.jsx)(a.p,{children:"The name \u201cmajor_version\u201d above is arbitrary, you could have called it \u201capplication.major.version\u201d for example."}),"\n",(0,s.jsx)(a.p,{children:"You can then specify that a consumer uses only the latest schema of a specific major version."}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-json",children:"use.latest.with.metadata=major_version=2\nlatest.cache.ttl.sec=300\n"})}),"\n",(0,s.jsxs)(a.p,{children:["The above example also specifies that the client should check for a new latest version every five minutes. This TTL configuration can also be used with the\xa0",(0,s.jsx)(a.code,{children:"use.latest.version=true"}),"\xa0configuration."]}),"\n",(0,s.jsx)(a.p,{children:'Finally, we can configure Schema Registry to only perform compatibility checks for schemas that share the same compatibility group, where the schemas are partitioned by "major_version":'}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-json",children:'PUT /config/{subject}\n{\n  "compatibilityGroup": "major_version"\n}\n'})}),"\n",(0,s.jsx)(a.h2,{id:"use-schema-migration-rules",children:"Use schema migration rules"}),"\n",(0,s.jsxs)(a.p,{children:["Once a subject uses compatibility groups to accommodate breaking changes in the version history, we can add schema migration rules so that old consumers can read messages using new schemas, and new consumers can read messages using old schemas. As mentioned, schema migration rules can handle many more scenarios than a full compatibility setting. Below is a set of rules using\xa0",(0,s.jsx)(a.a,{href:"https://yokota.blog/2023/05/29/understanding-jsonata/",children:"JSONata"}),'\xa0to handle changing a "state" field to "status". The\xa0',(0,s.jsx)(a.code,{children:"UPGRADE"}),'\xa0rule allows new consumers to transform the "state" field to "status", while the\xa0',(0,s.jsx)(a.code,{children:"DOWNGRADE"}),'\xa0rule allows old consumers to transform the "status" field to "state". This means that both old and new producers can continue producing data, and both old and new consumers will see the data in their desired format. Furthermore, producers and consumers can be upgraded or downgraded independently.']}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-json",children:'{\n  "ruleSet": {\n    "domainRules": [\n      ...\n    ],\n    "migrationRules": [\n      {\n        "name": "changeStateToStatus",\n        "kind": "TRANSFORM",\n        "type": "JSONATA",\n        "mode": "UPGRADE",\n        "expr": "$merge([$sift($, function($v, $k) {$k != \'state\'}), {\'status\': $.\'state\'}])"\n      },\n      {\n        "name": "changeStatusToState",\n        "kind": "TRANSFORM",\n        "type": "JSONATA",\n        "mode": "DOWNGRADE",\n        "expr": "$merge([$sift($, function($v, $k) {$k != \'status\'}), {\'state\': $.\'status\'}])"\n      }\n    ]\n  }\n}\n'})}),"\n",(0,s.jsx)(a.p,{children:"The following video shows how an old producer and a new producer can both simultaneously interoperate with an old consumer and a new consumer, allowing producers and consumers to be both upgraded (or downgraded) independently, even with a normally incompatible change."}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://www.youtube.com/watch?v=RQeo6Y_7Ahc",children:"How to Evolve your Schemas with Migration Rules | Data Quality Rules - YouTube"})}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Migration Rules"}),"\n",(0,s.jsx)(a.li,{children:"Compatibility Groups"}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"example-migration-rule",children:"Example Migration Rule"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-json",children:'{\n "name": "add_fullname",\n "kind": "TRANSFORM",\n "type": "JSONATA",\n "mode": "UPGRADE",\n "expr": "$merge([$,{\'fullname\':firstname & \' \' & lastname}])"\n}\n'})}),"\n",(0,s.jsx)(a.h2,{id:"data-quality-rules",children:"Data Quality Rules"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["Domain Validation and Event-Condition-Action Rules","\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Validates and constraints the values of fields and customize follow-up action on incompatible messages"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(a.li,{children:["Tranformation Rules","\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Change the value of a specific field or an entire message (eg. encrypt sensitive fields)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(a.li,{children:["Complex Migration Rules","\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Evolve a schema in an incompatible manner by applying transformations when consuming from a topic to translate the topic data from the old format to the new format"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-json",children:'{\n "ruleSet": {\n   "domainRules": [\n\t {\n\t   "name": "checkSsnLen",\n\t   "kind": "CONDITION",\n\t   "type": "CEL",\n\t   "mode": "WRITE",\n\t   "expr": "message.ssn.matches(r\\"\\\\d{3}-\\\\d{2}-\\\\d{4}\\")",\n\t   "onFailure": "DLQ",\n\t   "params": {\n\t     "dlq.topic": "bad_memberships"\n\t   }\n\t }\n   ]\n }\n}\n'})}),"\n",(0,s.jsx)(a.h2,{id:"data-contracts",children:"Data Contracts"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://docs.confluent.io/cloud/current/sr/fundamentals/data-contracts.html",children:"Data Contracts for Schema Registry on Confluent Cloud | Confluent Documentation"})}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://www.youtube.com/watch?v=3BoljGZf5as",children:"How To Improve Data Quality With Domain Validation Rules | Data Quality Rules - YouTube"})}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsx)(a.p,{children:"Current limitations are:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Kafka Connect on Confluent Cloud does not support rules execution."}),"\n",(0,s.jsx)(a.li,{children:"Flink SQL and ksqlDB do not support rules execution in either Confluent Platform or Confluent Cloud."}),"\n",(0,s.jsx)(a.li,{children:"Confluent Control Center (Legacy) does not show the new properties for Data Contracts on the schema view page, in particular metadata and rules."}),"\n",(0,s.jsx)(a.li,{children:"Schema rules are only executed for the root schema, not referenced schemas. For example, given a schema named \u201cOrder\u201d that references another schema named \u201cProduct\u201d which has some rules attached to it, the serialization/deserialization of the \u201cOrder\u201d object will not execute the rules of the \u201cProduct\u201d schema."}),"\n",(0,s.jsx)(a.li,{children:"The non-Java clients (.NET, go, Python, JavaScript) do not yet support the DLQ Action."}),"\n",(0,s.jsx)(a.li,{children:"JavaScript and .NET do not support schema migration rules for Protobuf due to a limitation of the underlying third-party Protobuf libraries."}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"understanding-the-scope-of-a-data-contract",children:"Understanding the scope of a data contract"}),"\n",(0,s.jsx)(a.p,{children:"A\xa0data contract\xa0is a formal agreement between an upstream component and a downstream component on the structure and semantics of data that is in motion. A schema is only one element of a data contract. A data contract specifies and supports the following aspects of an agreement:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Structure."}),"\xa0This is the part of the contract that is covered by the schema, which defines the fields and their types."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Integrity constraints."}),"\xa0This includes declarative constraints or data quality rules on the domain values of fields, such as the constraint that an age must be a positive integer."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Metadata."}),"\xa0Metadata is additional information about the schema or its constituent parts, such as whether a field contains sensitive information. Metadata can also include documentation for a data contract, such as who created it."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Rules or policies."}),"\xa0These data rules or policies can enforce that a field that contains sensitive information must be encrypted, or that a message containing an invalid age must be sent to a dead letter queue."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Change or evolution."}),"\xa0This implies that data contracts are versioned, and can support declarative migration rules for how to transform data from one version to another, so that even changes that would normally break downstream components can be easily accommodated."]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Keeping in mind that a data contract is an agreement between an upstream component and a downstream component, note that:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"The upstream component enforces the data contract."}),"\n",(0,s.jsx)(a.li,{children:"The downstream component can assume that the data it receives conforms to the contract."}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Data contracts are important because they provide transparency over dependencies and data usage in a stream architecture. They also help to ensure the consistency, reliability, and quality of the data in motion."}),"\n",(0,s.jsx)(a.p,{children:"The upstream component could be a Apache Kafka\xae producer, while the downstream component would be the Kafka consumer. But the upstream component could also be a Kafka consumer, and the downstream component would be the application in which the Kafka consumer resides. This differentiation is important in schema evolution, where the producer may be using a newer version of the data contract, but the downstream application still expects an older version. In this case the data contract is used by the Kafka consumer to mediate between the Kafka producer and the downstream application, ensuring that the data received by the application matches the older version of the data contract, possibly using declarative transformation rules to massage the data into the desired form."}),"\n",(0,s.jsx)(a.h2,{id:"links",children:"Links"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://www.confluent.io/blog/best-practices-for-confluent-schema-registry/",children:"Schema Registry Best Practices"})})]})}function h(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},588293:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/Screenshot 2025-12-15 at 7.57.21 PM-de5eaa160bc743ebb5dd6fc89e2f30e9.png"},28453:(e,a,t)=>{t.d(a,{R:()=>r,x:()=>o});var n=t(296540);const s={},i=n.createContext(s);function r(e){const a=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(i.Provider,{value:a},e.children)}}}]);