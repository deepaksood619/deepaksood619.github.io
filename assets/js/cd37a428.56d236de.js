"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[72049],{751758:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>_});const s=JSON.parse('{"id":"technologies/apache-airflow/commands-configs","title":"Commands / Configs","description":"Config","source":"@site/docs/technologies/apache-airflow/commands-configs.md","sourceDirName":"technologies/apache-airflow","slug":"/technologies/apache-airflow/commands-configs","permalink":"/technologies/apache-airflow/commands-configs","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache-airflow/commands-configs.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1734022610000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Astronomer","permalink":"/technologies/apache-airflow/astronomer"},"next":{"title":"Concepts","permalink":"/technologies/apache-airflow/concepts"}}');var t=o(474848),a=o(28453);const r={},i="Commands / Configs",l={},_=[{value:"Config",id:"config",level:3},{value:"Deployments - Helm",id:"deployments---helm",level:2},{value:"Links",id:"links",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"commands--configs",children:"Commands / Configs"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"'resetdb', 'render', 'variables', 'delete_user', 'connections', 'create_user', 'rotate_fernet_key', 'pause', 'sync_perm', 'task_failed_deps', 'version', 'trigger_dag', 'initdb', 'test', 'unpause', 'list_dag_runs', 'dag_state', 'run', 'list_tasks', 'backfill', 'list_dags', 'kerberos', 'worker', 'webserver', 'flower', 'scheduler', 'task_state', 'pool', 'serve_logs', 'clear', 'list_users', 'next_execution', 'upgradedb', 'delete_dag'\n\n- airflow initdb\n- airflow flower\n- airflow webserver\n- airflow scheduler\n- airflow db clean --skip-archive --clean-before-timestamp '2024-05-14 00:00:00'\n\n# Configurations\nAIRFLOW_HOME: /root/example/Docker/airflow\nAIRFLOW__CORE__AIRFLOW_HOME: /root/example/Docker/airflow\nAIRFLOW__CORE__DAGS_FOLDER: /root/example/Docker/airflow/dags\nAIRFLOW__CORE__BASE_LOG_FOLDER: /root/example/Docker/airflow/logs\nAIRFLOW__CORE__EXECUTOR: CeleryExecutor\nAIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:airflow@apg-postgresql-headless.airflow:5432/airflow\n\nAIRFLOW__CORE__FERNET_KEY: 3Hj3xtOHEkyFySDDJC1dMkHi5L3QyeJNBbLdgzbs4Dg=\nAIRFLOW__CORE__TASK_RUNNER: StandardTaskRunner\nAIRFLOW__CORE__LOAD_EXAMPLES: \"False\"\nAIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Kolkata\n\nAIRFLOW__WEBSERVER__BASE_URL: http://airweb.abc.com\nAIRFLOW__WEBSERVER__EXPOSE_CONFIG: \"True\"\nAIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: \"120\"\nGUNICORN_CMD_ARGS: \"--log-level WARNING\"\nAIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=60 (default 0)# Prevent airflow from reloading the dags all the time and set. This is the main setting that reduces CPU load in the scheduler\nAIRFLOW__SCHEDULER__SCHEDULER_MAX_THREADS=1 # This should be set to (CPU Cores - 1)\nAIRFLOW__WEBSERVER__WORKERS=2 # 2 * NUM_CPU_CORES + 1\nAIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL=1800 # Restart workers every 30min instead of 30seconds\nAIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=300 #Kill workers if they don't start within 5min instead of 2min\n\nAIRFLOW__SMTP__SMTP_HOST: email-smtp.us-east-1.amazonaws.com\nAIRFLOW__SMTP__SMTP_PORT: \"587\"\nAIRFLOW__SMTP__SMTP_USER: AKIAUNOK5YRX3AMH3ZEV\nAIRFLOW__SMTP__SMTP_PASSWORD: BEvBirvENUT/mDTWCmnZLuiuaFqMnqeDwutK9VPLpKcI\nAIRFLOW__SMTP__SMTP_MAIL_FROM: devops@wattman.io\n\nAIRFLOW__CELERY__BROKER_URL: redis://:airflow@ars-redis-headless.airflow:6379/0\nAIRFLOW__CELERY__CELERY_RESULT_BACKEND: db+postgresql://postgres:airflow@apg-postgresql-headless.airflow:5432/airflow\n\nAIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: \"False\"\n\nAIRFLOW__CORE__PARALLELISM: 32 (default)\n\nAIRFLOW__SCHEDULER__USE_ROW_LEVEL_LOCKING: True\n"})}),"\n",(0,t.jsx)(n.h3,{id:"config",children:"Config"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Increase max_connections of postgres, default = 100"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"deployments---helm",children:"Deployments - Helm"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://airflow.apache.org/docs/helm-chart/stable/index.html",children:"Helm Chart for Apache Airflow \u2014 helm-chart Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/airflow-helm/charts",children:"GitHub - airflow-helm/charts: The User-Community Airflow Helm Chart is the standard way to deploy Apache Airflow on Kubernetes with Helm. Originally created in 2017, it has since helped thousands of companies create production-ready deployments of Airflow on Kubernetes."})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html",children:"Configuration Reference \u2014 Airflow Documentation"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>i});var s=o(296540);const t={},a=s.createContext(t);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);