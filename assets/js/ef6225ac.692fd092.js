"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[24185],{378047:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"ai/computer-vision-cv/others","title":"Others","description":"Locally Linear Embedding","source":"@site/docs/ai/computer-vision-cv/others.md","sourceDirName":"ai/computer-vision-cv","slug":"/ai/computer-vision-cv/others","permalink":"/ai/computer-vision-cv/others","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/computer-vision-cv/others.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1726516705000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Model Building Stages","permalink":"/ai/computer-vision-cv/model-building-stages"},"next":{"title":"Pre-Trained Models","permalink":"/ai/computer-vision-cv/pre-trained-models"}}');var s=t(474848),a=t(28453);const r={},o="Others",l={},d=[{value:"Viola-Jones object detection framework",id:"viola-jones-object-detection-framework",level:2},{value:"single-image super-resolution (SISR)",id:"single-image-super-resolution-sisr",level:2},{value:"Waifu2",id:"waifu2",level:2},{value:"Rasterization",id:"rasterization",level:2},{value:"Dithering",id:"dithering",level:2},{value:"Video Multimethod Assessment Fusion (VMAF)",id:"video-multimethod-assessment-fusion-vmaf",level:2},{value:"Datasets",id:"datasets",level:2},{value:"Vertex AI Matching Engine",id:"vertex-ai-matching-engine",level:2},{value:"Liveness detection",id:"liveness-detection",level:2},{value:"Border Detection / Segementation",id:"border-detection--segementation",level:2},{value:"Others",id:"others-1",level:2}];function h(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"others",children:"Others"})}),"\n",(0,s.jsx)(i.p,{children:"Locally Linear Embedding"}),"\n",(0,s.jsx)(i.p,{children:"EigenFace"}),"\n",(0,s.jsx)(i.h2,{id:"viola-jones-object-detection-framework",children:"Viola-Jones object detection framework"}),"\n",(0,s.jsxs)(i.p,{children:["The Viola-Jones object detection framework is the first ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Object_detection",children:"object detection"})," framework to provide competitive object detection rates in real-time proposed in 2001 by ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Paul_Viola",children:"Paul Viola"})," and ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Michael_Jones_(scientist)",children:"Michael Jones"}),"."]}),"\n",(0,s.jsx)(i.p,{children:"The characteristics of Viola-Jones algorithm which make it a good detection algorithm are:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Robust - very high detection rate (true-positive rate) & very low false-positive rate always."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Real time - For practical applications at least 2 frames per second must be processed."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Face detection only (not recognition) - The goal is to distinguish faces from non-faces (detection is the first step in the recognition process)."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The algorithm has four stages:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Haar Feature Selection"}),"\n",(0,s.jsx)(i.li,{children:"Creating an Integral Image"}),"\n",(0,s.jsx)(i.li,{children:"Adaboost Training"}),"\n",(0,s.jsx)(i.li,{children:"Cascading Classifiers"}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["The features sought by the detection framework universally involve the sums of image pixels within rectangular areas. As such, they bear some resemblance to ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Haar-like_feature",children:"Haar basis functions"}),", which have been used previously in the realm of image-based object detection. However, since the features used by Viola and Jones all rely on more than one rectangular area, they are generally more complex. The figure on the right illustrates the four different types of features used in the framework. The value of any given feature is the sum of the pixels within clear rectangles subtracted from the sum of the pixels within shaded rectangles. Rectangular features of this sort are primitive when compared to alternatives such as ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Steerable_filter",children:"steerable filters"}),". Although they are sensitive to vertical and horizontal features, their feedback is considerably coarser."]}),"\n",(0,s.jsx)(i.h2,{id:"single-image-super-resolution-sisr",children:"single-image super-resolution (SISR)"}),"\n",(0,s.jsx)(i.p,{children:"create a large-sized image from a low- resolution image"}),"\n",(0,s.jsx)(i.p,{children:"EnhanceNet-PAT does not attempt pixel-perfect reconstruction, but rather aims for faithful texture synthesis"}),"\n",(0,s.jsx)(i.p,{children:"By detecting and generating patterns in a low-resolution image and applying these patterns in the upsampling process, EnhanceNet-PAT adds extra pixels to the low-resolution image"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://webdav.tue.mpg.de/pixel/enhancenet",children:"https://webdav.tue.mpg.de/pixel/enhancenet"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://github.com/msmsajjadi/EnhanceNet-Code",children:"https://github.com/msmsajjadi/EnhanceNet-Code"})}),"\n",(0,s.jsx)(i.h2,{id:"waifu2",children:"Waifu2"}),"\n",(0,s.jsxs)(i.p,{children:["waifu2x is an ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Image_scaling",children:"image scaling"})," and ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Noise_reduction",children:"noise reduction"})," program for anime-style art and other types of photos."]}),"\n",(0,s.jsxs)(i.p,{children:["waifu2x was inspired by ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Super-resolution_imaging",children:"Super-Resolution"}),(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Convolutional_Neural_Network",children:"Convolutional Neural Network"}),"(SRCNN).It uses ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Nvidia",children:"Nvidia"}),(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/CUDA",children:"CUDA"})," for computing, although alternative implementations that allow for ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/OpenCL",children:"OpenCL"})," and ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Vulkan_(API)",children:"Vulkan"})," have been created."]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://github.com/nagadomi/waifu2x",children:"https://github.com/nagadomi/waifu2x"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Comparison_gallery_of_image_scaling_algorithms",children:"https://en.wikipedia.org/wiki/Comparison_gallery_of_image_scaling_algorithms"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://analyticsindiamag.com/extraction-of-aadhar-ids-using-opencv-tensorflow-sushil-ostwal-head-data-science-at-motilal-oswal-financial-services",children:"https://analyticsindiamag.com/extraction-of-aadhar-ids-using-opencv-tensorflow-sushil-ostwal-head-data-science-at-motilal-oswal-financial-services"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://towardsdatascience.com/deep-learning-based-super-resolution-with-opencv-4fd736678066",children:"https://towardsdatascience.com/deep-learning-based-super-resolution-with-opencv-4fd736678066"})}),"\n",(0,s.jsx)(i.h2,{id:"rasterization",children:"Rasterization"}),"\n",(0,s.jsx)(i.p,{children:"Rasterisation (or rasterization) is the task of taking an image described in a vector graphics format (shapes) and converting it into a raster image (pixels or dots) for output on a video display or printer, or for storage in a bitmap file format. It refers to both rasterisation of models and 2D rendering primitives such as polygons, line segments, etc."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Rasterisation",children:"https://en.wikipedia.org/wiki/Rasterisation"})}),"\n",(0,s.jsx)(i.h2,{id:"dithering",children:"Dithering"}),"\n",(0,s.jsx)(i.p,{children:"Remove fuzziness in medical or other images (uses MST)"}),"\n",(0,s.jsx)(i.h2,{id:"video-multimethod-assessment-fusion-vmaf",children:"Video Multimethod Assessment Fusion (VMAF)"}),"\n",(0,s.jsxs)(i.p,{children:["Video Multimethod Assessment Fusion (VMAF)is an objective full-reference ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Video_quality",children:"video quality"})," metric developed by ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Netflix",children:"Netflix"})," in cooperation with the ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/University_of_Southern_California",children:"University of Southern California"})," and the Laboratory for Image and Video Engineering (LIVE) at ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/The_University_of_Texas_at_Austin",children:"The University of Texas at Austin"}),". It predicts subjective video quality based on a reference and distorted video sequence. The metric can be used to evaluate the quality of different ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Video_codec",children:"video codecs"}),", encoders, encoding settings, or transmission variants."]}),"\n",(0,s.jsx)(i.h2,{id:"datasets",children:"Datasets"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"ImageNet"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://image-net.org",children:"https://image-net.org"})}),"\n",(0,s.jsx)(i.h2,{id:"vertex-ai-matching-engine",children:"Vertex AI Matching Engine"}),"\n",(0,s.jsx)(i.p,{children:"Vertex AI Matching Engine provides the industry's leading high scale, low latency, vector-similarity matching (also known as approximate nearest neighbor) service, and industry-leading algorithms to train semantic embeddings for similarity-matching use cases."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://cloud.google.com/vertex-ai/docs/matching-engine/overview",children:"https://cloud.google.com/vertex-ai/docs/matching-engine/overview"})}),"\n",(0,s.jsx)(i.h2,{id:"liveness-detection",children:"Liveness detection"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://towardsdatascience.com/implementing-liveness-detection-with-google-ml-kit-5e8c9f6dba45",children:"https://towardsdatascience.com/implementing-liveness-detection-with-google-ml-kit-5e8c9f6dba45"})}),"\n",(0,s.jsx)(i.h2,{id:"border-detection--segementation",children:"Border Detection / Segementation"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://maxhalford.github.io/blog/comic-book-panel-segmentation/",children:"Comic book panel segmentation \u2022 Max Halford"})}),"\n",(0,s.jsx)(i.h2,{id:"others-1",children:"Others"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://github.com/imazen/imageflow",children:"https://github.com/imazen/imageflow"})})]})}function c(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},28453:(e,i,t)=>{t.d(i,{R:()=>r,x:()=>o});var n=t(296540);const s={},a=n.createContext(s);function r(e){const i=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(a.Provider,{value:i},e.children)}}}]);