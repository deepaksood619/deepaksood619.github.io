"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[13565],{802507:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var i=t(785893),n=t(511151);const s={},r="Others",o={id:"technologies/apache/apache-spark/99-others",title:"Others",description:"Data Types",source:"@site/docs/technologies/apache/apache-spark/99-others.md",sourceDirName:"technologies/apache/apache-spark",slug:"/technologies/apache/apache-spark/99-others",permalink:"/technologies/apache/apache-spark/99-others",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache/apache-spark/99-others.md",tags:[],version:"current",lastUpdatedAt:1707413368,formattedLastUpdatedAt:"Feb 8, 2024",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Built-In Functions",permalink:"/technologies/apache/apache-spark/11-built-in-functions"},next:{title:"Apache Storm",permalink:"/technologies/apache/apache-storm"}},l={},c=[{value:"Data Types",id:"data-types",level:2},{value:"DecimalType()",id:"decimaltype",level:3},{value:"Optimization",id:"optimization",level:2},{value:"Bucketing",id:"bucketing",level:2},{value:"Shuffling",id:"shuffling",level:2},{value:"SparkML",id:"sparkml",level:2}];function h(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,n.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.h1,{id:"others",children:"Others"}),"\n",(0,i.jsx)(a.h2,{id:"data-types",children:"Data Types"}),"\n",(0,i.jsx)(a.h3,{id:"decimaltype",children:"DecimalType()"}),"\n",(0,i.jsxs)(a.p,{children:["Represents arbitrary-precision signed decimal numbers. Backed internally by ",(0,i.jsx)(a.code,{children:"java.math.BigDecimal"}),". A ",(0,i.jsx)(a.code,{children:"BigDecimal"})," consists of an arbitrary precision integer ",(0,i.jsx)(a.strong,{children:"unscaled value"})," and a 32-bit ",(0,i.jsx)(a.strong,{children:"integer scale"}),"."]}),"\n",(0,i.jsx)(a.p,{children:"But what it really means? Let\u2019s break it down:"}),"\n",(0,i.jsx)(a.p,{children:"DecimalType() stores two operands (Precision and Scale), this way avoids storing trailing zeros."}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Precision - Number of digits in the Unscaled value"}),"\n",(0,i.jsx)(a.li,{children:"Unscaled value - Value without the floating-point (i.e 4.33 the unscaled value would be 433)"}),"\n",(0,i.jsx)(a.li,{children:"Scale - Number of digits to the right of the decimal point ( i.e 4.33 the scale is 2)"}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://medium.com/bild-journal/pyspark-data-types-explained-feb5e6f83c43",children:"Pyspark Data Types - Explained. The ins and outs-Data types\u2026 | by Diogo Veloso | BiLD Journal | Medium"})}),"\n",(0,i.jsx)(a.h2,{id:"optimization",children:"Optimization"}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html#aws-glue-programming-etl-partitions-writing",children:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html#aws-glue-programming-etl-partitions-writing"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://towardsdatascience.com/apache-spark-optimization-toolkit-17cf3e491992",children:"https://towardsdatascience.com/apache-spark-optimization-toolkit-17cf3e491992"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://github.com/aws-samples/aws-glue-samples/blob/master/examples/join_and_relationalize",children:"https://github.com/aws-samples/aws-glue-samples/blob/master/examples/join_and_relationalize"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://thedataguy.in/aws-glue-custom-output-file-size-and-fixed-number-of-files",children:"https://thedataguy.in/aws-glue-custom-output-file-size-and-fixed-number-of-files"})}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Option 1: groupFiles"}),"\n",(0,i.jsx)(a.li,{children:"Option 2: groupFiles while reading from S3"}),"\n",(0,i.jsx)(a.li,{children:"Option 3: Repartition"}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://medium.com/enigma-engineering/things-i-wish-id-known-about-spark-when-i-started-one-year-later-edition-d767430181ed",children:"https://medium.com/enigma-engineering/things-i-wish-id-known-about-spark-when-i-started-one-year-later-edition-d767430181ed"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://spark.apache.org/docs/latest/sql-performance-tuning.html",children:"Performance Tuning - Spark 3.3.2 Documentation"})}),"\n",(0,i.jsx)(a.h2,{id:"bucketing",children:"Bucketing"}),"\n",(0,i.jsx)(a.p,{children:"Bucketing is an optimization technique in Apache Spark SQL. Data is allocated among a specified number of buckets, according to values derived from one or more bucketing columns. Bucketing improves performance by shuffling and sorting data prior to downstream operations such as table joins. The tradeoff is the initial overhead due to shuffling and sorting, but for certain data transformations, this technique can improve performance by avoiding later shuffling and sorting."}),"\n",(0,i.jsx)(a.p,{children:"This technique is useful for dimension tables, which are frequently used tables containing primary keys. It is also useful when there are frequent join operations involving large and small tables."}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53",children:"Best Practices for Bucketing in Spark SQL | by David Vrba | Towards Data Science"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://www.clairvoyant.ai/blog/bucketing-in-spark",children:"Bucketing in Spark"})}),"\n",(0,i.jsx)(a.h2,{id:"shuffling",children:"Shuffling"}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Apache Spark processes queries by distributing data over multiple nodes and calculating the values separately on every node."}),"\xa0However, occasionally,\xa0",(0,i.jsx)(a.strong,{children:"the nodes need to exchange the data"}),". After all, that\u2019s the purpose of Spark - processing data that doesn\u2019t fit on a single machine."]}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Shuffling is the process of exchanging data between partitions"}),". As a result, data rows can move between worker nodes when their source partition and the target partition reside on a different machine."]}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://www.mikulskibartosz.name/shuffling-in-apache-spark/",children:"What is shuffling in Apache Spark, and when does it happen? | Bartosz Mikulski"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://www.youtube.com/watch?v=ffHboqNoW_A",children:"Spark Basics | Shuffling - YouTube"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://sparkbyexamples.com/spark/spark-shuffle-partitions/?expand_article=1",children:"Spark SQL Shuffle Partitions - Spark By Examples"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://www.linkedin.com/pulse/apache-spark-shuffle-akhil-pathirippilly-mana/",children:"Apache Spark : The Shuffle"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://www.youtube.com/watch?v=Kmb_pm8AQCE",children:"35. Databricks & Spark: Interview Question - Shuffle Partition - YouTube"})}),"\n",(0,i.jsx)(a.h2,{id:"sparkml",children:"SparkML"}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://spark.apache.org/docs/latest/ml-pipeline.html",children:"https://spark.apache.org/docs/latest/ml-pipeline.html"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.a,{href:"https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427",children:"https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427"})})]})}function p(e={}){const{wrapper:a}={...(0,n.a)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},511151:(e,a,t)=>{t.d(a,{Z:()=>o,a:()=>r});var i=t(667294);const n={},s=i.createContext(n);function r(e){const a=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),i.createElement(s.Provider,{value:a},e.children)}}}]);