"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[43285],{833569:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"ai/llm/llm-tuning","title":"LLM Tuning","description":"The process of adapting a model to a new domain or set of custom use cases by training the model on new data","source":"@site/docs/ai/llm/llm-tuning.md","sourceDirName":"ai/llm","slug":"/ai/llm/llm-tuning","permalink":"/ai/llm/llm-tuning","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/llm-tuning.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1745136565000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LLM Building","permalink":"/ai/llm/llm-building"},"next":{"title":"Models","permalink":"/ai/llm/models"}}');var r=i(474848),s=i(28453);const a={},l="LLM Tuning",o={},d=[{value:"Fine Tuning",id:"fine-tuning",level:2},{value:"LoRA",id:"lora",level:3},{value:"LoRA-FA (Frozen-A)",id:"lora-fa-frozen-a",level:3},{value:"VeRA",id:"vera",level:3},{value:"Delta-LoRA",id:"delta-lora",level:3},{value:"LoRA+",id:"lora-1",level:3},{value:"Supervised fine-tuning (SFT)",id:"supervised-fine-tuning-sft",level:2},{value:"Methods for fine-tuning LLMs",id:"methods-for-fine-tuning-llms",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GitHub - microsoft/BitNet: Official inference framework for 1-bit LLMs",id:"github---microsoftbitnet-official-inference-framework-for-1-bit-llms",level:3},{value:"Links",id:"links",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"llm-tuning",children:"LLM Tuning"})}),"\n",(0,r.jsx)(n.p,{children:"The process of adapting a model to a new domain or set of custom use cases by training the model on new data"}),"\n",(0,r.jsx)(n.h2,{id:"fine-tuning",children:"Fine Tuning"}),"\n",(0,r.jsx)(n.p,{children:"Large language model (LLM) fine-tuning is the process of taking pre-trained models and further training them on smaller, specific datasets to refine their capabilities and improve performance in a particular task or domain. Fine-tuning is about turning general-purpose models and turning them into specialized models. It bridges the gap between generic pre-trained models and the unique requirements of specific applications, ensuring that the language model aligns closely with human expectations."}),"\n",(0,r.jsx)(n.h3,{id:"lora",children:"LoRA"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Introduce two low-rank matrices, A and B, to work alongside the weight matrix W."}),"\n",(0,r.jsx)(n.li,{children:"Adjust these matrices instead of the behemoth W, making updates manageable."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lora-fa-frozen-a",children:"LoRA-FA (Frozen-A)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Takes LoRA a step further by freezing matrix A."}),"\n",(0,r.jsx)(n.li,{children:"Only matrix B is tweaked, reducing the activation memory needed."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vera",children:"VeRA"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"All about efficiency: matrices A and B are fixed and shared across all layers."}),"\n",(0,r.jsx)(n.li,{children:"Focuses on tiny, trainable scaling vectors in each layer, making it super memory-friendly."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"delta-lora",children:"Delta-LoRA"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A twist on LoRA: adds the difference (delta) between products of matrices A and B across training steps to the main weight matrix W."}),"\n",(0,r.jsx)(n.li,{children:"Offers a dynamic yet controlled approach to parameter updates."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lora-1",children:"LoRA+"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"An optimized variant of LoRA where matrix B gets a higher learning rate. This tweak leads to faster and more effective learning."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"5 Techniques to fine-tune LLMs",src:i(900517).A+"",width:"990",height:"1154"}),"\ntrr"]}),"\n",(0,r.jsx)(n.h2,{id:"supervised-fine-tuning-sft",children:"Supervised fine-tuning (SFT)"}),"\n",(0,r.jsx)(n.p,{children:"Supervised fine-tuning means updating a pre-trained language model using labeled data to do a specific task. The data used has been checked earlier. This is different from unsupervised methods, where data isn't checked. Usually, the initial training of the language model is unsupervised, but fine-tuning is supervised."}),"\n",(0,r.jsx)(n.h2,{id:"methods-for-fine-tuning-llms",children:"Methods for fine-tuning LLMs"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Instruction fine-tuning"}),"\n",(0,r.jsx)(n.li,{children:"Full fine-tuning"}),"\n",(0,r.jsxs)(n.li,{children:["Parameter-efficient fine-tuning (PEFT)","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=HcVtpLAGMXo",children:"LLM (Parameter Efficient) Fine Tuning - Explained! - YouTube"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"github---microsoftbitnet-official-inference-framework-for-1-bit-llms",children:(0,r.jsx)(n.a,{href:"https://github.com/microsoft/BitNet",children:"GitHub - microsoft/BitNet: Official inference framework for 1-bit LLMs"})}),"\n",(0,r.jsxs)(n.p,{children:["bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support\xa0",(0,r.jsx)(n.strong,{children:"fast"}),"\xa0and\xa0",(0,r.jsx)(n.strong,{children:"lossless"}),"\xa0inference of 1.58-bit models on CPU (with NPU and GPU support coming next)."]}),"\n",(0,r.jsxs)(n.p,{children:["The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of\xa0",(0,r.jsx)(n.strong,{children:"1.37x"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"5.07x"}),"\xa0on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by\xa0",(0,r.jsx)(n.strong,{children:"55.4%"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"70.0%"}),", further boosting overall efficiency. On x86 CPUs, speedups range from\xa0",(0,r.jsx)(n.strong,{children:"2.37x"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"6.17x"}),"\xa0with energy reductions between\xa0",(0,r.jsx)(n.strong,{children:"71.9%"}),"\xa0to\xa0",(0,r.jsx)(n.strong,{children:"82.2%"}),". Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.reddit.com/r/LocalLLaMA/comments/1g6jmwl/bitnet_inference_framework_for_1bit_llms/",children:"BitNet - Inference framework for 1-bit LLMs : r/LocalLLaMA"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://medium.com/@kldurga999/bitnet-cpp-an-opensource-llm-platform-by-microsoft-8cdeccf272c2",children:"Bitnet.cpp an opensource LLM platform by Microsoft | by Kldurga | Medium"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=C4OYJAs4O60",children:"bitnet.cpp from Microsoft: Run LLMs locally on CPU! (hands-on) - YouTube"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2504.12285",children:"[2504.12285] BitNet b1.58 2B4T Technical Report"})}),"\n",(0,r.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.superannotate.com/blog/llm-fine-tuning",children:"Fine-tuning large language models (LLMs) in 2024 | SuperAnnotate"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://mlops.substack.com/p/pruning-aware-trainingpat-in-llms",children:"Pruning Aware Training(PAT) in LLMs - by Bugra Akyildiz"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=t-0s_2uZZU0",children:"Generative AI Fine Tuning LLM Models Crash Course - YouTube"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},900517:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/Screenshot 2025-03-10 at 12.37.42 PM-b102673bba25e478b543d310f29fa379.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var t=i(296540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);