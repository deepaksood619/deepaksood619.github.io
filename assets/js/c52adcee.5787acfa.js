"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[26396],{95629:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var a=t(785893),i=t(511151);const s={},r="Amazon Athena",o={id:"cloud/aws/analytics/amazon-athena",title:"Amazon Athena",description:"Based on Presto",source:"@site/docs/cloud/aws/analytics/amazon-athena.md",sourceDirName:"cloud/aws/analytics",slug:"/cloud/aws/analytics/amazon-athena",permalink:"/cloud/aws/analytics/amazon-athena",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/cloud/aws/analytics/amazon-athena.md",tags:[],version:"current",lastUpdatedAt:1723484180,formattedLastUpdatedAt:"Aug 12, 2024",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Analytics",permalink:"/cloud/aws/analytics/"},next:{title:"Amazon DevOps Guru",permalink:"/cloud/aws/analytics/amazon-devops-guru"}},d={},c=[{value:"When should I use Athena",id:"when-should-i-use-athena",level:2},{value:"Partitioning Data",id:"partitioning-data",level:2},{value:"Note",id:"note",level:2},{value:"Key Points",id:"key-points",level:2},{value:"Athena iceberg",id:"athena-iceberg",level:2},{value:"Query",id:"query",level:2},{value:"Things to remember",id:"things-to-remember",level:2},{value:"Performance tuning guides",id:"performance-tuning-guides",level:2},{value:"Connectors",id:"connectors",level:2},{value:"CSV to bulk import",id:"csv-to-bulk-import",level:2},{value:"Debugging",id:"debugging",level:2},{value:"Links",id:"links",level:2},{value:"GitHub - quux00/hive-json-schema: Tool to generate a Hive schema from a JSON example doc",id:"github---quux00hive-json-schema-tool-to-generate-a-hive-schema-from-a-json-example-doc",level:3}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"amazon-athena",children:"Amazon Athena"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Based on Presto"})}),"\n",(0,a.jsx)(n.p,{children:"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run."}),"\n",(0,a.jsx)(n.p,{children:"Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there's no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets."}),"\n",(0,a.jsxs)(n.p,{children:["Athena is out-of-the-box integrated with ",(0,a.jsx)(n.a,{href:"https://aws.amazon.com/glue/",children:"AWS Glue"})," Data Catalog, allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/athena",children:"https://aws.amazon.com/athena"})}),"\n",(0,a.jsx)(n.h2,{id:"when-should-i-use-athena",children:"When should I use Athena"}),"\n",(0,a.jsx)(n.p,{children:"Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena."}),"\n",(0,a.jsxs)(n.p,{children:["Athena integrates with Amazon QuickSight for easy data visualization. You can use Athena to generate reports or to explore data with business intelligence tools or SQL clients connected with a JDBC or an ODBC driver. For more information, see ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/quicksight/latest/user/welcome.html",children:"What is Amazon QuickSight"})," in theAmazon QuickSight User Guideand ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/athena-bi-tools-jdbc-odbc.html",children:"Connecting to Amazon Athena with ODBC and JDBC Drivers"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Athena integrates with the AWS Glue Data Catalog, which offers a persistent metadata store for your data in Amazon S3. This allows you to create tables and query data in Athena based on a central metadata store available throughout your AWS account and integrated with the ETL and data discovery features of AWS Glue. For more information, see ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html",children:"Integration with AWS Glue"})," and ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html",children:"What is AWS Glue"})," in theAWS Glue Developer Guide."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html",children:"https://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html"})}),"\n",(0,a.jsx)(n.h2,{id:"partitioning-data",children:"Partitioning Data"}),"\n",(0,a.jsxs)(n.p,{children:["By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. Athena leverages Hive for ",(0,a.jsx)(n.a,{href:"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterPartition",children:"partitioning"})," data. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but loaded one time per day, may partition by a data source identifier and date."]}),"\n",(0,a.jsx)(n.p,{children:"If you issue queries against Amazon S3 buckets with a large number of objects and the data is not partitioned, such queries may affect the Get request rate limits in Amazon S3 and lead to Amazon S3 exceptions. To prevent errors, partition your data. Additionally, consider tuning your Amazon S3 request rates."}),"\n",(0,a.jsx)(n.h2,{id:"note",children:"Note"}),"\n",(0,a.jsxs)(n.p,{children:["If you query a partitioned table and specify the partition in theWHEREclause, Athena scans the data only from that partition. For more information, see ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/tables-location-format.html#table-location-and-partitions",children:"Table Location and Partitions"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"To create a table with partitions, you must define it during theCREATE TABLEstatement. UsePARTITIONED BYto define the keys by which to partition data. There are two scenarios discussed in the following sections:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Data is already partitioned, stored on Amazon S3, and you need to access the data on Athena."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Data is not partitioned."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/partitions.html",children:"https://docs.aws.amazon.com/athena/latest/ug/partitions.html"})}),"\n",(0,a.jsx)(n.p,{children:"When you create a table from CSV data in Athena, determine what types of values it contains:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:['If data contains values enclosed in double quotes ("), you can use the ',(0,a.jsx)(n.a,{href:"https://cwiki.apache.org/confluence/display/Hive/CSV+Serde",children:"OpenCSV SerDe"})," to deserialize the values in Athena. In the following sections, note the behavior of this SerDe withSTRINGdata types."]}),"\n",(0,a.jsxs)(n.li,{children:['If data does not contain values enclosed in double quotes ("), you can omit specifying any SerDe. In this case, Athena uses the defaultLazySimpleSerDe. For information, see ',(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/lazy-simple-serde.html",children:"LazySimpleSerDe for CSV, TSV, and Custom-Delimited Files"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-points",children:"Key Points"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Does not support embedded line breaks in CSV files."}),"\n",(0,a.jsx)(n.li,{children:"Does not support empty fields in columns defined as a numeric data type."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["ACID - ",(0,a.jsx)(n.a,{href:"https://aws.amazon.com/about-aws/whats-new/2022/04/amazon-athena-acid-transactions-powered-apache-iceberg",children:"https://aws.amazon.com/about-aws/whats-new/2022/04/amazon-athena-acid-transactions-powered-apache-iceberg"})]}),"\n",(0,a.jsx)(n.h2,{id:"athena-iceberg",children:"Athena iceberg"}),"\n",(0,a.jsx)(n.p,{children:"Athena supports read, time travel, write, and DDL queries for Apache Iceberg tables that use the Apache Parquet format for data and the AWS Glue catalog for their metastore."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://iceberg.apache.org/",children:"Apache Iceberg"})," is an open table format for very large analytic datasets. Iceberg manages large collections of files as tables, and it supports modern analytical data lake operations such as record-level insert, update, delete, and time travel queries. The Iceberg specification allows seamless table evolution such as schema and partition evolution, and its design is optimized for usage on Amazon S3. Iceberg also helps guarantee data correctness under concurrent write scenarios."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html",children:"https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/blogs/big-data/build-a-serverless-transactional-data-lake-with-apache-iceberg-amazon-emr-serverless-and-amazon-athena/",children:"Build a serverless transactional data lake with Apache Iceberg, Amazon EMR Serverless, and Amazon Athena | AWS Big Data Blog"})}),"\n",(0,a.jsx)(n.h2,{id:"query",children:"Query"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/functions-operators-reference-section.html",children:"https://docs.aws.amazon.com/athena/latest/ug/functions-operators-reference-section.html"})}),"\n",(0,a.jsxs)(n.p,{children:["Use ",(0,a.jsx)(n.strong,{children:"approx distinct count"})," whenever you don't have to have an exact count. so replace your distinct count(distinct x) with approx_distinct(x). This is a big one (exact distinct count causes your query to run on a single Presto node. This function has a standard error of 2.3% (you can actually customize this with approx_distinct(x, e))"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"SELECT count(*) AS TOTALNUMBEROFTABLES\n    FROM INFORMATION_SCHEMA.TABLES\n    WHERE TABLE_SCHEMA = 's3_db';\n\nSELECT device_id, message.sendernumber,message.message,message.dateTime, udsms, partition_0, partition_1,partition_2\nFROM \"s3_db\".\"folder_0d23a883b4a272dd4892202efbac8b59\"\nCROSS JOIN UNNEST(udsms) as t(message)\nwhere device_id = '1f385b6e17c395d0';\n\nExample Queries\n    select * FROM \"pinpointanalytics\".\"emailanalyticsfinal\" where  event_type in ('email.open',\n    '_email.softbounce',\n    '_email.send',\n    '_email.delivered',\n    '_email.rendering_failure',\n    '_email.click',\n    '_email.hardbounce')\n    and  year = '2021'\n    AND month = '11'\n    AND day >= '08'\n\n    Based event type aggregation query -\n        SELECT event_type,count(*) FROM \"pinpointanalytics\".\"emailanalytics\" where attributes.campaign_id = '33d07428932540adafcfd0679558957e' group by event_type\n\n    Detailed info on hard bounce\n        SELECT facets.email_channel.mail_event.mail.destination,facets.email_channel.mail_event.bounce.bounce_sub_type FROM \"pinpointanalytics\".\"emailanalytics\" where attributes.campaign_id = '33d07428932540adafcfd0679558957e' and event_type = '_email.hardbounce'\n\n    Detailed info on email click.\n        SELECT facets.email_channel.mail_event.mail.destination[0],facets.email_channel.mail_event.click.link FROM \"pinpointanalytics\".\"emailanalytics\" where attributes.campaign_id = '33d07428932540adafcfd0679558957e' and event_type = '_email.click'\n\n    Differen sub status on campaign send\n        SELECT attributes.campaign_send_status,count(*) FROM \"pinpointanalytics\".\"emailanalytics\" where attributes.campaign_id = '33d07428932540adafcfd0679558957e' and event_type = '_campaign.send' group by attributes.campaign_send_status\n\n    Email Open query\n        SELECT facets.email_channel.mail_event.mail.destination[0] FROM \"pinpointanalytics\".\"emailanalytics\" where attributes.campaign_id = '33d07428932540adafcfd0679558957e' and event_type = '_email.open'\n\n-- SMS Analytics\n        SELECT event_type, count(*)\nFROM \"pinpointanalytics\".\"emailanalyticsfinal\"\nWHERE year = '2021'\n    AND month = '08'\n    AND day = '22'\n    AND event_type in ('_SMS.FAILURE', '_SMS.SUCCESS')\nGROUP BY event_type;\n\n        SELECT attributes['record_status'], count(*)\nFROM \"pinpointanalytics\".\"emailanalyticsfinal\"\nWHERE year = '2021'\n    AND month = '08'\n    AND day = '22'\n    AND event_type in ('_SMS.FAILURE', '_SMS.SUCCESS')\nGROUP BY attributes['record_status']\n\n-- sms clicked\nSELECT attributes['customer_id'], count(*)\nFROM \"pinpointanalytics\".\"emailanalyticsfinal\"\nWHERE year = '2021'\n        AND month = '11'\n        AND day = '22'\n        AND attributes['template_id'] = 'razorpay_link'\n        AND event_type= '_SMS.CLICKED'\n    group by attributes['customer_id'];\n\nSELECT event_type, count(*)\nFROM \"pinpointanalytics\".\"emailanalyticsfinal\"\nWHERE year = '2021'\n        AND month >= '09'\n        AND day = '24'\n        AND event_type in ('_SMS.FAILURE', '_SMS.SUCCESS')\nGROUP BY event_type;\n\nSELECT attributes['record_status'], count(*)\nFROM \"pinpointanalytics\".\"emailanalyticsfinal\"\nWHERE year = '2021'\n        AND month >= '09'\n        AND day = '24'\n        AND event_type in ('_SMS.FAILURE', '_SMS.SUCCESS')\nGROUP BY attributes['record_status'];\n\nselect * FROM \"pinpointanalytics\".\"emailanalyticsfinal\" where event_type in ('_SMS.FAILURE') and json_extract_scalar(attributes['customer_context'], '$.customer_id') ='4547012' limit 5\n\nselect * FROM \"pinpointanalytics\".\"emailanalyticsfinal\" where json_extract_scalar(attributes['customer_context'], '$.customer_id') ='4990002' limit 5\n\nCREATE EXTERNAL TABLE IF NOT EXISTS join_test (\n    customer_id int,\n    occupation string,\n    salary int,\n    Current_City_Name string,\n    Current_State_Name string,\n    is_operational int,\n    is_elevate_operational int )\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nLOCATION 's3://example-migration-data/rds/equifax_raw_response/join_test_1000';\n\nCREATE EXTERNAL TABLE IF NOT EXISTS user_device_sms (\n    id int,\n    customer_id int,\n        occupation string,\n        sender string,\n        message string,\n        message_type string,\n        sms_time date,\n        create_date date,\n    device_id string)\n    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nLOCATION 's3://example-migration-data/rds/user_device_sms/user_device_sms.part_1000';\n\nCREATE EXTERNAL TABLE `user_device_sms`(\n`col0` bigint,\n`col1` bigint,\n`col2` string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n's3://example-migration-data/rds/user_device_sms/user_device_sms.part_00000'\n\nCREATE EXTERNAL TABLE IF NOT EXISTS `s3_db.st_bank_sms`(\n`avlbal` double,\n`amount` double,\n`trns_type` string,\n`hash_key` string,\n`sender` string,\n`created_at` timestamp,\n`id` int,\n`customer_id` int,\n`trns_mode` string,\n`sms_time` string)\nROW FORMAT SERDE\n'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n's3://example-migration-data/glue/st_bank_sms'\nTBLPROPERTIES (\n'has_encrypted_data'='false');\n\nCREATE EXTERNAL TABLE IF NOT EXISTS `s3_db.st_bank_sms`(\n    `avlbal` double,\n    `amount` double,\n    `trns_type` string,\n    `hash_key` string,\n    `sender` string,\n    `created_at` timestamp,\n    `id` int,\n    `customer_id` int,\n    `trns_mode` string,\n    `sms_time` string)\nROW FORMAT SERDE\n    'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT\n    'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n    's3://example/example/folder';\n\nCREATE EXTERNAL TABLE IF NOT EXISTS s3_db.test (\n    `id` string,\n    `customer_id` string,\n    `sender` string,\n    `message` string,\n    `message_type` string,\n    `sms_time` date,\n    `create_date` date,\n    `device_id` string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nWITH SERDEPROPERTIES (\n    'serialization.format' = ',',\n    'field.delim' = '|||'\n) LOCATION 's3://example/example/folder/'\nTBLPROPERTIES ('has_encrypted_data'='false');\n\nCREATE EXTERNAL TABLE `parquet`(\n`customer_id` bigint,\n`loanid` bigint,\n`@amount` string,\n`@balance` string,\n`@category` string,\n`@chqno` string,\n`@date` string,\n`@narration` string,\n`__index_level_0__` bigint)\nROW FORMAT SERDE\n'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nLOCATION\n's3://example-migration-data/parquet/'\nTBLPROPERTIES (\n'CrawlerSchemaDeserializerVersion'='1.0',\n'CrawlerSchemaSerializerVersion'='1.0',\n'UPDATED_BY_CRAWLER'='parquet',\n'averageRecordSize'='40',\n'classification'='parquet',\n'compressionType'='none',\n'objectCount'='1',\n'recordCount'='56464',\n'sizeKey'='2301512',\n'typeOfData'='file')\n\nCREATE EXTERNAL TABLE IF NOT EXISTS `s3_db.example_account_transactions`(\n`date` date,\n`example_account_id` int,\n`amount` double,\n`chqno` string,\n`second_category` string,\n`remark` string,\n`example_count_id` int,\n`balance` double,\n`narration` string,\n`id` int,\n`customer_id` int,\n`category` string,\n`loan_id` int)\nROW FORMAT SERDE\n'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n's3://example-migration-data/glue/example_account_transactions'\nTBLPROPERTIES (\n'has_encrypted_data'='false');\n\ns3://example-migration-data/test/request_1364644_20191017132643.json\ns3://ap-south-1.example.com/example-migration-data/test/\n\nCREATE EXTERNAL TABLE IF NOT EXISTS s3_db.test (\n    `id` string,\n    `customer_id` string,\n    `sender` string,\n    `message` string,\n    `message_type` string,\n    `sms_time` date,\n    `create_date` date,\n    `device_id` string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nWITH SERDEPROPERTIES (\n    'serialization.format' = ',',\n    'field.delim' = ','\n) LOCATION 's3://ap-south-1.example.com/example/example/folder/'\nTBLPROPERTIES ('has_encrypted_data'='false');\n\nCREATE EXTERNAL TABLE `test_json_1`(\n`header` map<string,string> COMMENT 'from deserializer',\n`tracking-id` string COMMENT 'from deserializer',\n`acknowledgement-id` string COMMENT 'from deserializer',\n`response-format` array<string> COMMENT 'from deserializer')\nROW FORMAT SERDE\n'org.openx.data.jsonserde.JsonSerDe'\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n's3://example-migration-data/test'\nTBLPROPERTIES (\n'transient_lastDdlTime'='1583144161')\n\nCREATE EXTERNAL TABLE x (\n    `acknowledgement-id` string,\n    `header` struct<`application-id`:string, `cust-id`:string, `request-received-time`:string, `request-type`:string>,\n    `response-format` array<string>,\n    `tracking-id` string)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n's3://example-migration-data/test'\n\nCREATE EXTERNAL TABLE example (\n            `acknowledgement-id` int,\n            `finished` array<struct<`bureau`:string,\n            `json-response-object`:struct<`accountlist`:array<struct<`accounttype`:string,\n            `currentbalance`:string,\n            `dateopenedordisbursed`:string,\n            `datereportedandcertified`:string,\n            `ownershipindicator`:string,\n            `paymenthistory1`:string,\n            `paymenthistoryenddate`:string,\n            `paymenthistorystartdate`:string,\n            `reportingmembershortname`:string>>,\n            `addresslist`:array<struct<`addresscategory`:string,\n            `addressline1`:string,\n            `datereported`:string,\n            `enrichedthroughtenquiry`:string,\n            `pincode`:string,\n            `statecode`:string>>,\n            `employmentlist`:array<struct<`accounttype`:string,\n            `datereported`:string,\n            `occupationcode`:string>>,\n            `enquirylist`:array<struct<`datereported`:string,\n            `enquiryamount`:string,\n            `enquirypurpose`:string,\n            `reportingmembershortname`:string>>,\n            `header`:struct<`dateproceed`:string,\n            `enquirycontrolnumber`:string,\n            `enquirymemberuserid`:string,\n            `memberreferencenumber`:string,\n            `serialversionuid`:int,\n            `subjectreturncode`:string,\n            `timeproceed`:string,\n            `version`:string>,\n            `idlist`:array<struct<`idtype`:string,\n            `idvalue`:string>>,\n            `name`:struct<`dob`:string,\n            `gender`:string,\n            `name1`:string,\n            `name2`:string,\n            `name3`:string>,\n            `phonelist`:array<struct<`enrichenquiryforphone`:string,\n            `telephonenumber`:string,\n            `telephonetype`:string>>,\n            `scorelist`:array<struct<`reasoncode1`:string,\n            `reasoncode2`:string,\n            `reasoncode3`:string,\n            `score`:string,\n            `scorecardname`:string,\n            `scorecardversion`:string,\n            `scoredate`:string,\n            `scorename`:string>>>,\n            `pdf report`:array<int>,\n            `product`:string,\n            `status`:string,\n            `tracking-id`:int>>,\n            `header` struct<`application-id`:string,\n            `cust-id`:string,\n            `request-received-time`:string,\n            `response-type`:string>,\n            `status` string\n)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://example-migration-data/example_response';\n\nWITH  SERDEPROPERTIES ('ignore.malformed.json' = 'true')\n\nCREATE external TABLE complex_json (\n    docid string,\n    `user` struct<\n                id:INT,\n                username:string,\n                name:string,\n                shippingaddress:struct<\n                                        address1:string,\n                                        address2:string,\n                                        city:string,\n                                        state:string\n                                        >,\n                orders:array<\n                            struct<\n                                    itemid:INT,\n                                    orderdate:string\n                                    >\n                                >\n                >\n    )\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nLOCATION 's3://mybucket/myjsondata/';\n\nCREATE EXTERNAL TABLE `folder_0d23a883b4a272dd4892202efbac8b59`(\n`device_id` string COMMENT 'from deserializer',\n`udsms` array<struct<sendernumber:string,message:string,datetime:string,messagetype:string,smsid:int>> COMMENT 'from deserializer',\n`cust_id` string COMMENT 'from deserializer')\nPARTITIONED BY (\n`partition_0` string,\n`partition_1` string,\n`partition_2` string)\nROW FORMAT SERDE\n'org.openx.data.jsonserde.JsonSerDe'\nWITH SERDEPROPERTIES (\n'paths'='cust_id,device_id,udsms')\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n's3://example-migration-data/folder/'\nTBLPROPERTIES (\n'CrawlerSchemaDeserializerVersion'='1.0',\n'CrawlerSchemaSerializerVersion'='1.0',\n'UPDATED_BY_CRAWLER'='s3_folder',\n'averageRecordSize'='28252',\n'classification'='json',\n'compressionType'='none',\n'objectCount'='4198',\n'recordCount'='4197',\n'sizeKey'='122901020',\n'typeOfData'='file')\n\n# adding partitions manually\n    ALTER TABLE s3_db.test_response add partition (partition_0=\"2020\", partition_1=\"04\", partition_2=\"23\")\n    location \"s3://example-migration-data/folder_request/response/2020/04/23\";\n\n# creating tables with or without some columns are supported\nCREATE EXTERNAL TABLE `test_response`(\n`delay_time` string COMMENT 'from deserializer',\n`create_date` string COMMENT 'from deserializer',\n`customer_id` double COMMENT 'from deserializer',\n`device_id` string COMMENT 'from deserializer',\n`oldest_sms` string COMMENT 'from deserializer',\n`latest_sms` string COMMENT 'from deserializer',\n`nummessages` int COMMENT 'from deserializer',\n`score` double COMMENT 'from deserializer',\n`model_execution_status` string COMMENT 'from deserializer',\n`reason_code` string COMMENT 'from deserializer',\n`tier` double COMMENT 'from deserializer',\n`model_name` string COMMENT 'from deserializer',\n`decision_model_flag` int COMMENT 'from deserializer',\n`request_key` string COMMENT 'from deserializer',\n`request_response_flag` string COMMENT 'from deserializer')\nPARTITIONED BY (\n`partition_0` string,\n`partition_1` string,\n`partition_2` string)\nROW FORMAT SERDE\n'org.openx.data.jsonserde.JsonSerDe'\nWITH SERDEPROPERTIES (\n'paths'='create_date,customer_id,decision_model_flag,delay_time,device_id,latest_sms,model_execution_status,model_name,numMessages,oldest_sms,reason_code,request_key,request_response_flag,score,tier')\nSTORED AS INPUTFORMAT\n'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n's3://example-migration-data/folder_request/response/'\n\nhttps://aws.amazon.com/blogs/big-data/analyzing-data-in-s3-using-amazon-athena/\n\nCREATE EXTERNAL TABLE IF NOT EXISTS http_requests (\n`referrer_url` string,\n`target_url` string,\n`method` string,\n`request_headers` map,\n`request_params` map,\n`is_https` boolean,\n`user_agent` string,\n`response_http_code` int,\n`response_headers` map,\n`transaction_id` string,\n`server_hostname` string)\nPARTITIONED BY (`date` string)\nSTORED AS PARQUET\nLOCATION 's3://httprequests/'\ntblproperties (\"parquet.compress\"=\"GZIP\");\n    https://medium.com/@costimuraru/querying-terabytes-of-proto-parquet-data-with-amazon-athena-or-apache-hive-fb51addce5ac\n\nSELECT  create_date,customer_id,loan_id,credit_amt_2m,credit_cnt_2m,debit_amt_2m,debit_cnt_2m\n    ,credit_amt_3m,credit_cnt_3m,debit_amt_3m,debit_cnt_3m,avg_eod_balance_3M\nFROM bank_data.meta_data where cast(substring(create_date,1,19) as timestamp) between date_add('month',-3,now()) and now();\n"})}),"\n",(0,a.jsx)(n.h2,{id:"things-to-remember",children:"Things to remember"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"MSCK repair statement loads the partition data for Hive-compatible data/partitions like year=2020/month=04/day=20. In this case, you would have to use ALTER TABLE ADD PARTITION to add each partition manually."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["msck - is Hive's ",(0,a.jsx)(n.strong,{children:"m"}),"eta",(0,a.jsx)(n.strong,{children:"s"}),"tore ",(0,a.jsx)(n.strong,{children:"c"}),"onsistency ",(0,a.jsx)(n.strong,{children:"c"}),"heck, similar to ",(0,a.jsx)(n.code,{children:"fsck"})," which stands for ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Fsck",children:"filesystem consistency check"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Hive Schema mismatch"}),"\n",(0,a.jsx)(n.p,{children:"It can happen that partition schema is different from table's schema leading to 'HIVE_PARTITION_SCHEMA_MISMATCH' error. At the beginning of query execution, Athena verifies the table's schema by checking that each column data type is compatible between the table and the partition. If you create a table in CSV, JSON, and AVRO in Athena with AWS Glue Crawler, after the Crawler finishes processing, the schemas for the table and its partitions may be different. If there is a mismatch between the table's schema and the partition schemas, your queries fail in Athena due to the schema verification error."}),"\n",(0,a.jsxs)(n.p,{children:["A typical workaround for such errors is to drop the partition that is causing the error and recreate it. Suggest to use ALTER TABLE DROP PARTITION and ALTER TABLE ADD PARTITION to drop and add partition manually. It is recommended to go thru documentation link provided below for more information on updates in tables with partitions. ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/updates-and-partitions.html",children:"https://docs.aws.amazon.com/athena/latest/ug/updates-and-partitions.html"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"performance-tuning-guides",children:"Performance tuning guides"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Partition your data (year=2019/month=12/day=21)"}),"\n",(0,a.jsx)(n.li,{children:"Bucket your data"}),"\n",(0,a.jsx)(n.li,{children:"Compress and split files (snappy)"}),"\n",(0,a.jsx)(n.li,{children:"Optimize file sizes (Athena (Presto) likes big files. Recommended minimum is 128MB per file.)"}),"\n",(0,a.jsx)(n.li,{children:"Optimize columnar data store generation (parquet/orc)"}),"\n",(0,a.jsx)(n.li,{children:"Optimize ORDER BY"}),"\n",(0,a.jsx)(n.li,{children:"Optimize joins"}),"\n",(0,a.jsx)(n.li,{children:"Optimize GROUP BY"}),"\n",(0,a.jsx)(n.li,{children:"Optimize the LIKE operator"}),"\n",(0,a.jsx)(n.li,{children:"Use approximate functions"}),"\n",(0,a.jsx)(n.li,{children:"Only include the columns that you need"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena",children:"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena"})}),"\n",(0,a.jsx)(n.h2,{id:"connectors",children:"Connectors"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"DynamoDB Connector"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The Amazon Athena DynamoDB connector enables Amazon Athena to communicate with DynamoDB so that you can query your tables with SQL."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/athena-prebuilt-data-connectors-dynamodb.html",children:"https://docs.aws.amazon.com/athena/latest/ug/athena-prebuilt-data-connectors-dynamodb.html"})}),"\n",(0,a.jsx)(n.h2,{id:"csv-to-bulk-import",children:"CSV to bulk import"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://mbejda.github.io/CSV-to-Athena-Bulk-Import",children:"https://mbejda.github.io/CSV-to-Athena-Bulk-Import"})}),"\n",(0,a.jsx)(n.h2,{id:"debugging",children:"Debugging"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/premiumsupport/knowledge-center/error-json-athena",children:"https://aws.amazon.com/premiumsupport/knowledge-center/error-json-athena"})}),"\n",(0,a.jsx)(n.h2,{id:"links",children:"Links"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/blogs/storage/understand-amazon-s3-data-transfer-costs-by-classifying-requests-with-amazon-athena/",children:"Understand Amazon S3 data transfer costs by classifying requests with Amazon Athena | AWS Storage Blog"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/athena/faqs",children:"https://aws.amazon.com/athena/faqs"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://www.linkedin.com/pulse/my-top-5-gotchas-working-aws-glue-tanveer-uddin",children:"https://www.linkedin.com/pulse/my-top-5-gotchas-working-aws-glue-tanveer-uddin"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/blogs/big-data/create-tables-in-amazon-athena-from-nested-json-and-mappings-using-jsonserde",children:"https://aws.amazon.com/blogs/big-data/create-tables-in-amazon-athena-from-nested-json-and-mappings-using-jsonserde"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"http://thornydev.blogspot.com/2013/07/querying-json-records-via-hive.html",children:"ThornyDev: Querying JSON records via Hive"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/engine-versions-reference.html",children:"Athena engine version reference - Amazon Athena"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://aws.amazon.com/blogs/big-data/multicloud-data-lake-analytics-with-amazon-athena/",children:"Multicloud data lake analytics with Amazon Athena | AWS Big Data Blog"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://aws.amazon.com/blogs/big-data/visualize-mongodb-data-from-amazon-quicksight-using-amazon-athena-federated-query/",children:"Visualize MongoDB data from Amazon QuickSight using Amazon Athena Federated Query | AWS Big Data Blog"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://www.mongodb.com/community/forums/t/real-time-mongodb-data-to-aws-quicksight/2314",children:"Real Time: MongoDB data to AWS QuickSight - Working with Data / Connectors & Integrations - MongoDB Developer Community Forums"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"github---quux00hive-json-schema-tool-to-generate-a-hive-schema-from-a-json-example-doc",children:(0,a.jsx)(n.a,{href:"https://github.com/quux00/hive-json-schema",children:"GitHub - quux00/hive-json-schema: Tool to generate a Hive schema from a JSON example doc"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"java -jar target/json-hive-schema-1.0-jar-with-dependencies.jar in.json TopQuark"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE TopQuark (\ndescription string,\nfoo struct<bar:string, level1:struct<l2string:string, l2struct:struct<level3:string>>, quux:string>,\nwibble string,\nwobble array<struct<entry:int, entrydetails:struct<details1:string, details2:int>>>)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe';\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},511151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>r});var a=t(667294);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);