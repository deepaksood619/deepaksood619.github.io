"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[37793],{127906:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"technologies/apache-spark/03-processing-model","title":"Processing Model","description":"Apache Spark is a next generation batch processing framework with stream processing capabilities. Built using many of the same principles of Hadoop\'s MapReduce engine, Spark focuses primarily on speeding up batch processing workloads by offering full in-memory computation and processing optimization.","source":"@site/docs/technologies/apache-spark/03-processing-model.md","sourceDirName":"technologies/apache-spark","slug":"/technologies/apache-spark/03-processing-model","permalink":"/technologies/apache-spark/03-processing-model","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache-spark/03-processing-model.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1734022610000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Spark Modules","permalink":"/technologies/apache-spark/02-spark-modules"},"next":{"title":"Key points","permalink":"/technologies/apache-spark/04-key-points"}}');var i=a(474848),n=a(28453);const o={},r="Processing Model",c={},l=[{value:"Batch Processing Model",id:"batch-processing-model",level:2},{value:"Stream Processing Model",id:"stream-processing-model",level:2},{value:"Advantages and Limitations",id:"advantages-and-limitations",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const t={h1:"h1",h2:"h2",header:"header",p:"p",...(0,n.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"processing-model",children:"Processing Model"})}),"\n",(0,i.jsx)(t.p,{children:"Apache Spark is a next generation batch processing framework with stream processing capabilities. Built using many of the same principles of Hadoop's MapReduce engine, Spark focuses primarily on speeding up batch processing workloads by offering full in-memory computation and processing optimization."}),"\n",(0,i.jsx)(t.p,{children:"Spark can be deployed as a standalone cluster (if paired with a capable storage layer) or can hook into Hadoop as an alternative to the MapReduce engine."}),"\n",(0,i.jsx)(t.h2,{id:"batch-processing-model",children:"Batch Processing Model"}),"\n",(0,i.jsx)(t.p,{children:"Unlike MapReduce, Spark processes all data in-memory, only interacting with the storage layer to initially load the data into memory and at the end to persist the final results. All intermediate results are managed in memory."}),"\n",(0,i.jsx)(t.p,{children:"While in-memory processing contributes substantially to speed, Spark is also faster on disk-related tasks because of holistic optimization that can be achieved by analyzing the complete set of tasks ahead of time. It achieves this by creating Directed Acyclic Graphs, orDAGswhich represent all of the operations that must be performed, the data to be operated on, as well as the relationships between them, giving the processor a greater ability to intelligently coordinate work."}),"\n",(0,i.jsx)(t.p,{children:"To implement in-memory batch computation, Spark uses a model called called Resilient Distributed Datasets, orRDDs, to work with data. These are immutable structures that exist within memory that represent collections of data. Operations on RDDs produce new RDDs. Each RDD can trace its lineage back through its parent RDDs and ultimately to the data on disk. Essentially, RDDs are a way for Spark to maintain fault tolerance without needing to write back to disk after each operation."}),"\n",(0,i.jsx)(t.h2,{id:"stream-processing-model",children:"Stream Processing Model"}),"\n",(0,i.jsx)(t.p,{children:"Stream processing capabilities are supplied by Spark Streaming. Spark itself is designed with batch-oriented workloads in mind. To deal with the disparity between the engine design and the characteristics of streaming workloads, Spark implements a concept calledmicro-batches*. This strategy is designed to treat streams of data as a series of very small batches that can be handled using the native semantics of the batch engine."}),"\n",(0,i.jsx)(t.p,{children:"Spark Streaming works by buffering the stream in sub-second increments. These are sent as small fixed datasets for batch processing. In practice, this works fairly well, but it does lead to a different performance profile than true stream processing frameworks."}),"\n",(0,i.jsx)(t.h2,{id:"advantages-and-limitations",children:"Advantages and Limitations"}),"\n",(0,i.jsx)(t.p,{children:"The obvious reason to use Spark over Hadoop MapReduce is speed. Spark can process the same datasets significantly faster due to its in-memory computation strategy and its advanced DAG scheduling."}),"\n",(0,i.jsx)(t.p,{children:"Another of Spark's major advantages is its versatility. It can be deployed as a standalone cluster or integrated with an existing Hadoop cluster. It can perform both batch and stream processing, letting you operate a single cluster to handle multiple processing styles."}),"\n",(0,i.jsx)(t.p,{children:"Beyond the capabilities of the engine itself, Spark also has an ecosystem of libraries that can be used for machine learning, interactive queries, etc. Spark tasks are almost universally acknowledged to be easier to write than MapReduce, which can have significant implications for productivity."}),"\n",(0,i.jsx)(t.p,{children:"Adapting the batch methodology for stream processing involves buffering the data as it enters the system. The buffer allows it to handle a high volume of incoming data, increasing overall throughput, but waiting to flush the buffer also leads to a significant increase in latency. This means that Spark Streaming might not be appropriate for processing where low latency is imperative."}),"\n",(0,i.jsx)(t.p,{children:"Since RAM is generally more expensive than disk space, Spark can cost more to run than disk-based systems. However, the increased processing speed means that tasks can complete much faster, which may completely offset the costs when operating in an environment where you pay for resources hourly."}),"\n",(0,i.jsx)(t.p,{children:"One other consequence of the in-memory design of Spark is that resource scarcity can be an issue when deployed on shared clusters. In comparison to Hadoop's MapReduce, Spark uses significantly more resources, which can interfere with other tasks that might be trying to use the cluster at the time. In essence, Spark might be a less considerate neighbor than other components that can operate on the Hadoop stack."}),"\n",(0,i.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(t.p,{children:"Spark is a great option for those with diverse processing workloads. Spark batch processing offers incredible speed advantages, trading off high memory usage. Spark Streaming is a good stream processing solution for workloads that value throughput over latency."})]})}function d(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},28453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>r});var s=a(296540);const i={},n=s.createContext(i);function o(e){const t=s.useContext(n);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(n.Provider,{value:t},e.children)}}}]);