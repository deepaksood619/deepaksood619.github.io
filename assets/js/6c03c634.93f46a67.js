"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[57010],{802639:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var n=t(785893),s=t(511151);const o={},a="CPU | GPU | TPU",r={id:"computer-science/operating-system/cpu-gpu-tpu",title:"CPU | GPU | TPU",description:"MAC - Multiplier, Adder, Accumulator",source:"@site/docs/computer-science/operating-system/cpu-gpu-tpu.md",sourceDirName:"computer-science/operating-system",slug:"/computer-science/operating-system/cpu-gpu-tpu",permalink:"/computer-science/operating-system/cpu-gpu-tpu",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/computer-science/operating-system/cpu-gpu-tpu.md",tags:[],version:"current",lastUpdatedAt:1707138374,formattedLastUpdatedAt:"Feb 5, 2024",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Coroutines",permalink:"/computer-science/operating-system/coroutines"},next:{title:"Disk IO",permalink:"/computer-science/operating-system/disk-io"}},c={},l=[{value:"CPU / GPU",id:"cpu--gpu",level:2},{value:"TPU",id:"tpu",level:2},{value:"The Systolic Array",id:"the-systolic-array",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"CPU Time",id:"cpu-time",level:2},{value:"Links",id:"links",level:2}];function h(e){const i={a:"a",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",ul:"ul",...(0,s.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.h1,{id:"cpu--gpu--tpu",children:"CPU | GPU | TPU"}),"\n",(0,n.jsx)(i.p,{children:"MAC - Multiplier, Adder, Accumulator"}),"\n",(0,n.jsx)(i.p,{children:"Tensor - n-dimensional array"}),"\n",(0,n.jsx)(i.p,{children:"Specifically for matrix operations"}),"\n",(0,n.jsx)(i.h2,{id:"cpu--gpu",children:"CPU / GPU"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"A CPU is a scalar machine, which means it processes instructions one step at a time."}),"\n",(0,n.jsxs)(i.li,{children:["A GPU is composed of hundreds of cores that can handle thousands of threads simultaneously.","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Thats because GPUs were designed for 3d game rendering, which often involves parallel operations -The ability of a GPU with 100+ cores to process thousands of threads can accelerate some software by 100x over a CPU alone."}),"\n",(0,n.jsx)(i.li,{children:"What's more, the GPU achieves this acceleration while being more power- and cost-efficient than a CPU."}),"\n",(0,n.jsx)(i.li,{children:"So when neural networks run on GPUs, they run much faster than on CPUs"}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.li,{children:(0,n.jsx)(i.img,{alt:"image",src:t(671515).Z+"",width:"817",height:"554"})}),"\n",(0,n.jsx)(i.li,{children:"A GPU is a vector machine. You can give it a long list of data - a 1D vector - and run computations on the entire list at the same time."}),"\n",(0,n.jsx)(i.li,{children:"This way, we can perform more computations per second, but we have to perform the same computation on a vector of data in parallel."}),"\n",(0,n.jsx)(i.li,{children:"GPUs are general purpose chips. They don't just perform matrix operations, they can really do any kind of computation."}),"\n",(0,n.jsx)(i.li,{children:"GPUs are optimized for taking huge batches of data and performing the same operation over and over very quickly"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"tpu",children:"TPU"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"TPU hardware is comprised of four independent chips."}),"\n",(0,n.jsx)(i.li,{children:"Each chip consists of two compute cores called Tensor Cores."}),"\n",(0,n.jsx)(i.li,{children:"A Tensor Core consists of scalar, vector and matrix units (MXU)."}),"\n",(0,n.jsx)(i.li,{children:"In addition, 8 GB of on-chip memory (HBM) is associated with each Tensor Core."}),"\n",(0,n.jsx)(i.li,{children:"The bulk of the compute horsepower in a Cloud TPU is provided by the MXU."}),"\n",(0,n.jsx)(i.li,{children:"Each MXU is capable of performing 16K multiply-accumulate operations in each cycle."}),"\n",(0,n.jsx)(i.li,{children:"While the MXU's inputs and outputs are 32-bit floating point values, the MXU performs multiplies at reduced bfloat16 precision."}),"\n",(0,n.jsx)(i.li,{children:"Bfloat16 is a 16-bit floating point representation that provides better training and model accuracy than the IEEE half-precision representation. -From a software perspective, each of the 8 cores on a Cloud TPU can execute user computations (XLA ops) independently."}),"\n",(0,n.jsxs)(i.li,{children:["High-bandwidth interconnects allow the chips to communicate directly with each other.- ",(0,n.jsx)(i.img,{alt:"image",src:t(819674).Z+"",width:"408",height:"589"})]}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"the-systolic-array",children:"The Systolic Array"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"The way to achieve that matrix performance is through a piece of architecture called a systolic array."}),"\n",(0,n.jsx)(i.li,{children:"This is the interesting bit, and it's why a TPU is performant."}),"\n",(0,n.jsx)(i.li,{children:"A systolic array is a kind of hardware algorithm, and it describes a pattern of cells on a chip that computes matrix multiplication."}),"\n",(0,n.jsx)(i.li,{children:'"Systolic" describes how data moves in waves across the chip, like the beating of a human heart.'}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"image",src:t(82893).Z+"",width:"648",height:"120"})}),"\n",(0,n.jsx)(i.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,n.jsx)(i.p,{children:"CPUs:"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Quick prototyping that requires maximum flexibility"}),"\n",(0,n.jsx)(i.li,{children:"Simple models that do not take long to train"}),"\n",(0,n.jsx)(i.li,{children:"Small models with small effective batch sizes"}),"\n",(0,n.jsx)(i.li,{children:"Models that are dominated by custom TensorFlow operations written in C++"}),"\n",(0,n.jsx)(i.li,{children:"Models that are limited by available I/O or the networking bandwidth of the host system"}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:"GPUs:"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Models that are not written in TensorFlow or cannot be written in TensorFlow"}),"\n",(0,n.jsx)(i.li,{children:"Models for which source does not exist or is too onerous to change"}),"\n",(0,n.jsx)(i.li,{children:"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs"}),"\n",(0,n.jsx)(i.li,{children:"Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops)"}),"\n",(0,n.jsx)(i.li,{children:"Medium-to-large models with larger effective batch sizes"}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:"TPUs:"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Models dominated by matrix computations"}),"\n",(0,n.jsx)(i.li,{children:"Models with no custom TensorFlow operations inside the main training loop"}),"\n",(0,n.jsx)(i.li,{children:"Models that train for weeks or months"}),"\n",(0,n.jsx)(i.li,{children:"Larger and very large models with very large effective batch sizes"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"cpu-time",children:"CPU Time"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.a,{href:"https://dzone.com/articles/nice-cpu-time-ni-time-in-top",children:"https://dzone.com/articles/nice-cpu-time-ni-time-in-top"})}),"\n",(0,n.jsx)(i.h2,{id:"links",children:"Links"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.a,{href:"https://www.youtube.com/watch?v=IPre5287P3I",children:"Chasing Silicon: The Race for GPUs"})})]})}function d(e={}){const{wrapper:i}={...(0,s.a)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},671515:(e,i,t)=>{t.d(i,{Z:()=>n});const n=t.p+"assets/images/CPU-GPU-TPU-image1-6e4e088af215dcc3f6975ef020f80aef.jpg"},819674:(e,i,t)=>{t.d(i,{Z:()=>n});const n=t.p+"assets/images/CPU-GPU-TPU-image2-c2adaef7ae08091ea5a54bf0ca8ab0be.jpg"},82893:(e,i,t)=>{t.d(i,{Z:()=>n});const n=t.p+"assets/images/CPU-GPU-TPU-image3-03ae5c97a61da8b7a77ac11bd79e138c.jpg"},511151:(e,i,t)=>{t.d(i,{Z:()=>r,a:()=>a});var n=t(667294);const s={},o=n.createContext(s);function a(e){const i=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(o.Provider,{value:i},e.children)}}}]);