"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[36527],{569931:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var a=o(785893),n=o(511151);const s={},r="Others",i={id:"technologies/apache-hadoop/others",title:"Others",description:"Traditionally, Hadoop saves its data internally in flat sequence files, which is a binary storage format for key value pairs. It has the benefit of being more compact than text and fits well the map-reduce output format. Sequence files can be compressed on value, or block level, to improve its IO profile further. Unfortunately, sequence files are not an optimal solution for Hive since it saves a complete row as a single binary value. Consequently, Hive has to read a full row and decompress it even if only one column is being requested.",source:"@site/docs/technologies/apache-hadoop/others.md",sourceDirName:"technologies/apache-hadoop",slug:"/technologies/apache-hadoop/others",permalink:"/technologies/apache-hadoop/others",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache-hadoop/others.md",tags:[],version:"current",lastUpdatedAt:1734022610,formattedLastUpdatedAt:"Dec 12, 2024",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"MapReduce Examples",permalink:"/technologies/apache-hadoop/mapreduce-examples"},next:{title:"Apache Spark",permalink:"/technologies/apache-spark/"}},l={},c=[{value:"Azkaban",id:"azkaban",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",p:"p",strong:"strong",...(0,n.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"others",children:"Others"}),"\n",(0,a.jsx)(t.p,{children:"Traditionally, Hadoop saves its data internally in flat sequence files, which is a binary storage format for key value pairs. It has the benefit of being more compact than text and fits well the map-reduce output format. Sequence files can be compressed on value, or block level, to improve its IO profile further. Unfortunately, sequence files are not an optimal solution for Hive since it saves a complete row as a single binary value. Consequently, Hive has to read a full row and decompress it even if only one column is being requested."}),"\n",(0,a.jsx)(t.h2,{id:"azkaban",children:(0,a.jsx)(t.a,{href:"https://azkaban.github.io/",children:"Azkaban"})}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Azkaban"}),"\xa0is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows."]})]})}function h(e={}){const{wrapper:t}={...(0,n.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},511151:(e,t,o)=>{o.d(t,{Z:()=>i,a:()=>r});var a=o(667294);const n={},s=a.createContext(n);function r(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);