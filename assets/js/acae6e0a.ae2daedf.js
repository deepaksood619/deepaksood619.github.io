"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[82259],{880477:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"databases/nosql-databases/druid/intro","title":"Druid","description":"Apache Druid (incubating) is a real-time analytics database designed for fast slice-and-dice analytics (\\"OLAP\\" queries) on large data sets. Druid is most often used as a database for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.","source":"@site/docs/databases/nosql-databases/druid/intro.md","sourceDirName":"databases/nosql-databases/druid","slug":"/databases/nosql-databases/druid/intro","permalink":"/databases/nosql-databases/druid/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/databases/nosql-databases/druid/intro.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1707138374000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"FAQs","permalink":"/databases/nosql-databases/druid/faqs"},"next":{"title":"Others","permalink":"/databases/nosql-databases/druid/others"}}');var t=n(474848),a=n(28453);const r={},l="Druid",d={},o=[{value:"Features",id:"features",level:2},{value:"Applications",id:"applications",level:2},{value:"Druid Important Points",id:"druid-important-points",level:2},{value:"Druid File Format",id:"druid-file-format",level:2},{value:"Segments",id:"segments",level:3},{value:"Druid Data Modelling",id:"druid-data-modelling",level:2},{value:"Supported Types",id:"supported-types",level:3},{value:"Multi-value and Nested Dimensions",id:"multi-value-and-nested-dimensions",level:4},{value:"Druid Native Batch",id:"druid-native-batch",level:4},{value:"Druid SQL Query API",id:"druid-sql-query-api",level:3},{value:"Druid kafka ingestion",id:"druid-kafka-ingestion",level:2},{value:"Kafka Indexing Service - Exactly once ingestion",id:"kafka-indexing-service---exactly-once-ingestion",level:2},{value:"References",id:"references",level:2},{value:"Imply Druid",id:"imply-druid",level:2},{value:"Others",id:"others",level:2},{value:"Ingestion spec tuning",id:"ingestion-spec-tuning",level:2}];function c(e){const i={a:"a",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"druid",children:"Druid"})}),"\n",(0,t.jsxs)(i.p,{children:['Apache Druid (incubating) is a real-time analytics database designed for fast slice-and-dice analytics ("',(0,t.jsx)(i.a,{href:"http://en.wikipedia.org/wiki/Online_analytical_processing",children:"OLAP"}),'" queries) on large data sets. Druid is most often used as a database for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.']}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"High performance, column oriented, distributed data store"}),"\n",(0,t.jsx)(i.li,{children:"Druid is primarily used to store, query, and analyze large event streams"}),"\n",(0,t.jsx)(i.li,{children:"Druid is optimized for sub-second queries to slice-and-dice, drill down, search, filter, and aggregate this data. Druid is commonly used to power interactive applications where performance, concurrency, and uptime are important."}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"features",children:"Features"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Column-oriented storage"}),"\n",(0,t.jsx)(i.li,{children:"Native search indexes"}),"\n"]}),"\n",(0,t.jsxs)(i.p,{children:["Druid uses ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/1004.0403",children:"CONCISE"})," or ",(0,t.jsx)(i.a,{href:"https://roaringbitmap.org/",children:"Roaring"})," compressed bitmap indexes to create indexes that power fast filtering and searching across multiple columns."]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Scalable distributed system"}),"\n",(0,t.jsx)(i.li,{children:"Massively parallel processing"}),"\n",(0,t.jsx)(i.li,{children:"Streaming and batch ingest"}),"\n",(0,t.jsx)(i.li,{children:"Self-healing, self-balancing, easy to operate"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"As an operator, to scale the cluster out or in, simply add or remove servers and the cluster will rebalance itself automatically, in the background, without any downtime. If any Druid servers fail, the system will automatically route around the damage until those servers can be replaced. Druid is designed to run 24/7 with no need for planned downtimes for any reason, including configuration changes and software updates."}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Cloud-native, fault-tolerant architecture that won't lose data"}),"\n"]}),"\n",(0,t.jsxs)(i.p,{children:["Once Druid has ingested your data, a copy is stored safely in ",(0,t.jsx)(i.a,{href:"http://druid.io/docs/latest/design/index.html#deep-storage",children:"deep storage"}),"(typically cloud storage, HDFS, or a shared filesystem). Your data can be recovered from deep storage even if every single Druid server fails. For more limited failures affecting just a few Druid servers, replication ensures that queries are still possible while the system recovers."]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Approximation algorithms"}),"\n",(0,t.jsx)(i.li,{children:"Automatic summarization at ingest time"}),"\n",(0,t.jsx)(i.li,{children:"Flexible schemas"}),"\n",(0,t.jsx)(i.li,{children:"Time-optimized partitioning"}),"\n",(0,t.jsx)(i.li,{children:"SQL support"}),"\n",(0,t.jsx)(i.li,{children:"Horizontally scalable"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"applications",children:"Applications"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Clickstream analytics"}),"\n",(0,t.jsx)(i.li,{children:"Network telemetry analytics (network performance monitoring)"}),"\n",(0,t.jsx)(i.li,{children:"Server metrics storage"}),"\n",(0,t.jsx)(i.li,{children:"Supply chain analytics (manufacturing metrics)"}),"\n",(0,t.jsx)(i.li,{children:"Application performance metrics"}),"\n",(0,t.jsx)(i.li,{children:"Digital marketing/advertising analytics"}),"\n",(0,t.jsx)(i.li,{children:"Business intelligence / OLAP"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"druid-important-points",children:"Druid Important Points"}),"\n",(0,t.jsxs)(i.p,{children:["Druid does not natively support nested data, so, we need to flatten arrays in our JSON events by providing a ",(0,t.jsx)(i.a,{href:"https://druid.apache.org/docs/latest/ingestion/index.html#flattenspec",children:"flattenspec"}),", or by doing some preprocessing before the event lands in it."]}),"\n",(0,t.jsx)(i.p,{children:"Druid assigns types to columns - string, long, float, complex, etc. The type enforcement at the column level can be restrictive if the incoming data presents with mixed types for a particular field/fields. Each column except the timestamp can be of type dimension or metric."}),"\n",(0,t.jsx)(i.p,{children:"One can filter and group by on dimension columns, but not on metric columns. This needs some forethought when picking which columns to pre-aggregate and which ones will be used for slice-and-dice analyses."}),"\n",(0,t.jsxs)(i.p,{children:["Partition keys must be picked carefully for load-balancing and scaling up. Streaming new updates to the table after creation requires using one of the ",(0,t.jsx)(i.a,{href:"https://druid.apache.org/docs/latest/ingestion/index.html#streaming",children:"supported ways of ingesting"})," - Kafka, Kinesis, or Tranquility."]}),"\n",(0,t.jsx)(i.p,{children:"Druid works well for event analytics in environments where data is somewhat predictable and rollups and pre-aggregations can be defined a priori. It involves some maintenance and tuning overhead in terms of engineering, but for event analytics that doesn't involve complex joins, it can serve queries with low latency and scale up as required."}),"\n",(0,t.jsxs)(i.p,{children:["Druid is fundamentally a column store, and is designed for analytical queries (GROUPBYs with complex WHERE clauses) that need to scan across multiple partitions. Druid stores its index in ",(0,t.jsx)(i.a,{href:"http://druid.io/docs/latest/design/segments.html",children:"segment files"}),", which are partitioned by time. Segment files are columnar, with the data for each column laid out in ",(0,t.jsx)(i.strong,{children:"separate data structures"}),'. By storing each column separately, Druid can decrease query latency by scanning only those columns that are required for a query. There are different column types for different data types (string, numbers, etc.). Different columns can have different encoding and compression algorithms applied. For example, string columns will be dictionary encoded, LZF compressed, and have search indexes created for faster filtering. Numeric columns will have completely different compression and encoding algorithms applied. Druid segments are immutable once finalized, so updates in Druid have limitations. Although more recent versions of Druid have added "lookups", or the ability to join a mutable table external to Druid with an immutable one in Druid, I would not recommend Druid for any workflows where the same underlying data is frequently updated and those updates need to complete in less than a second (say, powering a social media profile page). Druid supports bulk updates, which are more commonly seen with analytic workloads.']}),"\n",(0,t.jsx)(i.h2,{id:"druid-file-format",children:"Druid File Format"}),"\n",(0,t.jsx)(i.h3,{id:"segments",children:"Segments"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Datasources are broken into files called segments"}),"\n",(0,t.jsxs)(i.li,{children:["Segments are grouped into time chunks and potentially, further partitioned at creation time","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"This allows Druid to parallelize ingestion and querying of data"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["Segments are immutable","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Each segment is versioned to allow new versions to replace old versions"}),"\n",(0,t.jsx)(i.li,{children:"This can be done as a background task without blocking queries"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Druid segments have their own file format"}),"\n",(0,t.jsxs)(i.li,{children:["The file format is columnar","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Data is laid out into columns and each column is packed and compressed individually"}),"\n",(0,t.jsx)(i.li,{children:"String columns are dictionary encoded instead of having the full string value in every row"}),"\n",(0,t.jsx)(i.li,{children:"Numeric columns use various kinds of numeric compression based on the data"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Columnar formats are the standard for analytical workloads due to superior scan performance"}),"\n",(0,t.jsx)(i.li,{children:"String columns in Druid are indexed with bitmap indexes"}),"\n",(0,t.jsx)(i.li,{children:"Columns are further compressed with general-purpose algorithms like LZ4 (Lossless compression)"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(282786).A+"",width:"1090",height:"452"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["This on-disk format has several benefits:","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Filtering if a domain exists require reading less data"}),"\n",(0,t.jsx)(i.li,{children:"Compression of like data performs better than a row-oriented format"}),"\n",(0,t.jsx)(i.li,{children:"Druid only needs to read the columns involved in a query, eliminating extraneous fetches from disk and memory"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(31277).A+"",width:"1100",height:"800"})}),"\n",(0,t.jsx)(i.h2,{id:"druid-data-modelling",children:"Druid Data Modelling"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Designing a data model for Druid is different than a RDBMS"}),"\n",(0,t.jsxs)(i.li,{children:["Most RDBMS design focuses on normalization","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Each piece of data should only appear once in the database"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["Databases like Druid favor some denormalization","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Each piece of data may appear many different times in the database"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["Druid favors denormalized data because this is faster than doing a join with another large table","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Flat schemas increase performance by allowing Druid to operate directly on compressed dictionary-encoded data"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["When designing a table, you must","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Choose the datasource name"}),"\n",(0,t.jsx)(i.li,{children:"The source for the input data"}),"\n",(0,t.jsx)(i.li,{children:"The columns you want to store"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Creating a data model is more than just copying the table structure of a RDBMS table"}),"\n",(0,t.jsxs)(i.li,{children:["When transitioning from a RDBMS, you will need to choose","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Which columns should be included"}),"\n",(0,t.jsx)(i.li,{children:"Which types to use for each column (Druid doesn't support all RDBMS types)"}),"\n",(0,t.jsx)(i.li,{children:"Whether you will be totally flattening your tables, or using Druid's query-time lookups to partially normalize data"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["In Druid, columns will become","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Dimensions (stored as-is)"}),"\n",(0,t.jsx)(i.li,{children:"Metrics (partially aggregated)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"supported-types",children:"Supported Types"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(387824).A+"",width:"1101",height:"521"})}),"\n",(0,t.jsx)(i.h4,{id:"multi-value-and-nested-dimensions",children:"Multi-value and Nested Dimensions"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(918979).A+"",width:"1101",height:"389"})}),"\n",(0,t.jsx)(i.h4,{id:"druid-native-batch",children:"Druid Native Batch"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Native batch is a built-in way to index data in Druid"}),"\n",(0,t.jsx)(i.li,{children:"Native batch ingestion is useful when you have a file that you want to load into Druid"}),"\n",(0,t.jsx)(i.li,{children:"There has to be a method to convert your file to Druid's file format and make segments (Native batch performs this conversion)"}),"\n",(0,t.jsx)(i.li,{children:"Druid also supports batching ingestion with Hadoop"}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.strong,{children:"Native Batch Architecture"})}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(537926).A+"",width:"1099",height:"957"})}),"\n",(0,t.jsx)(i.h3,{id:"druid-sql-query-api",children:"Druid SQL Query API"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Data stored in Druid can be queried with a SQL API"}),"\n",(0,t.jsxs)(i.li,{children:["These calls go over a HTTP REST API","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"The payload of the REST call is JSON"}),"\n",(0,t.jsx)(i.li,{children:"The query results come back as JSON or CSV"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Not every SQL feature is supported"}),"\n",(0,t.jsx)(i.li,{children:"The query engine is implemented with Apache Calcite"}),"\n",(0,t.jsx)(i.li,{children:"The SQL queries are translated into Druid's native queries"}),"\n",(0,t.jsxs)(i.li,{children:["There is a native JSON query API available too","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"The SQL API is virtually at parity with the JSON and is easier to use"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Where clauses"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(695697).A+"",width:"1000",height:"376"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Limiting"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(117172).A+"",width:"974",height:"306"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Group By Query"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(998823).A+"",width:"912",height:"332"})}),"\n",(0,t.jsx)(i.h2,{id:"druid-kafka-ingestion",children:"Druid kafka ingestion"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html",children:"http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"http://druid.io/docs/latest/tutorials/tutorial-kafka.html",children:"http://druid.io/docs/latest/tutorials/tutorial-kafka.html"})}),"\n",(0,t.jsx)(i.h2,{id:"kafka-indexing-service---exactly-once-ingestion",children:"Kafka Indexing Service - Exactly once ingestion"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://imply.io/post/exactly-once-streaming-ingestion",children:"https://imply.io/post/exactly-once-streaming-ingestion"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(376170).A+"",width:"1098",height:"683"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(168144).A+"",width:"1099",height:"705"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"image",src:n(820365).A+"",width:"952",height:"848"})}),"\n",(0,t.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"http://druid.io/docs/latest/design/index.html",children:"http://druid.io/docs/latest/design/index.html"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://medium.com/@leventov/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9",children:"https://medium.com/@leventov/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://medium.com/@leventov/the-challenges-of-running-druid-at-large-scale-and-future-directions-part-2-ef594ce298f2",children:"https://medium.com/@leventov/the-challenges-of-running-druid-at-large-scale-and-future-directions-part-2-ef594ce298f2"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://druid.apache.org/docs/latest/ingestion/data-management.html",children:"https://druid.apache.org/docs/latest/ingestion/data-management.html"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://imply.io/druid-university/intro-to-druid-university",children:"https://imply.io/druid-university/intro-to-druid-university"})}),"\n",(0,t.jsx)(i.h2,{id:"imply-druid",children:"Imply Druid"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"tiered data"}),"\n",(0,t.jsxs)(i.li,{children:["data volumes- Plywood","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"queries"}),"\n",(0,t.jsx)(i.li,{children:"expressions"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Pivot - alerts"}),"\n",(0,t.jsx)(i.li,{children:"Measures"}),"\n",(0,t.jsx)(i.li,{children:"Roll up"}),"\n",(0,t.jsx)(i.li,{children:"Virtual columns (middle manager)"}),"\n",(0,t.jsx)(i.li,{children:"Clarity"}),"\n",(0,t.jsx)(i.li,{children:"Imply Manager"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"others",children:"Others"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Ingestion Spec can handle flattening of schema"}),"\n",(0,t.jsx)(i.li,{children:"Primary partition keys"}),"\n",(0,t.jsx)(i.li,{children:"Secondary partition keys"}),"\n",(0,t.jsx)(i.li,{children:"2 threads in middlemanager can handle 25 GB/hour of ingestion (2 HyperThreads per task)"}),"\n",(0,t.jsx)(i.li,{children:"Per tasks per partition ingest from Kafka"}),"\n",(0,t.jsxs)(i.li,{children:["Segment scan rate","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"4 GB/s/HT for data sitting in RAM"}),"\n",(0,t.jsx)(i.li,{children:"0.5 GB/s/HT for data sitting in disk"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Druid is not good for computing moving averages"}),"\n",(0,t.jsx)(i.li,{children:"No ML library in Druid"}),"\n",(0,t.jsxs)(i.li,{children:["Duplicate data removal using HyperLogLog- ",(0,t.jsx)(i.a,{href:"https://cleanprogrammer.net/getting-unique-counts-from-druid-using-hyperloglog",children:"https://cleanprogrammer.net/getting-unique-counts-from-druid-using-hyperloglog"})]}),"\n",(0,t.jsx)(i.li,{children:"Druid not build for historical storage"}),"\n",(0,t.jsx)(i.li,{children:"MiddleManager creates segments, and then put it in deep storage, Historical pulls data from deep storage, create segment cache and serves it. Local Cache is there until the retention policy."}),"\n",(0,t.jsx)(i.li,{children:"Middle manager will skip the data if it's not in the right format. There's no way to know which data was skipped."}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"ingestion-spec-tuning",children:"Ingestion spec tuning"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsx)(i.li,{children:"Task duration - 60 minutes (PT60M) (current - 5 minutes)"}),"\n",(0,t.jsx)(i.li,{children:"Task completion time - 60 minutes (current - 5 minutes)"}),"\n",(0,t.jsx)(i.li,{children:"Segment size - 5 Million"}),"\n",(0,t.jsx)(i.li,{children:"Segment granularity - 1 day/1hour (current - 1 hour)"}),"\n",(0,t.jsx)(i.li,{children:"Handoff period - currently too large (tune it)"}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},282786:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image1-dd6b7d1dd240b307adad2707e811ae4a.jpg"},168144:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image10-594d74179643a97eb7cbc71d6de002bd.jpg"},820365:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image11-62a454dedeaa322c56e052dc92f7f610.jpg"},31277:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image2-e4cb5b575f34560d5575b589282daa33.jpg"},387824:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image3-3881169a2dc415b62d3c2ec9f0880521.jpg"},918979:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image4-56e35b91f85f426e91d09c8bea90c9c1.jpg"},537926:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image5-0f1e1182cd03717d25b28ea913d64714.jpg"},695697:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image6-2bcfd2bc1a65ea8601bfbf632b6e592e.jpg"},117172:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image7-ed4fbc9d342a07f49e7e88c3bb2e4ca6.jpg"},998823:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image8-81fe8150bcd2b609319e1b7fe530e025.jpg"},376170:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Druid-image9-66071d25d1b25c7d8da3ada05ba00fef.jpg"},28453:(e,i,n)=>{n.d(i,{R:()=>r,x:()=>l});var s=n(296540);const t={},a=s.createContext(t);function r(e){const i=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:i},e.children)}}}]);