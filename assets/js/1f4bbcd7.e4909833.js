"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[39333],{999016:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var t=n(785893),r=n(511151);const a={},o="JAX",s={id:"ai/libraries/jax",title:"JAX",description:"- J - Just-in-time",source:"@site/docs/ai/libraries/jax.md",sourceDirName:"ai/libraries",slug:"/ai/libraries/jax",permalink:"/ai/libraries/jax",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/libraries/jax.md",tags:[],version:"current",lastUpdatedAt:1726756705,formattedLastUpdatedAt:"Sep 19, 2024",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Distributed Training",permalink:"/ai/libraries/distributed-training"},next:{title:"Keras",permalink:"/ai/libraries/keras"}},l={},c=[{value:"Links",id:"links",level:2}];function d(e){const i={a:"a",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h1,{id:"jax",children:"JAX"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"J - Just-in-time"}),"\n",(0,t.jsx)(i.li,{children:"A - Autograd"}),"\n",(0,t.jsx)(i.li,{children:"X - XLA - Accelerated Linear Algebra"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning."}),"\n",(0,t.jsx)(i.p,{children:"JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research."}),"\n",(0,t.jsxs)(i.p,{children:["JAX a library for array-oriented numerical computation (",(0,t.jsx)(i.em,{children:"\xe0 la"}),"\xa0",(0,t.jsx)(i.a,{href:"https://numpy.org/",children:"NumPy"}),"), with automatic differentiation and JIT compilation to enable high-performance machine learning research."]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"JAX provides a unified NumPy-like interface to computations that run on CPU, GPU, or TPU, in local or distributed settings."}),"\n",(0,t.jsxs)(i.li,{children:["JAX features built-in Just-In-Time (JIT) compilation via\xa0",(0,t.jsx)(i.a,{href:"https://github.com/openxla",children:"Open XLA"}),", an open-source machine learning compiler ecosystem."]}),"\n",(0,t.jsx)(i.li,{children:"JAX functions support efficient evaluation of gradients via its automatic differentiation transformations."}),"\n",(0,t.jsx)(i.li,{children:"JAX functions can be automatically vectorized to efficiently map them over arrays representing batches of inputs."}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"links",children:"Links"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://github.com/google/jax",children:"GitHub - google/jax: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://jax.readthedocs.io/en/latest/notebooks/quickstart.html",children:"JAX Quickstart \u2014 JAX documentation"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://youtu.be/_0D5lXDjNpw",children:"JAX in 100 Seconds"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Google_JAX",children:"Google JAX - Wikipedia"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://www.kaggle.com/learn-guide/jax",children:"JAX Guide | Kaggle"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb",children:"neural-network-and-data-loading.ipynb - Colab"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://www.youtube.com/watch?v=uySOfXq-II0",children:"What is JAX? - YouTube"})}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,r.a)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},511151:(e,i,n)=>{n.d(i,{Z:()=>s,a:()=>o});var t=n(667294);const r={},a=t.createContext(r);function o(e){const i=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);