"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[55906],{640545:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ai/computer-vision-cv/pre-trained-models","title":"Pre-Trained Models","description":"Pre-trained models\xa0are neural networks trained on large datasets before being fine-tuned for specific tasks. These models capture intricate patterns and features, making them highly effective for image classification. By leveraging pre-trained models, developers can save time and computational resources. They can also achieve high accuracy with less data. Popular models like VGG, ResNet, and Inception have set benchmarks in the field.","source":"@site/docs/ai/computer-vision-cv/pre-trained-models.md","sourceDirName":"ai/computer-vision-cv","slug":"/ai/computer-vision-cv/pre-trained-models","permalink":"/ai/computer-vision-cv/pre-trained-models","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/computer-vision-cv/pre-trained-models.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1734554726000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Others","permalink":"/ai/computer-vision-cv/others"},"next":{"title":"Data Science","permalink":"/ai/data-science/"}}');var r=n(474848),t=n(28453);const l={},a="Pre-Trained Models",o={},d=[{value:"Overview of Pre-Trained Models",id:"overview-of-pre-trained-models",level:2},{value:"Top Pre-Trained Models for Image Classification",id:"top-pre-trained-models-for-image-classification",level:2},{value:"1.\xa0ResNet (Residual Networks)",id:"1resnet-residual-networks",level:3},{value:"2.\xa0Inception (GoogLeNet)",id:"2inception-googlenet",level:3},{value:"3.\xa0VGG (Visual Geometry Group)",id:"3vgg-visual-geometry-group",level:3},{value:"4.\xa0EfficientNet",id:"4efficientnet",level:3},{value:"5.\xa0DenseNet (Dense Convolutional Network)",id:"5densenet-dense-convolutional-network",level:3},{value:"6.\xa0MobileNet",id:"6mobilenet",level:3},{value:"7.\xa0NASNet (Neural Architecture Search Network)",id:"7nasnet-neural-architecture-search-network",level:3},{value:"8.\xa0Xception (Extreme Inception)",id:"8xception-extreme-inception",level:3},{value:"9.\xa0AlexNet",id:"9alexnet",level:3},{value:"10.\xa0Vision Transformers (ViT)",id:"10vision-transformers-vit",level:3},{value:"YOLO - You Only Look Once",id:"yolo---you-only-look-once",level:2},{value:"Pre-trained models",id:"pre-trained-models-1",level:3},{value:"Differences",id:"differences",level:2},{value:"Benefits of Pre-Trained Models for Image Classification",id:"benefits-of-pre-trained-models-for-image-classification",level:2},{value:"Challenges of Pre-Trained Models for Image Classification",id:"challenges-of-pre-trained-models-for-image-classification",level:2},{value:"Links",id:"links",level:2}];function c(e){const i={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"pre-trained-models",children:"Pre-Trained Models"})}),"\n",(0,r.jsx)(i.p,{children:"Pre-trained models\xa0are neural networks trained on large datasets before being fine-tuned for specific tasks. These models capture intricate patterns and features, making them highly effective for image classification. By leveraging pre-trained models, developers can save time and computational resources. They can also achieve high accuracy with less data. Popular models like VGG, ResNet, and Inception have set benchmarks in the field."}),"\n",(0,r.jsx)(i.h2,{id:"overview-of-pre-trained-models",children:"Overview of Pre-Trained Models"}),"\n",(0,r.jsx)(i.p,{children:"Pre-trained models\xa0are an essential part of modern deep learning. These models are initially trained on large, general-purpose datasets like ImageNet. They learn to recognise various features, from simple edges to complex textures and objects. This extensive training allows them to generalise well, making them effective starting points for new tasks. By fine-tuning these models on specific datasets, developers can achieve high performance with less data and computation."}),"\n",(0,r.jsx)(i.p,{children:"The architecture of pre-trained models varies, but they share common traits. They consist of multiple layers that progressively extract features from the input images. Early layers capture low-level features, while deeper layers recognise high-level patterns. Pre-trained models can be adapted to various domains, from medical imaging to autonomous driving. Their versatility and effectiveness make them invaluable tools in the field of computer vision."}),"\n",(0,r.jsx)(i.h2,{id:"top-pre-trained-models-for-image-classification",children:"Top Pre-Trained Models for Image Classification"}),"\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.img,{alt:"Overview of architectures until 2018",src:n(478591).A+"",width:"655",height:"650"})}),"\n",(0,r.jsx)(i.p,{children:"Several pre-trained models have become standards in image classification due to their performance and reliability. Here are the key models:"}),"\n",(0,r.jsx)(i.h3,{id:"1resnet-residual-networks",children:"1.\xa0ResNet (Residual Networks)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview:"})," ResNet, introduced by Microsoft Research, revolutionized\xa0",(0,r.jsx)(i.a,{href:"https://www.geeksforgeeks.org/introduction-deep-learning/",children:"deep learning"}),"\xa0by using residual connections to mitigate the vanishing gradient problem in deep networks."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Variants:"})," ResNet-50, ResNet-101, ResNet-152."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Deep architectures (up to 152 layers)."}),"\n",(0,r.jsx)(i.li,{children:"Residual blocks to allow gradients to flow through shortcut connections."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications:"})," General image classification, object detection, and feature extraction."]}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Vanishing_gradient_problem",children:"Vanishing gradient problem - Wikipedia"})}),"\n",(0,r.jsx)(i.h3,{id:"2inception-googlenet",children:"2.\xa0Inception (GoogLeNet)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by Google, the Inception network uses inception modules to capture multi-scale features."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Variants"}),": Inception v3, Inception v4, Inception-ResNet."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Inception modules with convolutional filters of multiple sizes."}),"\n",(0,r.jsx)(i.li,{children:"Efficient architecture balancing accuracy and computational cost."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification, object detection, and transfer learning."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"3vgg-visual-geometry-group",children:"3.\xa0VGG (Visual Geometry Group)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by the Visual Geometry Group at the University of Oxford, VGG models are known for their simplicity and depth."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Variants"}),": VGG-16, VGG-19."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Deep networks with 16 or 19 layers."}),"\n",(0,r.jsx)(i.li,{children:"Simple architecture using only 3\xd73 convolutions."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification and feature extraction."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"4efficientnet",children:"4.\xa0EfficientNet"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by Google, EfficientNet models achieve high accuracy with fewer parameters and computational resources."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Variants"}),": EfficientNet-B0 to EfficientNet-B7."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Compound scaling method to scale depth, width, and resolution."}),"\n",(0,r.jsx)(i.li,{children:"Efficient and accurate."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification and transfer learning."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"5densenet-dense-convolutional-network",children:"5.\xa0DenseNet (Dense Convolutional Network)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by researchers at Cornell University, DenseNet connects each layer to every other layer in a feed-forward fashion."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Variants"}),": DenseNet-121, DenseNet-169, DenseNet-201."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Dense connections to improve gradient flow and feature reuse."}),"\n",(0,r.jsx)(i.li,{children:"Reduces the number of parameters compared to traditional convolutional networks."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification and feature extraction."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"6mobilenet",children:"6.\xa0MobileNet"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by Google, MobileNet models are designed for mobile and embedded vision applications."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Variants"}),": MobileNetV1, MobileNetV2, MobileNetV3."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Lightweight architecture optimized for mobile devices."}),"\n",(0,r.jsx)(i.li,{children:"Depthwise separable convolutions."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": Mobile image classification and embedded vision applications."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"7nasnet-neural-architecture-search-network",children:"7.\xa0NASNet (Neural Architecture Search Network)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by Google using neural architecture search techniques to optimize the network structure."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Variants"}),": NASNet-A, NASNet-B, NASNet-C."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Automatically designed architectures using reinforcement learning."}),"\n",(0,r.jsx)(i.li,{children:"High accuracy with efficient performance."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification and transfer learning."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"8xception-extreme-inception",children:"8.\xa0Xception (Extreme Inception)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by Google, Xception is an extension of the Inception architecture with depthwise separable convolutions."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Fully convolutional architecture."}),"\n",(0,r.jsx)(i.li,{children:"Depthwise separable convolutions for improved performance."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification and transfer learning."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"9alexnet",children:"9.\xa0AlexNet"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by Alex Krizhevsky, AlexNet is one of the earliest deep learning models that popularized the use of CNNs in image classification."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Simple architecture with 8 layers."}),"\n",(0,r.jsx)(i.li,{children:"ReLU activation functions and dropout regularization."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification and historical benchmarks."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"10vision-transformers-vit",children:"10.\xa0Vision Transformers (ViT)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overview"}),": Developed by Google, Vision Transformers apply the transformer architecture, initially designed for NLP, to image classification."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Key Features"}),":","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Transformer encoder architecture."}),"\n",(0,r.jsx)(i.li,{children:"Scales well with large datasets and computational resources."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": General image classification and large-scale vision tasks."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"yolo---you-only-look-once",children:"YOLO - You Only Look Once"}),"\n",(0,r.jsx)(i.p,{children:"YOLO (You Only Look Once) is an object detection algorithm that uses a convolutional neural network (CNN), that's known for its speed and accuracy."}),"\n",(0,r.jsx)(i.p,{children:"Here's how YOLO works:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Grid:"}),"\xa0YOLO's CNN divides an image into a grid."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Bounding boxes:"}),"\xa0Each cell in the grid predicts a number of bounding boxes."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Class probabilities:"}),"\xa0Each cell also predicts a class probability, which indicates the likelihood of an object being present in the box."]}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"YOLO is popular because of its single-stage architecture, real-time performance, and accuracy.\xa0It's well-suited for real-time applications like self-driving cars, video surveillance, and augmented reality."}),"\n",(0,r.jsx)(i.h3,{id:"pre-trained-models-1",children:"Pre-trained models"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"YOLOv8:"}),"\xa0This model offers a variety of pretrained models for different tasks and performance needs.\xa0It's easy to use, even for those new to computer vision, machine learning, or deep learning."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"YOLOv9:"}),"\xa0The pretrained models for YOLOv9 are open-source and available on GitHub."]}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.a,{href:"https://ubiai.tools/why-yolov7-is-better-than-cnns/",children:"Why YOLOv7 is better than CNN in 2024 ?"})}),"\n",(0,r.jsx)(i.h2,{id:"differences",children:"Differences"}),"\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.img,{alt:"Differences between different pre-trained models",src:n(741954).A+"",width:"539",height:"371"})}),"\n",(0,r.jsxs)(i.table,{children:[(0,r.jsx)(i.thead,{children:(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.th,{children:"Model name"}),(0,r.jsx)(i.th,{children:"Number of parameters\xa0(Millions)"}),(0,r.jsx)(i.th,{children:"ImageNet Top 1 Accuracy"}),(0,r.jsx)(i.th,{children:"Year"})]})}),(0,r.jsxs)(i.tbody,{children:[(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"AlexNet"}),(0,r.jsx)(i.td,{children:"60 M"}),(0,r.jsx)(i.td,{children:"63.3 %"}),(0,r.jsx)(i.td,{children:"2012"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"Inception V1"}),(0,r.jsx)(i.td,{children:"5 M"}),(0,r.jsx)(i.td,{children:"69.8 %"}),(0,r.jsx)(i.td,{children:"2014"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"VGG 16"}),(0,r.jsx)(i.td,{children:"138 M"}),(0,r.jsx)(i.td,{children:"74.4 %"}),(0,r.jsx)(i.td,{children:"2014"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"VGG 19"}),(0,r.jsx)(i.td,{children:"144 M"}),(0,r.jsx)(i.td,{children:"74.5 %"}),(0,r.jsx)(i.td,{children:"2014"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"Inception V2"}),(0,r.jsx)(i.td,{children:"11.2 M"}),(0,r.jsx)(i.td,{children:"74.8 %"}),(0,r.jsx)(i.td,{children:"2015"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"ResNet-50"}),(0,r.jsx)(i.td,{children:"26 M"}),(0,r.jsx)(i.td,{children:"77.15 %"}),(0,r.jsx)(i.td,{children:"2015"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"ResNet-152"}),(0,r.jsx)(i.td,{children:"60 M"}),(0,r.jsx)(i.td,{children:"78.57 %"}),(0,r.jsx)(i.td,{children:"2015"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"Inception V3"}),(0,r.jsx)(i.td,{children:"27 M"}),(0,r.jsx)(i.td,{children:"78.8 %"}),(0,r.jsx)(i.td,{children:"2015"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"DenseNet-121"}),(0,r.jsx)(i.td,{children:"8 M"}),(0,r.jsx)(i.td,{children:"74.98 %"}),(0,r.jsx)(i.td,{children:"2016"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"DenseNet-264"}),(0,r.jsx)(i.td,{children:"22M"}),(0,r.jsx)(i.td,{children:"77.85 %"}),(0,r.jsx)(i.td,{children:"2016"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"BiT-L (ResNet)"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"928 M"})}),(0,r.jsx)(i.td,{children:"87.54 %"}),(0,r.jsx)(i.td,{children:"2019"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"NoisyStudent EfficientNet-L2"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"480 M"})}),(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.strong,{children:"88.4"}),"\xa0%"]}),(0,r.jsx)(i.td,{children:"2020"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"Meta Pseudo Labels"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"480 M"})}),(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.strong,{children:"90.2"}),"\xa0%"]}),(0,r.jsx)(i.td,{children:"2021"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"CoCa (finetuned)"}),(0,r.jsx)(i.td,{children:"2100M"}),(0,r.jsx)(i.td,{children:"91.0%"}),(0,r.jsx)(i.td,{children:"2022"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"OmniVec (ViT)"}),(0,r.jsx)(i.td,{}),(0,r.jsx)(i.td,{children:"92.4%"}),(0,r.jsx)(i.td,{children:"2023"})]})]})]}),"\n",(0,r.jsxs)(i.p,{children:["Leaderboard - ",(0,r.jsx)(i.a,{href:"https://paperswithcode.com/sota/image-classification-on-imagenet",children:"ImageNet Benchmark (Image Classification) | Papers With Code"})]}),"\n",(0,r.jsxs)(i.p,{children:["Models - ",(0,r.jsx)(i.a,{href:"https://huggingface.co/models?pipeline_tag=image-classification",children:"Models - Hugging Face"})]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.a,{href:"https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5",children:"CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more\u2026 | by Siddharth Das | Analytics Vidhya | Medium"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.a,{href:"https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96",children:"Difference between AlexNet, VGGNet, ResNet, and Inception | by Aqeel Anwar | Towards Data Science"})}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"benefits-of-pre-trained-models-for-image-classification",children:"Benefits of Pre-Trained Models for Image Classification"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Reduced Training Time:"}),"\xa0Pre-trained models significantly cut down on training time. Since they are already trained on large datasets, they only require fine-tuning for specific tasks. This efficiency allows developers to deploy models more quickly."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Improved Accuracy:"}),"\xa0These models have been trained on vast amounts of data, enabling them to generalize well. As a result, they often achieve higher accuracy on various tasks compared to models trained from scratch. This leads to more reliable image classification results."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Resource Efficiency:"}),"\xa0Using pre-trained models reduces the need for large datasets and computational power. Fine-tuning a pre-trained model requires fewer resources than training a new model, making it more accessible for organisations with limited resources."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"challenges-of-pre-trained-models-for-image-classification",children:"Challenges of Pre-Trained Models for Image Classification"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Adaptability:"}),"\xa0Fine-tuning pre-trained models to fit specific tasks can be complex. Not all models adapt well to all tasks, and sometimes extensive tweaking is required to achieve optimal performance."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Overfitting:"}),"\xa0There is a risk of overfitting, especially when fine-tuning on small datasets. The model might learn to perform well on the training data but fail to generalize to new, unseen data, reducing its effectiveness."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Complexity:"}),"\xa0Some pre-trained models have intricate architectures that are difficult to implement and modify. This complexity can pose a barrier for developers who are not familiar with advanced neural network structures, potentially hindering their use."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"links",children:"Links"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.a,{href:"https://www.geeksforgeeks.org/top-pre-trained-models-for-image-classification/",children:"Top Pre-Trained Models for Image Classification - GeeksforGeeks"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.a,{href:"https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/",children:"Top 4 Pre-Trained Models for Image Classification + Python Code"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.a,{href:"https://theaisummer.com/cnn-architectures/",children:"Best deep CNN architectures and their principles: from AlexNet to EfficientNet | AI Summer"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.a,{href:"https://jonascleveland.com/best-image-classification-models/",children:"7 Best Image Classification Models You Should Know in 2023 - Jonas Cleveland"})}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},741954:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Pasted image 20240916184828-c4395975f27a4f5b92bf6eca575653c4.jpg"},478591:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/Pasted image 20240916193555-608721dcaa4c44e28143d8aa847a30e1.jpg"},28453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>a});var s=n(296540);const r={},t=s.createContext(r);function l(e){const i=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:i},e.children)}}}]);