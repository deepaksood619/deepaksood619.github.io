"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[39555],{670148:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"databases/sql-databases/aws-redshift/deep-dive-best-practices","title":"Deep dive / Best practices","description":"Insert performance","source":"@site/docs/databases/sql-databases/aws-redshift/deep-dive-best-practices.md","sourceDirName":"databases/sql-databases/aws-redshift","slug":"/databases/sql-databases/aws-redshift/deep-dive-best-practices","permalink":"/databases/sql-databases/aws-redshift/deep-dive-best-practices","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/databases/sql-databases/aws-redshift/deep-dive-best-practices.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1709492840000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Architecture","permalink":"/databases/sql-databases/aws-redshift/architecture"},"next":{"title":"Documentation","permalink":"/databases/sql-databases/aws-redshift/documentation"}}');var i=n(474848),a=n(28453);const r={},o="Deep dive / Best practices",l={},c=[{value:"Insert performance",id:"insert-performance",level:2},{value:"Optimizations / Best practices",id:"optimizations--best-practices",level:2},{value:"Data storage, ingestion and ELT",id:"data-storage-ingestion-and-elt",level:2},{value:"Redundancy",id:"redundancy",level:3},{value:"Transactions",id:"transactions",level:3},{value:"Data ingestion: COPY statement",id:"data-ingestion-copy-statement",level:3},{value:"Best practices: COPY ingestion",id:"best-practices-copy-ingestion",level:3},{value:"Data ingestion: Amazon Redshift Spectrum",id:"data-ingestion-amazon-redshift-spectrum",level:3},{value:"Design considerations: Data ingestion",id:"design-considerations-data-ingestion",level:3},{value:"Best practices: ELT",id:"best-practices-elt",level:2}];function d(e){const s={a:"a",code:"code",del:"del",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"deep-dive--best-practices",children:"Deep dive / Best practices"})}),"\n",(0,i.jsx)(s.h2,{id:"insert-performance",children:"Insert performance"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://stackoverflow.com/questions/16485425/aws-redshift-jdbc-insert-performance",children:"https://stackoverflow.com/questions/16485425/aws-redshift-jdbc-insert-performance"})}),"\n",(0,i.jsx)(s.h2,{id:"optimizations--best-practices",children:"Optimizations / Best practices"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-redshift-announces-automatic-table-optimization",children:"https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-redshift-announces-automatic-table-optimization"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://docs.aws.amazon.com/redshift/latest/dg/c_designing-tables-best-practices.html",children:(0,i.jsx)(s.strong,{children:"https://docs.aws.amazon.com/redshift/latest/dg/c_designing-tables-best-practices.html"})})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html",children:(0,i.jsx)(s.strong,{children:"https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html"})})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=TJDtQom7SAA",children:"https://www.youtube.com/watch?v=TJDtQom7SAA"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift",children:"https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift",children:"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-preamble-prerequisites-and-prioritization",children:"https://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-preamble-prerequisites-and-prioritization"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://aws.amazon.com/blogs/big-data/improve-your-etl-performance-using-multiple-redshift-warehouses-for-writes/",children:"Improve your ETL performance using multiple Redshift warehouses for writes (Preview) | AWS Big Data Blog"})}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"data-storage-ingestion-and-elt",children:"Data storage, ingestion and ELT"}),"\n",(0,i.jsx)(s.h3,{id:"redundancy",children:"Redundancy"}),"\n",(0,i.jsx)(s.p,{children:"Amazon Redshift utilizes locally attached storage devices"}),"\n",(0,i.jsx)(s.p,{children:"Compute nodes have 2 to 3 times the advertised storage capacity"}),"\n",(0,i.jsx)(s.p,{children:"Global commit ensures all permanent tables have written blocks to another node in the cluster to ensure data redundancy"}),"\n",(0,i.jsx)(s.p,{children:"Asynchronously backup blocks to Amazon S3 - always consistent snapshot"}),"\n",(0,i.jsx)(s.p,{children:"Every 5 GB of changed data or eight hours"}),"\n",(0,i.jsx)(s.p,{children:"User on-demand manual snapshots"}),"\n",(0,i.jsx)(s.p,{children:"To disable backups at the table level: CREATE TABLE example (id int) BACKUP NO;\nTemporary tables"}),"\n",(0,i.jsx)(s.p,{children:"Blocks are not mirrored to the remote partition - two-times faster write performance"}),"\n",(0,i.jsx)(s.p,{children:"Do not trigger a full commit or backups"}),"\n",(0,i.jsx)(s.h3,{id:"transactions",children:"Transactions"}),"\n",(0,i.jsx)(s.p,{children:"Amazon Redshift is a fully transactional, ACID complaint data warehourse"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Isolation level is serializable"}),"\n",(0,i.jsx)(s.li,{children:"Two phase commits (local and global commit phases)"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Design consideration"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Because of the expense of commit overhead, limit commits by explicitly creating transactions"}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"data-ingestion-copy-statement",children:"Data ingestion: COPY statement"}),"\n",(0,i.jsx)(s.p,{children:"Ingestion throughput"}),"\n",(0,i.jsx)(s.p,{children:"Each slice's query processors can load one file at a time:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Streaming decompression"}),"\n",(0,i.jsx)(s.li,{children:"Parse"}),"\n",(0,i.jsx)(s.li,{children:"Distribute"}),"\n",(0,i.jsx)(s.li,{children:"Write"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Realizing only partial node usage at 6.25% of slices are active"}),"\n",(0,i.jsx)(s.p,{children:"Number of input files should be a multiple of the number of slices"}),"\n",(0,i.jsx)(s.p,{children:"Splitting the single file into 16 input files, all slices are working to maximize ingestion performance"}),"\n",(0,i.jsx)(s.p,{children:"COPY continues to scale linearly as you add nodes"}),"\n",(0,i.jsx)(s.p,{children:"Recommendation is to use delimited files - 1 MB to 1 GB after gzip compression"}),"\n",(0,i.jsx)(s.h3,{id:"best-practices-copy-ingestion",children:"Best practices: COPY ingestion"}),"\n",(0,i.jsx)(s.p,{children:"Delimited files are recommend"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Pick a simple delimites '|' or ',' or tabs"}),"\n",(0,i.jsx)(s.li,{children:"Pick a simple NULL character (N)"}),"\n",(0,i.jsx)(s.li,{children:"Use double quotes and an escape character ('') for varchars"}),"\n",(0,i.jsx)(s.li,{children:"UTF-8 varchar columns take four bytes per char"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Split files into a nuber that is a multiple of the total number of slices in the Amazon Redshift cluster"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.code,{children:"SELECT count(slice) from stv_slices;"})}),"\n",(0,i.jsx)(s.h3,{id:"data-ingestion-amazon-redshift-spectrum",children:"Data ingestion: Amazon Redshift Spectrum"}),"\n",(0,i.jsx)(s.p,{children:"Use INSERT INTO SELECT against external Amazon S3 tables"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Aggregate incoming data"}),"\n",(0,i.jsx)(s.li,{children:"Select subset of columns and/or rows"}),"\n",(0,i.jsx)(s.li,{children:"Manipulate incoming column data with SQL"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Best practices:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Save cluster resources for querying and reporting rather than on ELT"}),"\n",(0,i.jsx)(s.li,{children:"Filtering/aggregating incoming data can improve performance over COPY"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Design considerations"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Repeated reads against Amazon S3 are not transactional"}),"\n",(0,i.jsx)(s.li,{children:"$5/TB of (compressed) data scanned"}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"design-considerations-data-ingestion",children:"Design considerations: Data ingestion"}),"\n",(0,i.jsx)(s.p,{children:"Designed for large writes"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Batch processing system, optimized for processing massive amounts of data"}),"\n",(0,i.jsx)(s.li,{children:"1 MB size plus immutable blocks means that we clone blocks on write so as not to introduce fragmentation"}),"\n",(0,i.jsxs)(s.li,{children:["Small write (",(0,i.jsx)(s.del,{children:"1"}),"10 rows) has similar cost to a larger write (~100K rows)"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"UPDATE and DELETE"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Immutable blocks means that we only logically delete rows on UPDATE or DELETE"}),"\n",(0,i.jsx)(s.li,{children:"Must VACUUM or DEEP COPY to remove ghost rows from table"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image",src:n(778955).A+"",width:"999",height:"410"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image",src:n(486972).A+"",width:"998",height:"452"})}),"\n",(0,i.jsx)(s.p,{children:"Steps"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"Load CSV data into a staging table"}),"\n",(0,i.jsx)(s.li,{children:"Delete duplicate data from the production table"}),"\n",(0,i.jsx)(s.li,{children:"Insert (or append) data from the staging into the production table"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Create a Transaction"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image",src:n(508025).A+"",width:"954",height:"408"})}),"\n",(0,i.jsx)(s.h2,{id:"best-practices-elt",children:"Best practices: ELT"}),"\n",(0,i.jsx)(s.p,{children:"Wrap workflow/statements in an explicit transaction"}),"\n",(0,i.jsx)(s.p,{children:"Consider using DROP TABLE or TRUNCATE instead of DELETE"}),"\n",(0,i.jsx)(s.p,{children:"Staging tables:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:'Use temporary table or permanent table with the "BACKUP NO" option'}),"\n",(0,i.jsx)(s.li,{children:"If possible use DISTSTYLE KEY on both the staging table and production table to speed up the the INSERT INTO SELECT statement"}),"\n",(0,i.jsx)(s.li,{children:"Turn off automatic compression - COMPUPDATE OFF"}),"\n",(0,i.jsxs)(s.li,{children:["Copy compression settings from production table or use ANALYZE COMPRESSION statement","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Use CREATE TABLE LIKE or write encodings into the DDL"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.li,{children:"For copying a large number of rows (> hundreds of millions) consider using ALTER TABLE APPEND instead of INSERT INTO SELECT"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"VACUUM and ANALYZE"}),"\n",(0,i.jsx)(s.p,{children:"VACUUM will globally sort the table and remove rows that are marked as deleted"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"For tables with a sort key, ingestion operations will locally sort new data and write it into the unsorted region"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"ANALYZE collects table statistics for optimal query planning"}),"\n",(0,i.jsx)(s.p,{children:"Best Practices:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["VACUUM should be run only as necessary","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Typically nightly or weekly"}),"\n",(0,i.jsx)(s.li,{children:"Consider Deep Copy (recreating and copying data) for larger or wide tables"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.li,{children:"ANALYZE can be run periodically after ingestion on just the columns taht WHERE predicates are filtered on"}),"\n",(0,i.jsx)(s.li,{children:"Utility to VACUUM and ANALYZE all the tables in the cluster"}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},778955:(e,s,n)=>{n.d(s,{A:()=>t});const t=n.p+"assets/images/AWS-Redshift_Deep-dive-Best-practices-image1-6d10d6b7045220bf6d61dd0d855d240b.jpg"},486972:(e,s,n)=>{n.d(s,{A:()=>t});const t=n.p+"assets/images/AWS-Redshift_Deep-dive-Best-practices-image2-56deb6e729c2a79f2ac9605eb14fc1f0.jpg"},508025:(e,s,n)=>{n.d(s,{A:()=>t});const t=n.p+"assets/images/AWS-Redshift_Deep-dive-Best-practices-image3-a9972fa6c52e2c3ba19e26d4e49c276a.jpg"},28453:(e,s,n)=>{n.d(s,{R:()=>r,x:()=>o});var t=n(296540);const i={},a=t.createContext(i);function r(e){const s=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:s},e.children)}}}]);