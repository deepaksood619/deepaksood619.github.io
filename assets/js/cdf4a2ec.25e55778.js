"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[2705],{183013:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"ai/move-37/others","title":"Others","description":"MCMC - Markov Chain Monte Carlo","source":"@site/docs/ai/move-37/others.md","sourceDirName":"ai/move-37","slug":"/ai/move-37/others","permalink":"/ai/move-37/others","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/others.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1701793554000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Open AI Gym","permalink":"/ai/move-37/open-ai-gym"},"next":{"title":"Q-Learning Algorithms","permalink":"/ai/move-37/q-learning-algorithms"}}');var a=n(474848),i=n(28453);const o={},s="Others",c={},l=[{value:"AlphaGO",id:"alphago",level:2},{value:"Deep Q Neural Network",id:"deep-q-neural-network",level:2},{value:"Asynchronous Actor-Critic Agent",id:"asynchronous-actor-critic-agent",level:2}];function h(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"others",children:"Others"})}),"\n",(0,a.jsx)(t.p,{children:"MCMC - Markov Chain Monte Carlo"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://skymind.ai/wiki/markov-chain-monte-carlo",children:"https://skymind.ai/wiki/markov-chain-monte-carlo"})}),"\n",(0,a.jsx)(t.p,{children:"Bayesian approach"}),"\n",(0,a.jsx)(t.p,{children:"In the Bayesian approach to decision-making, you first start with the prior, this is what your beliefs are, then as data comes in, you incorporate that data to update these priors to get the posterior."}),"\n",(0,a.jsx)(t.p,{children:"Bayesian Model"}),"\n",(0,a.jsx)(t.p,{children:"A Bayesian model is a statistical model where you use probability to represent all uncertainty within the model."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://www.toptal.com/algorithms/metropolis-hastings-bayesian-inference",children:"https://www.toptal.com/algorithms/metropolis-hastings-bayesian-inference"})}),"\n",(0,a.jsx)(t.h2,{id:"alphago",children:"AlphaGO"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Policy Network"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Value Network"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Tree Search"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Q-Learning: an algorithm which produces a Q-table that an agent uses to find the best action to take given a state."}),"\n",(0,a.jsx)(t.h2,{id:"deep-q-neural-network",children:"Deep Q Neural Network"}),"\n",(0,a.jsx)(t.p,{children:"A Neural Network that takes a state and approximates Q-values for each action based on that state."}),"\n",(0,a.jsx)(t.h2,{id:"asynchronous-actor-critic-agent",children:"Asynchronous Actor-Critic Agent"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2",children:"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"})})]})}function d(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>s});var r=n(296540);const a={},i=r.createContext(a);function o(e){const t=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(i.Provider,{value:t},e.children)}}}]);