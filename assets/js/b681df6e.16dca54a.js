"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[31519],{832030:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"management/product-analytics/clustering","title":"Clustering","description":"Types of Clustering","source":"@site/docs/management/product-analytics/clustering.md","sourceDirName":"management/product-analytics","slug":"/management/product-analytics/clustering","permalink":"/management/product-analytics/clustering","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/management/product-analytics/clustering.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1740595274000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Product Analytics","permalink":"/management/product-analytics/"},"next":{"title":"Customer Analytics in Python","permalink":"/management/product-analytics/customer-analytics-in-python"}}');var t=s(474848),l=s(28453);const r={},a="Clustering",c={},o=[{value:"Types of Clustering",id:"types-of-clustering",level:2},{value:"Hierarchical Clustering",id:"hierarchical-clustering",level:2},{value:"Finding a cluster",id:"finding-a-cluster",level:2},{value:"K-means Clustering",id:"k-means-clustering",level:2},{value:"Within Cluster Sum of Squares (WCSS) is used to determine best clustering solution",id:"within-cluster-sum-of-squares-wcss-is-used-to-determine-best-clustering-solution",level:2}];function d(e){const n={a:"a",code:"code",del:"del",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"clustering",children:"Clustering"})}),"\n",(0,t.jsx)(n.h2,{id:"types-of-clustering",children:"Types of Clustering"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Hierarchical"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Flat"})}),"\n",(0,t.jsx)(n.li,{children:"Flat clustering are way faster than hierarchical"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hierarchical-clustering",children:"Hierarchical Clustering"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ex - Taxonomy of Animal kingdom"}),"\n",(0,t.jsxs)(n.li,{children:["Types of Hierarchical clustering","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Divisive (Top down)"}),"\n",(0,t.jsx)(n.li,{children:"Agglomerative (Bottom up) (easier to solve mathematically)"}),"\n",(0,t.jsx)(n.li,{children:"Should reach same results"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["How do we measure the distances between observations?","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Euclidean distance"}),"\n",(0,t.jsx)(n.li,{children:"Manhattan distance"}),"\n",(0,t.jsx)(n.li,{children:"Maximum distance"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:s(7033).A+"",width:"999",height:"1058"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Segmentation between clusters","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Ward method (|A-B|^2)/n",(0,t.jsx)(n.del,{children:"scale"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Hierarchical clustering with the Sci Py library. We'll use the dendrogram and linkage modules.\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Perform Hierarchical Clustering. The results are returned as a linkage matrix.\nhier_clust = linkage(segmentation_std, method = 'ward')\n\n# We plot the results from the Hierarchical Clustering using a Dendrogram.\n# We truncate the dendrogram for better readability. The level p shows only the last p merged clusters\n# We also omit showing the labels for each point.\nplt.figure(figsize = (12,9))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Observations')\nplt.ylabel('Distance')\ndendrogram(hier_clust,\n            truncate_mode = 'level',\n            p = 5,\n            show_leaf_counts = False,\n            no_labels = True)\nplt.show()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"finding-a-cluster",children:"Finding a cluster"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"We need to find a horizontal line on the dendogram on which to cut"}),"\n",(0,t.jsx)(n.li,{children:"Rule of thumb: We find the longest vertical line unintercepted by a horizontal line from the dendrogram. This is where we should make the cut"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"k-means-clustering",children:"K-means Clustering"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Most commonly used method in clustering because of its simplicity"}),"\n",(0,t.jsx)(n.li,{children:"K-means is perfect for segmentation data"}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Choose the number of clusters we want to identify. K in K-means means number of clusters"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Specify cluster seeds"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Based on Euclidean squared distance add points to one of the cluster"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Calculate the centroid (geometrical center)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Repeat until the centroids stop changing"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Problems"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The squared Euclidean distance is quite sensitive to outliers (solved by k-median clustering)"}),"\n",(0,t.jsx)(n.li,{children:"Choose number of clusters before hand"}),"\n",(0,t.jsx)(n.li,{children:"K-means enforces spherical clusters"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://clickhouse.com/blog/kmeans-clustering-with-clickhouse",children:"How to Scale K-Means Clustering with just ClickHouse SQL"})}),"\n",(0,t.jsx)(n.h2,{id:"within-cluster-sum-of-squares-wcss-is-used-to-determine-best-clustering-solution",children:"Within Cluster Sum of Squares (WCSS) is used to determine best clustering solution"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:s(136138).A+"",width:"999",height:"478"})}),"\n",(0,t.jsx)(n.p,{children:"Choosing number of clusters - Elbow method"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:s(891975).A+"",width:"842",height:"650"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:s(154520).A+"",width:"998",height:"562"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:s(762389).A+"",width:"998",height:"562"})})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},7033:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/Customer-Analytics-in-Python_Intro-image3-964e98f6d8a809f671c6e1117c93e7fc.jpg"},136138:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/Customer-Analytics-in-Python_Intro-image4-223a2d5ba77a722eafebfe86957fe5b4.jpg"},891975:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/Customer-Analytics-in-Python_Intro-image5-b53e683fdc946ad04b1ac6746c2f625f.jpg"},154520:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/Customer-Analytics-in-Python_Intro-image6-56fcabb088d4aa7f63cb26bb6375f9dd.jpg"},762389:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/Customer-Analytics-in-Python_Intro-image7-35fe6c9afcae2b0f14d1a91d052a8f1a.jpg"},28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(296540);const t={},l=i.createContext(t);function r(e){const n=i.useContext(l);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);