"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[67762],{108210:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var t=i(785893),a=i(511151);const o={},r="2. Dynamic Programming",s={id:"ai/move-37/2-dynamic-programming",title:"2. Dynamic Programming",description:"Sports Betting",source:"@site/docs/ai/move-37/2-dynamic-programming.md",sourceDirName:"ai/move-37",slug:"/ai/move-37/2-dynamic-programming",permalink:"/ai/move-37/2-dynamic-programming",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/2-dynamic-programming.md",tags:[],version:"current",lastUpdatedAt:1701846168,formattedLastUpdatedAt:"Dec 6, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"1. Markov Decision Process",permalink:"/ai/move-37/1-markov-decision-process"},next:{title:"3. Monte Carlo Methods",permalink:"/ai/move-37/3-monte-carlo-methods"}},c={},l=[{value:"Sports Betting",id:"sports-betting",level:2},{value:"Bellman Advanced",id:"bellman-advanced",level:2},{value:"Dynamic Programming",id:"dynamic-programming",level:2},{value:"Key Points",id:"key-points",level:2}];function m(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"2-dynamic-programming",children:"2. Dynamic Programming"}),"\n",(0,t.jsx)(n.h2,{id:"sports-betting",children:"Sports Betting"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:i(306310).Z+"",width:"1100",height:"688"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Dynamic programming refers to the collection of algorithms that can be used to compute optimal policies given a perfect model of the environment."}),"\n",(0,t.jsx)(n.li,{children:"Value iteration is a type of dynamic programming algorithm that computes the optimal value function and consequently the optimal policy"}),"\n",(0,t.jsx)(n.li,{children:"Dynamic programming is useful, but limited because it requires a perfect environment model and is computationally expensive"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"bellman-advanced",children:"Bellman Advanced"}),"\n",(0,t.jsx)(n.p,{children:"Stochastic bellman equation, where each state transition has some probability for performing an action."}),"\n",(0,t.jsx)(n.p,{children:"Ex - If we tell our RL agent to move left or right, it has 80% chance of moving left or right but 20% chance of moving up and down and vice-versa."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:i(784229).Z+"",width:"1099",height:"232"})}),"\n",(0,t.jsx)(n.h2,{id:"dynamic-programming",children:"Dynamic Programming"}),"\n",(0,t.jsx)(n.p,{children:"Create a lookup table to solve Markov Decision Processes in stochastic environments."}),"\n",(0,t.jsx)(n.p,{children:"Dynamic Programming Algorithms -"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Policy Iteration - starts with a totally random policy and start taking actions, it then start estimating values for each square based on the reward received from this random actions updating the value table and using improved values to calculate and improve policy. This continues until both the value table stablizes and stops changing (algorithm converges)"}),"\n",(0,t.jsx)(n.li,{children:"Value Iteration - completly ignores policy and focuses on applying bellman equation"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:i(65168).Z+"",width:"1100",height:"582"})}),"\n",(0,t.jsx)(n.p,{children:"3 Parts -"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Policy Evaluation"}),"\n",(0,t.jsx)(n.li,{children:"Policy Iteration"}),"\n",(0,t.jsx)(n.li,{children:"Value Iteration"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-points",children:"Key Points"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Policy iteration"})," includes: ",(0,t.jsx)(n.strong,{children:"policy evaluation"}),"+",(0,t.jsx)(n.strong,{children:"policy improvement"}),", and the two are repeated iteratively until policy converges."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Value iteration"})," includes: ",(0,t.jsx)(n.strong,{children:"finding optimal value function"})," + one ",(0,t.jsx)(n.strong,{children:"policy extraction"}),". There is no repeat of the two because once the value function is optimal, then the policy out of it should also be optimal (i.e. converged)."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Finding optimal value function"})," can also be seen as a combination of policy improvement (due to max) and truncated policy evaluation (the reassignment of v_(s) after just one sweep of all states regardless of convergence)."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The algorithms fo ",(0,t.jsx)(n.strong,{children:"policy evaluation"})," and ",(0,t.jsx)(n.strong,{children:"finding optimal value function"})," are highly similar except for a max operation (as highlighted)"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Similarly, the key step to ",(0,t.jsx)(n.strong,{children:"policy improvement"})," and ",(0,t.jsx)(n.strong,{children:"policy extraction"})," are identical except the former involves a stability check."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["In my experience,",(0,t.jsx)(n.em,{children:"policy iteration"}),"is faster than",(0,t.jsx)(n.em,{children:"value iteration"}),", as a policy converges more quickly than a value function."]}),"\n",(0,t.jsx)(n.p,{children:"Why Discount Factor?"}),"\n",(0,t.jsxs)(n.p,{children:["The idea of using discount factor is to prevent the total reward from going to infinity (because ",(0,t.jsx)(n.code,{children:"0 <= \u03b3 <= 1"}),"). It also models the agent behavior when the agent prefers immediate rewards than rewards that are potentially received later in the future."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:i(504988).Z+"",width:"1030",height:"364"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:i(316988).Z+"",width:"1100",height:"511"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/dennybritz/reinforcement-learning/tree/master/DP",children:"https://github.com/dennybritz/reinforcement-learning/tree/master/DP"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919",children:"https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919"})})]})}function d(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},306310:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/2.-Dynamic-Programming-image1-f4125024d34dac26e99846af72412a8c.jpg"},784229:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/2.-Dynamic-Programming-image2-82dc2bf3fa1cbb875a2eef4b6e615215.jpg"},65168:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/2.-Dynamic-Programming-image3-14a78a6305b80aa791c97efca10fb04e.jpg"},504988:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/2.-Dynamic-Programming-image4-bd71738301b86a9b9f2ae4e4ee570bf6.jpg"},316988:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/2.-Dynamic-Programming-image5-0d3d2a6e789d436fe883ddfb8e4796a0.jpg"},511151:(e,n,i)=>{i.d(n,{Z:()=>s,a:()=>r});var t=i(667294);const a={},o=t.createContext(a);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);