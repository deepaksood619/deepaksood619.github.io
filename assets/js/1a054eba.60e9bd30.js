"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[66522],{423578:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"technologies/apache/others","title":"Others","description":"Apache Tez","source":"@site/docs/technologies/apache/others.md","sourceDirName":"technologies/apache","slug":"/technologies/apache/others","permalink":"/technologies/apache/others","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache/others.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1739996367000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Data Pipeline Architecture","permalink":"/technologies/apache/data-pipeline-architecture"},"next":{"title":"Airflow","permalink":"/technologies/apache-airflow/"}}');var n=r(474848),t=r(28453);const s={},o="Others",l={},c=[{value:"Apache Tez",id:"apache-tez",level:2},{value:"Apache Spark",id:"apache-spark",level:2},{value:"Features",id:"features",level:3},{value:"Apache Superset",id:"apache-superset",level:2},{value:"Features",id:"features-1",level:3},{value:"Apache Beam",id:"apache-beam",level:2},{value:"ParDo",id:"pardo",level:3},{value:"Apache Storm",id:"apache-storm",level:2},{value:"Apache POI",id:"apache-poi",level:2}];function h(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"others",children:"Others"})}),"\n",(0,n.jsx)(a.h2,{id:"apache-tez",children:"Apache Tez"}),"\n",(0,n.jsxs)(a.p,{children:["The Apache TEZ project is aimed at building an application framework which allows for a complex directed-acyclic-graph of tasks for processing data. It is currently built atop ",(0,n.jsx)(a.a,{href:"http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html",children:"Apache Hadoop YARN"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"The 2 main design themes for Tez are:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Empowering end users by:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Expressive dataflow definition APIs"}),"\n",(0,n.jsx)(a.li,{children:"Flexible Input-Processor-Output runtime model"}),"\n",(0,n.jsx)(a.li,{children:"Data type agnostic"}),"\n",(0,n.jsx)(a.li,{children:"Simplifying deployment"}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Execution Performance"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Performance gains over Map Reduce"}),"\n",(0,n.jsx)(a.li,{children:"Optimal resource management"}),"\n",(0,n.jsx)(a.li,{children:"Plan reconfiguration at runtime"}),"\n",(0,n.jsx)(a.li,{children:"Dynamic physical data flow decisions"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"http://tez.apache.org",children:"http://tez.apache.org"})}),"\n",(0,n.jsx)(a.h2,{id:"apache-spark",children:"Apache Spark"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Sparkis an ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Open-source_software",children:"open-source"})," distributed general-purpose ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Cluster_computing",children:"cluster-computing"}),(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Software_framework",children:"framework"}),". Spark provides an ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Application_programming_interface",children:"interface"})," for programming entire clusters with implicit ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Data_parallelism",children:"data parallelism"})," and ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Fault_tolerance",children:"fault tolerance"})]}),"\n",(0,n.jsx)(a.h3,{id:"features",children:"Features"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Scatter/gather paradigm (similar to MapReduce)"}),"\n",(0,n.jsx)(a.li,{children:"More general data model (RDDs, DataSets)"}),"\n",(0,n.jsx)(a.li,{children:"More general programming model (transform/action)"}),"\n",(0,n.jsx)(a.li,{children:"Storage agnostic"}),"\n",(0,n.jsx)(a.li,{children:"Faster version of MapReduce(does all the mapreduce in-memory)"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Apache_Spark",children:"https://en.wikipedia.org/wiki/Apache_Spark"})}),"\n",(0,n.jsx)(a.h2,{id:"apache-superset",children:"Apache Superset"}),"\n",(0,n.jsx)(a.p,{children:"Apache Superset is an open-source modern data exploration and visualization platform."}),"\n",(0,n.jsx)(a.p,{children:"Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts."}),"\n",(0,n.jsx)(a.h3,{id:"features-1",children:"Features"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"A rich set of data visualizations"}),"\n",(0,n.jsx)(a.li,{children:"An easy-to-use interface for exploring and visualizing data"}),"\n",(0,n.jsx)(a.li,{children:"Create and share dashboards"}),"\n",(0,n.jsx)(a.li,{children:"Enterprise-ready authentication with integration with major authentication providers (database, OpenID, LDAP, OAuth & REMOTE_USER through Flask AppBuilder)"}),"\n",(0,n.jsx)(a.li,{children:"An extensible, high-granularity security/permission model allowing intricate rules on who can access individual features and the dataset"}),"\n",(0,n.jsx)(a.li,{children:"A simple semantic layer, allowing users to control how data sources are displayed in the UI by defining which fields should show up in which drop-down and which aggregation and function metrics are made available to the user"}),"\n",(0,n.jsx)(a.li,{children:"Integration with most SQL-speaking RDBMS through SQLAlchemy"}),"\n",(0,n.jsx)(a.li,{children:"Deep integration with Druid.io"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://superset.apache.org/",children:"Welcome | Superset"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://superset.apache.org/docs/intro/",children:"Introduction | Superset"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://balachandar-paulraj.medium.com/can-superset-or-redash-replace-tableau-c333e017bc87",children:"Can Superset or Redash replace Tableau? | by Balachandar Paulraj | Medium"})}),"\n",(0,n.jsx)(a.h2,{id:"apache-beam",children:"Apache Beam"}),"\n",(0,n.jsx)(a.p,{children:"An advanced unified programming model"}),"\n",(0,n.jsx)(a.p,{children:"Implement batch and streaming data processing jobs that run on any execution engine."}),"\n",(0,n.jsx)(a.h3,{id:"pardo",children:"ParDo"}),"\n",(0,n.jsxs)(a.p,{children:["Overall, there are five core transforms in the Apache Beam model. ",(0,n.jsx)(a.strong,{children:(0,n.jsx)(a.code,{children:"ParDo"})})," is one of the most commonly used transform functions. ",(0,n.jsx)(a.code,{children:"ParDo"})," works similar to the map phase of the ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/MapReduce",children:"map-reduce"})," algorithm."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"ParDo"})," is the transform for parallel processing. It applies the processing function to every element in the ",(0,n.jsx)(a.code,{children:"PCollection"})," input and returns zero or more elements to the output ",(0,n.jsx)(a.code,{children:"PCollection"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["Using the ",(0,n.jsx)(a.code,{children:"ParDo"})," function requires a user-defined ",(0,n.jsx)(a.code,{children:"DoFn."})," This function will have the transformation you plan to apply. ",(0,n.jsx)(a.code,{children:"DoFn"})," is a Beam SDK class that describes a distributed processing function."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://www.educative.io/answers/what-is-pardo-transform-in-apache-beam",children:"What is ParDo transform in Apache Beam?"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://beam.apache.org/documentation/transforms/python/elementwise/pardo/",children:"ParDo"})}),"\n",(0,n.jsx)(a.h2,{id:"apache-storm",children:"Apache Storm"}),"\n",(0,n.jsx)(a.p,{children:"Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language."}),"\n",(0,n.jsx)(a.p,{children:"Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Storm is fast: a benchmark clocked it at overa million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate."}),"\n",(0,n.jsx)(a.p,{children:"Storm integrates with the queueing and database technologies you already use. A Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"http://storm.apache.org",children:"http://storm.apache.org"})}),"\n",(0,n.jsx)(a.h2,{id:"apache-poi",children:"Apache POI"}),"\n",(0,n.jsx)(a.p,{children:"Apache POI is an open-source java library to create and manipulate various file formats based on Microsoft Office. Using POI, one should be able to perform create, modify and display/read operations on the following file formats. For Example, Java doesn\u2019t provide built-in support for working with excel files, so we need to look for open-source APIs for the job."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://www.geeksforgeeks.org/apache-poi-introduction/",children:"Introduction to Apache POI - GeeksforGeeks"})})]})}function d(e={}){const{wrapper:a}={...(0,t.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},28453:(e,a,r)=>{r.d(a,{R:()=>s,x:()=>o});var i=r(296540);const n={},t=i.createContext(n);function s(e){const a=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),i.createElement(t.Provider,{value:a},e.children)}}}]);