"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[4642],{18022:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ai/move-37/syllabus","title":"Syllabus","description":"1. Markov Decision Processes","source":"@site/docs/ai/move-37/syllabus.md","sourceDirName":"ai/move-37","slug":"/ai/move-37/syllabus","permalink":"/ai/move-37/syllabus","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/syllabus.md","tags":[],"version":"current","lastUpdatedBy":"Deepak Sood","lastUpdatedAt":1678191863000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning","permalink":"/ai/move-37/reinforcement-learning"},"next":{"title":"NLP","permalink":"/ai/nlp/"}}');var l=i(474848),r=i(28453);const t={},o="Syllabus",c={},d=[];function a(n){const e={h1:"h1",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,r.R)(),...n.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.header,{children:(0,l.jsx)(e.h1,{id:"syllabus",children:"Syllabus"})}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Markov Decision Processes"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Introduction"}),"\n",(0,l.jsx)(e.li,{children:"Sensor Networks"}),"\n",(0,l.jsx)(e.li,{children:"Supply Chain Management"}),"\n",(0,l.jsx)(e.li,{children:"Energy Efficiency"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (Policy Functions)"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (Bellman Equation)"}),"\n",(0,l.jsx)(e.li,{children:"Markov Decision Processes"}),"\n",(0,l.jsx)(e.li,{children:"The Bellman Equations"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Dynamic Programming"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Route Planning"}),"\n",(0,l.jsx)(e.li,{children:"Options Pricing"}),"\n",(0,l.jsx)(e.li,{children:"Scheduling"}),"\n",(0,l.jsx)(e.li,{children:"Operating Systems"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (History of DP)"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (Value Iteration)"}),"\n",(0,l.jsx)(e.li,{children:"Dynamic Programming"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Monte Carlo Methods"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Medical Diagnosis"}),"\n",(0,l.jsx)(e.li,{children:"Network Routing Optimization"}),"\n",(0,l.jsx)(e.li,{children:"Physics Research"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (Exploration vs Exploitation)"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (Greedy Policies)"}),"\n",(0,l.jsx)(e.li,{children:"MC Prediction and MC Control"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Model Free Learning"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Delivery Management"}),"\n",(0,l.jsx)(e.li,{children:"Automated Trading"}),"\n",(0,l.jsx)(e.li,{children:"Backgammon"}),"\n",(0,l.jsx)(e.li,{children:"Dopamine in Neuroscience"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (SARSA)"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (Q Learning)"}),"\n",(0,l.jsx)(e.li,{children:"Temporal Difference Learning"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"RL in Continuous Spaces"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Self Driving Cars"}),"\n",(0,l.jsx)(e.li,{children:"Delivery Drones"}),"\n",(0,l.jsx)(e.li,{children:"Rescue Robots"}),"\n",(0,l.jsx)(e.li,{children:"Assembly Robots"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (Control Theory)"}),"\n",(0,l.jsx)(e.li,{children:"Midterm Assignment (Make a Bipedal Robot Walk )"}),"\n",(0,l.jsx)(e.li,{children:"Continuous Space Techniques"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Deep Reinforcement Learning"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Traffic Optimization"}),"\n",(0,l.jsx)(e.li,{children:"Gaming"}),"\n",(0,l.jsx)(e.li,{children:"Meta Learning"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (Deep Q Learning)"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (DQN Improvements)"}),"\n",(0,l.jsx)(e.li,{children:"The Evolution of Deep Q Learning"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Policy Based Methods"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Web System Configuration"}),"\n",(0,l.jsx)(e.li,{children:"Text Summarization"}),"\n",(0,l.jsx)(e.li,{children:"AI Assisted Design"}),"\n",(0,l.jsx)(e.li,{children:"Portfolio Optimization"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (Evolutionary Algorithms)"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (REINFORCE)"}),"\n",(0,l.jsx)(e.li,{children:"Stochastic Policy Search"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Policy Gradient Methods"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Dialogue Systems"}),"\n",(0,l.jsx)(e.li,{children:"Photo Editing"}),"\n",(0,l.jsx)(e.li,{children:"Language Translation"}),"\n",(0,l.jsx)(e.li,{children:"Tutoring Systems"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (Evolved Policy Gradients)"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (TRPO)"}),"\n",(0,l.jsx)(e.li,{children:"Generalized Advatange Estimation"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Actor Critic Methods"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Advanced Trading Techniques"}),"\n",(0,l.jsx)(e.li,{children:"Human-Machine Cooperation"}),"\n",(0,l.jsx)(e.li,{children:"Insurance Cost Analysis"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (Actor Critic Algorithms)"}),"\n",(0,l.jsx)(e.li,{children:"Homework Assignment (Bayesian Actor Critic)"}),"\n",(0,l.jsx)(e.li,{children:"Asynchronous Advantage Actor Critic"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:"Multi Agent RL"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Move 37"}),"\n",(0,l.jsx)(e.li,{children:"Transporation Networks"}),"\n",(0,l.jsx)(e.li,{children:"Decentralized Autonomous Organizations"}),"\n",(0,l.jsx)(e.li,{children:"The Future of AI"}),"\n",(0,l.jsx)(e.li,{children:"Reading Assignment (Cooperative Agents)"}),"\n",(0,l.jsx)(e.li,{children:"Inverse Reinforcement Learning"}),"\n",(0,l.jsx)(e.li,{children:"Final Project (Multi Agent Research Project)"}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,l.jsx)(e,{...n,children:(0,l.jsx)(a,{...n})}):a(n)}},28453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>o});var s=i(296540);const l={},r=s.createContext(l);function t(n){const e=s.useContext(r);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(l):n.components||l:t(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);