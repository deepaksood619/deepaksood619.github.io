"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[21166],{610203:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"mathematics/probability/intro-to-probability/14.-intro-to-bayesian-inference","title":"14. Intro to Bayesian Inference","description":"The power of Bayesian statistics","source":"@site/docs/mathematics/probability/intro-to-probability/14.-intro-to-bayesian-inference.md","sourceDirName":"mathematics/probability/intro-to-probability","slug":"/mathematics/probability/intro-to-probability/14.-intro-to-bayesian-inference","permalink":"/mathematics/probability/intro-to-probability/14.-intro-to-bayesian-inference","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/mathematics/probability/intro-to-probability/14.-intro-to-bayesian-inference.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1707138374000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"13. Conditional expectation and variance revisited","permalink":"/mathematics/probability/intro-to-probability/13.-conditional-expectation-and-variance-revisited"},"next":{"title":"2. Conditioning and Independence","permalink":"/mathematics/probability/intro-to-probability/2.-conditioning-and-independence"}}');var a=n(474848),s=n(28453);const o={},r="14. Intro to Bayesian Inference",c={},l=[{value:"The power of Bayesian statistics",id:"the-power-of-bayesian-statistics",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"14-intro-to-bayesian-inference",children:"14. Intro to Bayesian Inference"})}),"\n",(0,a.jsx)(t.h2,{id:"the-power-of-bayesian-statistics",children:"The power of Bayesian statistics"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Outcome characterization - This is the distribution of things that happened"}),"\n",(0,a.jsx)(t.li,{children:"Latent factor analysis - These are the things affect your outcome"}),"\n",(0,a.jsx)(t.li,{children:"Decision making - Given all the potential outcomes here's the most optimal choice we should make today"}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://www.youtube.com/watch?v=pJH_2y9J9-I",children:"https://www.youtube.com/watch?v=pJH_2y9J9-I"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"We apply the Bayes rule to find the posterior distribution of an unknown random variable given one or multiple observations of related random variables."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"We discuss the most common methods for coming up with a point estimate of the unknown random variable (Maximum a Posteriori probability estimate, Least Mean Squares estimate, and Linear Least Mean Squares estimate)."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"We consider the question of performance analysis, namely, the calculation of the probability of error in hypothesis testing problems or the calculation of the mean squared error in estimation problems."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"To illustrate the methodology, we pay special attention to a few canonical problems such as linear normal models and the problem of estimating the unknown bias of a coin."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(488686).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(768953).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(782076).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(899975).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(932234).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(524309).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(103640).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(761251).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"image",src:n(387494).A+"",width:"1755",height:"987"})}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://www.youtube.com/watch?v=HZGCoVF3YvM",children:"Bayes theorem, and making probability intuitive"}),"- Bayes' theorem describe the probability of an event occurring, based upon prior knowledge of other variables related to that event"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"In effect, it is a conditional probability, with the probability of an event conditioned on the information/knowledge you have"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Since the information/knowledge that different individuals can have about an event can vary, Bayes' thorem allows for differences in probability estimates for the same event across individuals"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"In Bayesian Inference, you update the probability of an event happening as you receive new evidence or information"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"The probability that you assign to an event before you receive the new information represent your priors"}),"\n",(0,a.jsx)(t.li,{children:"The probability that you assign to that same event after receiving and processing new information represent your posterior estimate"}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},488686:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image1-8b988724e8fdbfd6937f8d2c6ddd0dfc.jpg"},768953:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image2-195b4a89d5837b668a2ea003cd0c5d5d.jpg"},782076:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image3-982c3c3a36e7c5a0b8830e8176df0290.jpg"},899975:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image4-b27690b9668f06b2d32b134cc52d19b7.jpg"},932234:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image5-2e80adbe3bdec1c160b436920bd0e4d3.jpg"},524309:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image6-c371c3eeaae3a4c7954f3261b56f5a59.jpg"},103640:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image7-19a3906525edab9683d6fc0d2580252d.jpg"},761251:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image8-9acdf54431eb6112669bd6d5fd837a23.jpg"},387494:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/Intro-Syllabus_14.-Intro-to-Bayesian-Inference-image9-26cac48841c8adb1f23e420dc12dfe9e.jpg"},28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var i=n(296540);const a={},s=i.createContext(a);function o(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);