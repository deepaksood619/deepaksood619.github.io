"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[40325],{456871:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>o});const a=JSON.parse('{"id":"ai/move-37/4-model-free-learning","title":"4. Model Free Learning","description":"image","source":"@site/docs/ai/move-37/4-model-free-learning.md","sourceDirName":"ai/move-37","slug":"/ai/move-37/4-model-free-learning","permalink":"/ai/move-37/4-model-free-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/4-model-free-learning.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1701793554000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"3. Monte Carlo Methods","permalink":"/ai/move-37/3-monte-carlo-methods"},"next":{"title":"5. RL in Continuous Space","permalink":"/ai/move-37/5-rl-in-continuous-space"}}');var s=i(474848),t=i(28453);const r={},l="4. Model Free Learning",d={},o=[{value:"Model Based vs Model Free learning",id:"model-based-vs-model-free-learning",level:2},{value:"Episodic and Continuous Task",id:"episodic-and-continuous-task",level:2},{value:"Difference b/w MC andTemporal Difference (TD)",id:"difference-bw-mc-andtemporal-difference-td",level:2},{value:"Monte-Carlo",id:"monte-carlo",level:2},{value:"Temporal Difference (TD)",id:"temporal-difference-td",level:2},{value:"Summary",id:"summary",level:2},{value:"Temporal Difference Learning",id:"temporal-difference-learning",level:2},{value:"Q-Learning",id:"q-learning",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"4-model-free-learning",children:"4. Model Free Learning"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(941674).A+"",width:"999",height:"842"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(92661).A+"",width:"999",height:"948"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(301752).A+"",width:"998",height:"156"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(124587).A+"",width:"995",height:"151"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(498126).A+"",width:"999",height:"718"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(154265).A+"",width:"1000",height:"533"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(556892).A+"",width:"998",height:"546"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(184639).A+"",width:"999",height:"647"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(364642).A+"",width:"1000",height:"1078"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Associative Learning is a learning process in which a new response becomse associated with a particular stimulus"}),"\n",(0,s.jsx)(n.li,{children:"When we build mathematical models of learning, we can use distributions instead of single values to help represent uncertainity about the world"}),"\n",(0,s.jsx)(n.li,{children:"Temperal difference learning is a model free learning technique that predicts the expected value of a variable occuring at the end of a sequence of states"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"model-based-vs-model-free-learning",children:"Model Based vs Model Free learning"}),"\n",(0,s.jsx)(n.p,{children:"A model is basically a plan for our agent. When we have a set of defined state transition probabilities, we call that working with a model. Reinforcement learning can be applied with or without a model, or even used to define a model."}),"\n",(0,s.jsx)(n.p,{children:"A complete model of the environment is required to do Dynamic Programming. If our agent doesn't have a complete map of what to expect, we can instead employ what is called model-free learning. In layman's terms, this is learning by trial and error."}),"\n",(0,s.jsx)(n.p,{children:"For some board games such as chess and go, although we can accurately model the environment's dynamics, computational power constrains us from calculating the Bellman optimality equation. This is where model-free learning methods shine. We handle this situation by optimizing for a smaller subset of states that are frequently encountered, at the cost of knowing less about the infrequently visited states."}),"\n",(0,s.jsx)(n.h2,{id:"episodic-and-continuous-task",children:"Episodic and Continuous Task"}),"\n",(0,s.jsx)(n.p,{children:"In RL problems we have two different tasks in nature"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Episodic Task"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A task which can last a finite amount of time is calledEpisodic task (an episode)"}),"\n",(0,s.jsx)(n.li,{children:"Ex: Playing a game of chess (win or lose or draw)"}),"\n",(0,s.jsx)(n.li,{children:"we only get the reward at the end of the task or another option is to distribute the reward evenly across all actions taken in that episode."}),"\n",(0,s.jsx)(n.li,{children:"Ex: you lost the queen (-10 points), you lost one of the rooks (-5 points) etc.."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Continuous Task"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A task which never ends is calledContinuous task"}),"\n",(0,s.jsx)(n.li,{children:"Ex: Trading in the cryptocurrency markets or learning Machine learning on internet."}),"\n",(0,s.jsx)(n.li,{children:"In this, rewards may be given with discounting with a discount factor \u03bb\u2208[0,1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"difference-bw-mc-andtemporal-difference-td",children:"Difference b/w MC andTemporal Difference (TD)"}),"\n",(0,s.jsx)(n.h2,{id:"monte-carlo",children:"Monte-Carlo"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"it only works for episodic tasks"}),"\n",(0,s.jsx)(n.li,{children:"it can only learn from complete sequences"}),"\n",(0,s.jsx)(n.li,{children:"it has to wait until the end of the episode to get the reward"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"temporal-difference-td",children:"Temporal Difference (TD)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"it only for both episodic and continuous tasks"}),"\n",(0,s.jsx)(n.li,{children:"it can learn from incomplete sequences"}),"\n",(0,s.jsx)(n.li,{children:"it will only wait until the next time step to update the value estimates."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"We use model free algorithms when Transition probability P (dynamics of the system)is not given in MDP."}),"\n",(0,s.jsx)(n.li,{children:"Monte Carlo takes the means of episodes to calculate the V and Q values for both prediction and control tasks."}),"\n",(0,s.jsx)(n.li,{children:"TD combines the features of both DP and MC by learning through interacting with the environment with bootstrapping."}),"\n",(0,s.jsx)(n.li,{children:"We use epsilon greedy policy to avoid the exploration problem in the env."}),"\n",(0,s.jsx)(n.li,{children:"Sarsa updates the Q value by choosing the current action and next action using the same policy, making it an on policy method."}),"\n",(0,s.jsx)(n.li,{children:"Q-learning updates Q values by acting greedily on the environment while following another policy, making it an off policy method."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"temporal-difference-learning",children:"Temporal Difference Learning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Conditioned stimulus - conditioned response (dog shown food + blowing whistle analogy)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(452360).A+"",width:"998",height:"430"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(96325).A+"",width:"1000",height:"653"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Find the Utility matrix using the above update rule"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(392122).A+"",width:"999",height:"502"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(379639).A+"",width:"999",height:"700"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(673324).A+"",width:"1000",height:"637"})}),"\n",(0,s.jsx)(n.h2,{id:"q-learning",children:"Q-Learning"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(828585).A+"",width:"999",height:"571"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(48990).A+"",width:"1000",height:"601"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:i(533947).A+"",width:"998",height:"545"})}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4",children:"https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4"})}),"\n",(0,s.jsxs)(n.li,{children:["Reinforcement Learning: An Introduction, by ",(0,s.jsx)(n.a,{href:"http://incompleteideas.net/index.html",children:"Richard S. Sutton"})," and ",(0,s.jsx)(n.a,{href:"http://www-anw.cs.umass.edu/~barto/",children:"Andrew G. Barto"})]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://bair.berkeley.edu/blog/2018/04/26/tdm",children:"https://bair.berkeley.edu/blog/2018/04/26/tdm"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},941674:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image1-c26a5dd43bb1ba4189247ecce8a1c4e7.jpg"},452360:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image10-b30a91b9e56c8ff2435eed744a6fb1cd.jpg"},96325:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image11-39a3018f229d0a2f270b472c9ef9bfe8.jpg"},392122:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image12-f962af0ddfceb18bcdb3d5b7660b540c.jpg"},379639:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image13-3af9f3c9d1095561ec133b4ec43f04b9.jpg"},673324:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image14-ce0c61655b8cc41a2ce2818f4fdc4abf.jpg"},828585:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image15-bd993c7c5e5235dfead108f2e04dc35e.jpg"},48990:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image16-373c9745165ffadd6a55df33157caafb.jpg"},533947:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image17-d64e95a1c876996451538b6b783a14b4.jpg"},92661:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image2-99141c21a680b3275941ba015f64b8de.jpg"},301752:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image3-b383f67303a2ebc4ff0d2a73e5122f33.jpg"},124587:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image4-83284946f5f9de61129c18d65b1ddf0d.jpg"},498126:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image5-7b5bee89cca079d63e9892206675cbde.jpg"},154265:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image6-d24be2889f5d0bc796412feedb9ca1f1.jpg"},556892:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image7-ebba9ed23e0941802dbecba7d1cb1927.jpg"},184639:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image8-fd8016ca91e647769eb2bf6e3d911c03.jpg"},364642:(e,n,i)=>{i.d(n,{A:()=>a});const a=i.p+"assets/images/4.-Model-Free-Learning-image9-6e831f1d9e191263ddad28324cd16b36.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var a=i(296540);const s={},t=a.createContext(s);function r(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);