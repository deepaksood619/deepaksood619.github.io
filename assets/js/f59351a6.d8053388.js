"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[85555],{414553:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"courses/attention-in-transformers","title":"Attention in Transformers","description":"Attention in Transformers: Concepts and Code in PyTorch.","source":"@site/docs/courses/attention-in-transformers.md","sourceDirName":"courses","slug":"/courses/attention-in-transformers","permalink":"/courses/attention-in-transformers","draft":false,"unlisted":false,"editUrl":"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/courses/attention-in-transformers.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1746292599000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"365 DS - Mathematics","permalink":"/courses/365-ds-mathematics"},"next":{"title":"AWS Certified Cloud Practitioner (CCP)","permalink":"/courses/aws-certified-cloud-practitioner-ccp"}}');var o=n(474848),s=n(28453);const a={},i="Attention in Transformers",d={},l=[{value:"What you will do",id:"what-you-will-do",level:3}];function c(e){const t={a:"a",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"attention-in-transformers",children:"Attention in Transformers"})}),"\n",(0,o.jsx)(t.p,{children:"Attention in Transformers: Concepts and Code in PyTorch."}),"\n",(0,o.jsx)(t.p,{children:'The attention mechanism was a breakthrough that led to transformers, the architecture powering large language models like ChatGPT. Transformers, introduced in the 2017 paper: "Attention is All You Need" by Viswani and others, took off because of its highly scalable design.'}),"\n",(0,o.jsx)(t.p,{children:"In this course, you\u2019ll learn how the attention mechanism, a key element of transformer-based LLMs, works and implement it in PyTorch. You'll develop deep intuition about building reliable, functional, and scalable AI applications."}),"\n",(0,o.jsx)(t.h3,{id:"what-you-will-do",children:"What you will do"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Understand the evolution of the attention mechanism, a key breakthrough that led to transformers."}),"\n",(0,o.jsx)(t.li,{children:"Learn the relationships between word embeddings, positional embeddings, and attention."}),"\n",(0,o.jsx)(t.li,{children:"Learn about the Query, Key, and Value matrices, and how to produce and use them in attention."}),"\n",(0,o.jsx)(t.li,{children:"Walk through the math required to calculate self-attention and masked self-attention to learn why and how they work."}),"\n",(0,o.jsx)(t.li,{children:"Understand the difference between self-attention and masked self-attention and how one is used in the encoder to build context-aware embeddings and the other is used in the decoder for generative outputs."}),"\n",(0,o.jsx)(t.li,{children:"Learn the details of the encoder-decoder architecture, cross-attention, and multi-head attention and how they are all incorporated into a transformer."}),"\n",(0,o.jsx)(t.li,{children:"Use PyTorch to code a class that implements self-attention, masked self-attention, and multi-head attention."}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.a,{href:"https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/",children:"Attention in Transformers: Concepts and Code in PyTorch - DeepLearning.AI"})}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.a,{href:"https://youtu.be/DFqWPwF0OH0",children:"Decoder Architecture in Transformers | Step-by-Step from Scratch"})})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>i});var r=n(296540);const o={},s=r.createContext(o);function a(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);