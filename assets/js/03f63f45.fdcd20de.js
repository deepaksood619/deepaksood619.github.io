"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[37370],{887852:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>l,default:()=>o,frontMatter:()=>n,metadata:()=>p,toc:()=>i});var r=t(785893),s=t(511151);const n={},l="DataFrame",p={id:"technologies/apache/apache-spark/09-dataframe",title:"DataFrame",description:"- PySpark - Create an empty DataFrame",source:"@site/docs/technologies/apache/apache-spark/09-dataframe.md",sourceDirName:"technologies/apache/apache-spark",slug:"/technologies/apache/apache-spark/09-dataframe",permalink:"/technologies/apache/apache-spark/09-dataframe",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/technologies/apache/apache-spark/09-dataframe.md",tags:[],version:"current",lastUpdatedAt:1710702797,formattedLastUpdatedAt:"Mar 17, 2024",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Examples",permalink:"/technologies/apache/apache-spark/08-examples"},next:{title:"SQL Functions / Datasources",permalink:"/technologies/apache/apache-spark/10-sql-functions-datasources"}},c={},i=[{value:"CAST",id:"cast",level:3},{value:"Spark DataFrame vs Pandas DataFrame",id:"spark-dataframe-vs-pandas-dataframe",level:2},{value:"Spark collect",id:"spark-collect",level:2},{value:"When to avoid Collect()",id:"when-to-avoid-collect",level:4},{value:"collect () vs select ()",id:"collect--vs-select-",level:4}];function d(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.h1,{id:"dataframe",children:"DataFrame"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-create-an-empty-dataframe/",children:"PySpark - Create an empty DataFrame"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/convert-pyspark-rdd-to-dataframe/",children:"PySpark - Convert RDD to DataFrame"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pandas/convert-pyspark-dataframe-to-pandas/",children:"PySpark - Convert DataFrame to Pandas"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-show-display-dataframe-contents-in-table/",children:"PySpark - show()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/",children:"PySpark - StructType & StructField"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-column-functions/",children:"PySpark - Column Class"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/",children:"PySpark - select()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-collect/",children:"PySpark - collect()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-withcolumn/",children:"PySpark - withColumn()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-rename-dataframe-column/",children:"PySpark - withColumnRenamed()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-where-filter/",children:"PySpark - where() & filter()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-distinct-to-drop-duplicates/",children:"PySpark - drop() & dropDuplicates()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/",children:"PySpark - orderBy() and sort()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/",children:"PySpark - groupBy()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/",children:"PySpark - join()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-union-and-unionall/",children:"PySpark - union() & unionAll()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-unionbyname/",children:"PySpark - unionByName()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/",children:"PySpark - UDF (User Defined Function)"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-transform-function/",children:"PySpark - transform()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-apply-function-to-column/",children:"PySpark - apply()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-map-transformation/",children:"PySpark - map()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-flatmap-transformation/",children:"PySpark - flatMap()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-foreach-usage-with-examples/",children:"PySpark - foreach()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-sampling-example/",children:"PySpark - sample() vs sampleBy()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/",children:"PySpark - fillna() & fill()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-pivot-and-unpivot-dataframe/",children:"PySpark - pivot() (Row to Column)"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/",children:"PySpark - partitionBy()"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/",children:"PySpark - MapType (Map/Dict)"})}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# Copy pyspark.sql.dataframe.DataFrame to another dataframe\ndf_copy = df.alias('df')\n\n# Subtract two pyspark.sql.dataframe.DataFrame\ndiff_df = df.subtract(df_copy)\ndiff_df.display()\n\nresult_df = df.where(df[\"id\"].isin(['5edc8f7d-0036-4910-84c4-48d46f7eeb04']))\nresult_df.display()\nresult_df.head()\n\n# group by\ndf1.groupBy(F.date_format('updatedAt','yyyy-MM-dd').alias('day')).count().display()\n\n# filter\ndf2 = df1.filter((df1.updatedAt >= \"2023-04-05\"))\ndf3 = df2.filter(df.amount.isNotNull())\n\n# select\ndf1.select('amount').groupby('amount').count().display()\n\n# group by count, order by count desc\nfrom pyspark.sql.functions import desc\n\ndfFilter.sort(desc(df.groupby('DepositTransactionId').count())).display()\n\n# dropDuplicates\ndf = df.dropDuplicates()\n"})}),"\n",(0,r.jsx)(a.h3,{id:"cast",children:"CAST"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'# cast columns to different types\nfrom pyspark.sql.functions import col\n\nfrom pyspark.sql.types import DateType, LongType, DoubleType, IntegerType, BooleanType\n\ndf = df.withColumn("col_name", col("col_name").cast(IntegerType())) \\\n.withColumn("col_name2", col("col_name2").cast(IntegerType())) \\\n.withColumn("col_name3", col("col_name3").cast(BooleanType()))\n'})}),"\n",(0,r.jsx)(a.h2,{id:"spark-dataframe-vs-pandas-dataframe",children:"Spark DataFrame vs Pandas DataFrame"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{style:{textAlign:"center"},children:"Spark DataFrame"}),(0,r.jsx)(a.th,{style:{textAlign:"center"},children:"Pandas DataFrame"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame supports parallelization."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame does not support parallelization."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame has Multiple Nodes."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame has a Single  Node."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"It follows Lazy Execution which means that a task is not executed until an action is performed."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"It follows Eager Execution, which means task is executed immediately."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame is Immutable."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame is Mutable."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Complex operations are difficult to perform as compared to Pandas DataFrame."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Complex operations are easier to perform as compared to Spark DataFrame."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame is distributed and hence processing in the Spark DataFrame is faster for a large amount of data."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame is not distributed and hence processing in the Pandas DataFrame will be slower for a large amount of data."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"sparkDataFrame.count() returns the number of rows."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"pandasDataFrame.count() returns the number of non NA/null observations for each column."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrames are excellent for building a scalable application."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrames can\u2019t be used to build a scalable application."})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Spark DataFrame assures fault tolerance."}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Pandas DataFrame does not assure fault tolerance. We need to implement our own framework to assure it."})]})]})]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://www.geeksforgeeks.org/difference-between-spark-dataframe-and-pandas-dataframe/",children:"Difference Between Spark DataFrame and Pandas DataFrame - GeeksforGeeks"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pandas-vs-pyspark-dataframe-with-examples/",children:"Pandas vs PySpark DataFrame With Examples - Spark By Examples"})}),"\n",(0,r.jsx)(a.h2,{id:"spark-collect",children:"Spark collect"}),"\n",(0,r.jsxs)(a.p,{children:["PySpark RDD/DataFrame ",(0,r.jsx)(a.code,{children:"collect()"})," is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after ",(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-where-filter/",children:"filter()"}),", ",(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/",children:"group()"})," e.t.c. Retrieving larger datasets results in ",(0,r.jsx)(a.code,{children:"OutOfMemory"})," error."]}),"\n",(0,r.jsx)(a.h4,{id:"when-to-avoid-collect",children:"When to avoid Collect()"}),"\n",(0,r.jsxs)(a.p,{children:["Usually, collect() is used to retrieve the action output when you have very small result set and calling ",(0,r.jsx)(a.code,{children:"collect()"})," on an RDD/DataFrame with a bigger result set causes out of memory as it returns the entire dataset (from all workers) to the driver hence we should avoid calling collect() on a larger dataset."]}),"\n",(0,r.jsx)(a.h4,{id:"collect--vs-select-",children:"collect () vs select ()"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.code,{children:"select()"})," is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/pyspark/pyspark-collect/",children:"PySpark Collect() - Retrieve data from DataFrame - Spark By Examples"})})]})}function o(e={}){const{wrapper:a}={...(0,s.a)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},511151:(e,a,t)=>{t.d(a,{Z:()=>p,a:()=>l});var r=t(667294);const s={},n=r.createContext(s);function l(e){const a=r.useContext(n);return r.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function p(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(n.Provider,{value:a},e.children)}}}]);