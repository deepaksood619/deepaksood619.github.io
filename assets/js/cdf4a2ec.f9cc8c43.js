"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[40320],{243182:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var r=n(785893),a=n(511151);const i={},o="Others",s={id:"ai/move-37/others",title:"Others",description:"MCMC - Markov Chain Monte Carlo",source:"@site/docs/ai/move-37/others.md",sourceDirName:"ai/move-37",slug:"/ai/move-37/others",permalink:"/ai/move-37/others",draft:!1,unlisted:!1,editUrl:"https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/others.md",tags:[],version:"current",lastUpdatedAt:1701793554,formattedLastUpdatedAt:"Dec 5, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Open AI Gym",permalink:"/ai/move-37/open-ai-gym"},next:{title:"Q-Learning Algorithms",permalink:"/ai/move-37/q-learning-algorithms"}},c={},l=[{value:"AlphaGO",id:"alphago",level:2},{value:"Deep Q Neural Network",id:"deep-q-neural-network",level:2},{value:"Asynchronous Actor-Critic Agent",id:"asynchronous-actor-critic-agent",level:2}];function h(e){const t={a:"a",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",...(0,a.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h1,{id:"others",children:"Others"}),"\n",(0,r.jsx)(t.p,{children:"MCMC - Markov Chain Monte Carlo"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://skymind.ai/wiki/markov-chain-monte-carlo",children:"https://skymind.ai/wiki/markov-chain-monte-carlo"})}),"\n",(0,r.jsx)(t.p,{children:"Bayesian approach"}),"\n",(0,r.jsx)(t.p,{children:"In the Bayesian approach to decision-making, you first start with the prior, this is what your beliefs are, then as data comes in, you incorporate that data to update these priors to get the posterior."}),"\n",(0,r.jsx)(t.p,{children:"Bayesian Model"}),"\n",(0,r.jsx)(t.p,{children:"A Bayesian model is a statistical model where you use probability to represent all uncertainty within the model."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://www.toptal.com/algorithms/metropolis-hastings-bayesian-inference",children:"https://www.toptal.com/algorithms/metropolis-hastings-bayesian-inference"})}),"\n",(0,r.jsx)(t.h2,{id:"alphago",children:"AlphaGO"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Policy Network"}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Value Network"}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Tree Search"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"Q-Learning: an algorithm which produces a Q-table that an agent uses to find the best action to take given a state."}),"\n",(0,r.jsx)(t.h2,{id:"deep-q-neural-network",children:"Deep Q Neural Network"}),"\n",(0,r.jsx)(t.p,{children:"A Neural Network that takes a state and approximates Q-values for each action based on that state."}),"\n",(0,r.jsx)(t.h2,{id:"asynchronous-actor-critic-agent",children:"Asynchronous Actor-Critic Agent"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2",children:"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"})})]})}function d(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},511151:(e,t,n)=>{n.d(t,{Z:()=>s,a:()=>o});var r=n(667294);const a={},i=r.createContext(a);function o(e){const t=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(i.Provider,{value:t},e.children)}}}]);