<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ai/llm/interview-questions" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.0">
<title data-rh="true">Interview Questions | Deep Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://deepaksood619.github.io/img/timeline_compressed.webp"><meta data-rh="true" name="twitter:image" content="https://deepaksood619.github.io/img/timeline_compressed.webp"><meta data-rh="true" property="og:url" content="https://deepaksood619.github.io/ai/llm/interview-questions"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Interview Questions | Deep Notes"><meta data-rh="true" name="description" content="Can you provide a high-level overview of Transformers&#x27; architecture?"><meta data-rh="true" property="og:description" content="Can you provide a high-level overview of Transformers&#x27; architecture?"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepaksood619.github.io/ai/llm/interview-questions"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/llm/interview-questions" hreflang="en"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/llm/interview-questions" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://X3OY8NGHVH-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AI","item":"https://deepaksood619.github.io/ai/"},{"@type":"ListItem","position":2,"name":"LLM","item":"https://deepaksood619.github.io/ai/llm/"},{"@type":"ListItem","position":3,"name":"Interview Questions","item":"https://deepaksood619.github.io/ai/llm/interview-questions"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZSZMJXWSH3"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZSZMJXWSH3",{})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-TN3KBF4",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="search" type="application/opensearchdescription+xml" title="Deep Notes" href="/opensearch.xml">
<link rel="icon" href="/img/logo.webp">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(53, 120, 229)">


<script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5616865131274062" async crossorigin="anonymous"></script><link rel="stylesheet" href="/assets/css/styles.9f55d0bd.css">
<script src="/assets/js/runtime~main.de031a2f.js" defer="defer"></script>
<script src="/assets/js/main.10bee9b4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TN3KBF4" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Notes</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/deepaksood619" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://www.linkedin.com/in/deepaksood619/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/about-deepak-sood/">About Deepak Sood</a><button aria-label="Expand sidebar category &#x27;About Deepak Sood&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ai/">AI</a><button aria-label="Collapse sidebar category &#x27;AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/big-data/">Big Data</a><button aria-label="Expand sidebar category &#x27;Big Data&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/computer-vision-cv/">Computer Vision</a><button aria-label="Expand sidebar category &#x27;Computer Vision&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/content-moderation">Content Moderation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-science/">Data Science</a><button aria-label="Expand sidebar category &#x27;Data Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-visualization/">Data Visualization</a><button aria-label="Expand sidebar category &#x27;Data Visualization&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/deep-learning/">Deep Learning</a><button aria-label="Expand sidebar category &#x27;Deep Learning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathon-hackhound">Hackathon HackHound</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathon-rabbittai">Hackathon Rabbitt AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathons">Hackathons</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/libraries/">Libraries</a><button aria-label="Expand sidebar category &#x27;Libraries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai/llm/">LLM</a><button aria-label="Collapse sidebar category &#x27;LLM&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/agents">Agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/building">Building</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/code-generators-coding-generators">Code Generators / Coding Generators</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/design-patterns">Design patterns</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/ethics">Ethics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/evaluation-benchmarking">Evaluation / Benchmarking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/fintech-use-cases">Fintech Use Cases</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/genai-projects">GenAI Projects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/interview-questions-top-50">Top 50 Large Language Model (LLM) Interview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai/llm/interview-questions">Interview Questions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/intro">Intro</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/langchain">Langchain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/libraries">Libraries</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/limitations-problems">Model Limitations / Problems</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/model-context-protocol-mcp">Model Context Protocol (MCP)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/models">Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/natural-language-to-sql-generative-bi">Natural Language to SQL / Generative BI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/pricing-costs">Pricing / Costs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/prompt-engineering">Prompt Engineering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/prompt-examples">Prompt Examples</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/rag-hackathon-questions">RAG Hackathon Questions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/rag-retrieval-augmented-generation">RAG - retrieval-augmented generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/tools">Tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/llm/tuning">Tuning</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-algorithms/">ML Algorithms</a><button aria-label="Expand sidebar category &#x27;ML Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-fundamentals/">ML Fundamentals</a><button aria-label="Expand sidebar category &#x27;ML Fundamentals&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/model-evaluation/">Model Evaluation</a><button aria-label="Expand sidebar category &#x27;Model Evaluation&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/move-37/">Move37</a><button aria-label="Expand sidebar category &#x27;Move37&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/nlp/">NLP</a><button aria-label="Expand sidebar category &#x27;NLP&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/numpy/">Numpy</a><button aria-label="Expand sidebar category &#x27;Numpy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/others-resources-interview-learning-courses">Others / Resources / Interview / Learning / Courses</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/pandas/">Pandas</a><button aria-label="Expand sidebar category &#x27;Pandas&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/scikit-learn/">Scikit Learn / Scipy</a><button aria-label="Expand sidebar category &#x27;Scikit Learn / Scipy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/social-media-analytics-solution">Social Media Analytics Solution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/solutions">Solutions</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/algorithms/">Algorithms</a><button aria-label="Expand sidebar category &#x27;Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/book-summaries/">Book Summaries</a><button aria-label="Expand sidebar category &#x27;Book Summaries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/cloud/">Cloud</a><button aria-label="Expand sidebar category &#x27;Cloud&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/computer-science/">Computer Science</a><button aria-label="Expand sidebar category &#x27;Computer Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/courses/">Courses / Certifications</a><button aria-label="Expand sidebar category &#x27;Courses / Certifications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-structures/">Data Structures</a><button aria-label="Expand sidebar category &#x27;Data Structures&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-warehouses/">Data Warehouses</a><button aria-label="Expand sidebar category &#x27;Data Warehouses&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases/">Databases</a><button aria-label="Expand sidebar category &#x27;Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases-nosql/">NoSQL Databases</a><button aria-label="Expand sidebar category &#x27;NoSQL Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases-sql/">SQL Databases</a><button aria-label="Expand sidebar category &#x27;SQL Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/decentralized-applications/">Decentralized Applications</a><button aria-label="Expand sidebar category &#x27;Decentralized Applications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/devops/">DevOps</a><button aria-label="Expand sidebar category &#x27;DevOps&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/economics/">Economics</a><button aria-label="Expand sidebar category &#x27;Economics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/education/">Education</a><button aria-label="Expand sidebar category &#x27;Education&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/frontend/">Frontend</a><button aria-label="Expand sidebar category &#x27;Frontend&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/knowledge/">Knowledge</a><button aria-label="Expand sidebar category &#x27;Knowledge&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/languages/">Languages</a><button aria-label="Expand sidebar category &#x27;Languages&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/management/">Management</a><button aria-label="Expand sidebar category &#x27;Management&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/mathematics/">Mathematics</a><button aria-label="Expand sidebar category &#x27;Mathematics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/networking/">Networking</a><button aria-label="Expand sidebar category &#x27;Networking&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/psychology/">Psychology</a><button aria-label="Expand sidebar category &#x27;Psychology&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/python/">Python</a><button aria-label="Expand sidebar category &#x27;Python&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Deepak&#x27;s Wiki</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/technologies/">Technologies</a><button aria-label="Expand sidebar category &#x27;Technologies&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai/"><span>AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai/llm/"><span>LLM</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Interview Questions</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Interview Questions</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="can-you-provide-a-high-level-overview-of-transformers-architecture">Can you provide a high-level overview of Transformers&#x27; architecture?<a href="#can-you-provide-a-high-level-overview-of-transformers-architecture" class="hash-link" aria-label="Direct link to Can you provide a high-level overview of Transformers&#x27; architecture?" title="Direct link to Can you provide a high-level overview of Transformers&#x27; architecture?">​</a></h2>
<p>Let’s begin by looking at the model as a <em>single black box</em>. In a machine translation application, it would take a sentence in one language, and output its translation in another, as illustrated below,</p>
<p><img decoding="async" loading="lazy" src="https://jalammar.github.io/images/t/the_transformer_3.png" alt="image" class="img_ev3q"></p>
<p>Getting closer into the black box, transformers have on the inside:</p>
<ul>
<li>An <strong>encoding component</strong>: which is a stack of <code>N</code> encoders.</li>
<li>A <strong>decoding component</strong>: which is a stack of <code>N</code> decoders,</li>
<li>and <strong>connections between them</strong>.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png" alt="image" class="img_ev3q"></p>
<p>Now, each <strong>encoder</strong> is broken down into two sub-layers: the <strong>self-attention layer</strong> and the <strong>feed-forward neural network layer</strong>.</p>
<p>The inputs first flow through a <strong>self-attention layer</strong>, and the outputs of the <strong>self-attention layer</strong> are fed to a <strong>feed-forward neural network</strong>. And this sequence is repeated till reaches the last encoder.</p>
<p>Finally, the <strong>decoder</strong> receives the output of the <strong>encoder component</strong> and also has both the <strong>self-attention layer</strong> and <strong>feed-forward layer</strong>, and the flow is similar to before, but between them there is an <strong>attention layer</strong> that helps the decoder focus on relevant parts of the input sentence.</p>
<p><img decoding="async" loading="lazy" src="https://jalammar.github.io/images/t/Transformer_decoder.png" alt="image" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-next-sentence-prediction-nsp-is-used-in-language-modeling">How <em>next sentence prediction (NSP)</em> is used in language modeling?<a href="#how-next-sentence-prediction-nsp-is-used-in-language-modeling" class="hash-link" aria-label="Direct link to how-next-sentence-prediction-nsp-is-used-in-language-modeling" title="Direct link to how-next-sentence-prediction-nsp-is-used-in-language-modeling">​</a></h2>
<p><strong>Next sentence prediction (NSP)</strong> is used in language modeling as <em>one-half</em> of the training process behind the <strong>BERT</strong> model (the other half is <em>masked-language modeling (MLM)</em>). The objective of next-sentence prediction training is to predict whether one sentence logically follows the other sentence presented to the model.</p>
<p>During training, the model is presented with pairs of sentences, some of which are consecutive in the original text, and some of which are not. The model is then trained to predict whether a given pair of sentences are adjacent or not. This allows the model to <strong>understand longer-term dependencies across sentences</strong>.</p>
<p>Researchers have found that without <strong>NSP</strong>, <strong>BERT</strong> performs worse on every single metric - so its use it’s relevant to language modeling.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-evaluate-the-performance-of-language-models">How can you <em>evaluate the performance</em> of Language Models?<a href="#how-can-you-evaluate-the-performance-of-language-models" class="hash-link" aria-label="Direct link to how-can-you-evaluate-the-performance-of-language-models" title="Direct link to how-can-you-evaluate-the-performance-of-language-models">​</a></h2>
<p>There are two ways to evaluate language models in <strong>NLP</strong>: <strong><em>Extrinsic evaluation</em></strong> and <strong><em>Intrinsic evaluation</em></strong>.</p>
<ul>
<li><strong>Intrinsic evaluation</strong> captures how well the model captures what it is supposed to capture, like probabilities.</li>
<li><strong>Extrinsic evaluation</strong> (or task-based evaluation) captures how useful the model is in a particular task.</li>
</ul>
<p>A common <strong>intrinsic evaluation</strong> of <strong>LM</strong> is the <strong>perplexity</strong>. It&#x27;s a geometric average of the inverse probability of words predicted by the model. Intuitively, perplexity means to be <em>surprised</em>. We measure <em>how much the model is surprised</em> by seeing new data. The lower the perplexity, the better the training is. Another common measure is the <strong>cross-entropy</strong>, which is the Logarithm (base <code>2</code>) of <em>perplexity</em>. As a thumb rule, a reduction of <code>10-20%</code> in perplexity is noteworthy.</p>
<p>The <strong>extrinsic evaluation</strong> will depend on the task. Example: For <em>speech recognition</em>, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-generative-language-models-work">How do <em>generative language models</em> work?<a href="#how-do-generative-language-models-work" class="hash-link" aria-label="Direct link to how-do-generative-language-models-work" title="Direct link to how-do-generative-language-models-work">​</a></h2>
<p>The very basic idea is the following: they take <code>n</code> tokens as input, and produce <code>one</code> token as output.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*zaRZlVk-dl0zVUOe0g_ufg.png" alt="image" class="img_ev3q"></p>
<p>A token is a chunk of text. In the context of OpenAI GPT models, common and short words typically correspond to a single token and long and less commonly used words are generally broken up into several tokens.</p>
<p>This basic idea is applied in an <em>expanding-window pattern</em>. You give it <code>n</code> tokens in, it produces <code>one</code> token out, then it incorporates that output token as part of the input of the next iteration, produces a new token out, and so on. This pattern keeps repeating until a stopping condition is reached, indicating that it finished generating all the text you need.</p>
<p>Now, behind the output is a probability distribution over all the possible tokens. What the model does is return a vector in which each entry expresses the probability of a particular token being chosen.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*UZ_hexQINPnJV14HWLorAg.png" alt="image" class="img_ev3q"></p>
<p>This probability distribution comes from the training phase. During training, the model is exposed to a lot of text, and its weights are tuned to predict good probability distributions, given a sequence of input tokens.</p>
<p>GPT generative models are trained with a large portion of the internet, so their predictions reflect a mix of the information they’ve seen.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-token-in-the-large-language-models-context">What is a <em>token</em> in the Large Language Models context?<a href="#what-is-a-token-in-the-large-language-models-context" class="hash-link" aria-label="Direct link to what-is-a-token-in-the-large-language-models-context" title="Direct link to what-is-a-token-in-the-large-language-models-context">​</a></h2>
<p>ChatGPT and other LLMs rely on input text being broken into pieces. Each piece is about a word-sized sequence of characters or smaller. We call those sub-word tokens. That process is called <em>tokenization</em> and is done using a <em>tokenizer</em>.</p>
<p><strong>Tokens</strong> can be words or just chunks of characters. For example, the word &quot;hamburger&quot; gets broken up into the tokens &quot;ham&quot;, &quot;bur&quot; and &quot;ger&quot;, while a short and common word like &quot;pear&quot; is a single token. Many tokens start with whitespace, for example, &quot; hello&quot; and &quot; bye&quot;.</p>
<p>The models understand the statistical relationships between these tokens and excel at producing the next token in a sequence of tokens.</p>
<p>The number of tokens processed in a given API request depends on the length of both your inputs and outputs. As a rough rule of thumb, the <code>1</code> token is approximately <code>4</code> characters or <code>0.75</code> words for English text.</p>
<p>Consider:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*yM5gg3cPHDvo7AAqDZryZA.png" alt="https://miro.medium.com/v2/resize:fit:1400/1*yM5gg3cPHDvo7AAqDZryZA.png" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-advantage-of-using-transformer-based-vs-lstm-based-architectures-in-nlp">What&#x27;s the advantage of using transformer-based vs LSTM-based architectures in NLP?<a href="#whats-the-advantage-of-using-transformer-based-vs-lstm-based-architectures-in-nlp" class="hash-link" aria-label="Direct link to What&#x27;s the advantage of using transformer-based vs LSTM-based architectures in NLP?" title="Direct link to What&#x27;s the advantage of using transformer-based vs LSTM-based architectures in NLP?">​</a></h2>
<p>To create <strong>sequence-to-sequence</strong> models before the <strong>Transformer</strong>, we used the famous <strong>LSTM</strong> with its <strong><em>Encoder-Decoder architecture</em></strong>, where</p>
<ul>
<li>The &quot;<strong>Encoder</strong>&quot; part that creates a <em>vector representation</em> of a <em>sequence of words</em>.</li>
<li>The &quot;<strong>Decoder</strong>&quot; returns a <em>sequence of words</em> from the <em>vector representation</em>.</li>
</ul>
<p>The <strong>LSTM</strong> model takes into account the <strong>interdependence of words</strong>, so we need inputs of the previous state to make any operations on the current state. This model has a limitation: it is relatively slow to train and the input sequence can&#x27;t be passed in parallel.</p>
<p>Now, the idea of the <strong>Transformer</strong> is to maintain the <strong>interdependence of the words</strong> in a sequence without using a recurrent network but only the <strong>attention mechanism</strong> that is at the center of its architecture. The <strong>attention</strong> measures <strong>how closely two elements of two sequences are related</strong>.</p>
<p>In transformer-based architectures, the attention mechanism is applied to a <em>single sequence</em> (also known as a <strong><em>self-attention layer</em></strong>). The <strong>self-attention layer</strong> determines the <strong>interdependence of different words</strong> in the same sequence, to associate a relevant representation with it. Take for example the sentence: &quot;<em>The dog didn&#x27;t cross the street because it was too tired</em>&quot;. It is obvious to a human being that &quot;<em>it</em>&quot; refers to the &quot;<em>dog</em>&quot; and not to the &quot;<em>street</em>&quot;. The objective of the self-attention process will therefore be to detect the link between &quot;<em>dog</em>&quot; and &quot;<em>it</em>&quot;. This feature makes transformers much faster to train compared to their predecessors, and they have been proven to be more robust against noisy and missing data.</p>
<p>As a plus, in <strong>contextual embeddings</strong>, transformers can draw information from the context to correct missing or noisy data and that is something that other neural networks couldn’t offer.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="can-you-provide-some-examples-of-alignment-problems-in-large-language-models">Can you provide some examples of <em>alignment problems</em> in Large Language Models?<a href="#can-you-provide-some-examples-of-alignment-problems-in-large-language-models" class="hash-link" aria-label="Direct link to can-you-provide-some-examples-of-alignment-problems-in-large-language-models" title="Direct link to can-you-provide-some-examples-of-alignment-problems-in-large-language-models">​</a></h2>
<p>The <strong>alignment problem</strong> refers to the extent to which a model&#x27;s goals and behavior align with human values and expectations.</p>
<p>Large Language Models, such as <code>GPT-3</code>, are trained on vast amounts of text data from the internet and are capable of generating human-like text, but they may not always produce output that is consistent with human expectations or desirable values.</p>
<p>The <em>alignment problem</em> in Large Language Models typically manifests as:</p>
<ul>
<li><strong>Lack of helpfulness</strong>: when the model is not following the user&#x27;s explicit instructions.</li>
<li><strong>Hallucinations</strong>: when the model is making up unexisting or wrong facts.</li>
<li><strong>Lack of interpretability</strong>: when it is difficult for humans to understand how the model arrived at a particular decision or prediction.</li>
<li><strong>Generating biased or toxic output</strong>: when a language model that is trained on biased/toxic data may reproduce that in its output, even if it was not explicitly instructed to do so.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-adaptative-softmax-is-useful-in-large-language-models">How <em>Adaptative Softmax</em> is useful in Large Language Models?<a href="#how-adaptative-softmax-is-useful-in-large-language-models" class="hash-link" aria-label="Direct link to how-adaptative-softmax-is-useful-in-large-language-models" title="Direct link to how-adaptative-softmax-is-useful-in-large-language-models">​</a></h2>
<p><strong>Adaptive softmax</strong> is useful in large language models because it allows for efficient training and inference when dealing with <em>large vocabularies</em>. <strong><em>Traditional softmax</em></strong> involves computing probabilities for <em>each word</em> in the vocabulary, which can become computationally expensive as the vocabulary size grows.</p>
<p><strong>Adaptive softmax</strong> reduces the number of computations required by grouping words together into clusters based on <strong><em>how common the words are</em></strong>. This reduces the number of computations required to compute the probability distribution over the vocabulary.</p>
<p>Therefore, by using adaptive softmax, large language models can be trained and run more efficiently, allowing for faster experimentation and development.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-bert-training-work">How does BERT <em>training</em> work?<a href="#how-does-bert-training-work" class="hash-link" aria-label="Direct link to how-does-bert-training-work" title="Direct link to how-does-bert-training-work">​</a></h2>
<p><strong>BERT</strong> (<em>Bidirectional Encoder Representations from Transformers</em>) utilizes a <strong><em>transformer architecture</em></strong> that learns contextual relationships between words in a text and since BERT’s goal is to generate a language representation model, it only needs the <strong><em>encoder</em></strong> part.</p>
<p>The input to the <strong><em>encoder</em></strong> for <strong>BERT</strong> is a sequence of <em>tokens</em>, which are first converted into <em>vectors</em> and then processed in the neural network. Then, the <strong>BERT</strong> algorithm makes use of the following two training techniques:</p>
<ul>
<li><strong>Masked LM (MLM)</strong>: Before feeding word sequences into <strong>BERT</strong>, a percentage of the words in each sequence are replaced with a <code>[MASK]</code> token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.</li>
<li><strong>Next Sentence Prediction (NSP)</strong>: the model concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to sentences that were next to each other in the original text, and sometimes not. The model then has to predict if the two sentences were following each other or not.</li>
</ul>
<p>Now, to help the model distinguish between the two sentences in training, the input is processed with some extra metadata such as:</p>
<ul>
<li><strong><em>Token embeddings</em></strong>: A <code>[CLS]</code> token is inserted at the beginning of the first sentence and a <code>[SEP]</code> token is inserted at the end of each sentence.</li>
<li><strong><em>Segment embeddings</em></strong>: these assign markers to identify each sentence and allows the encoder to distinguish between them.</li>
<li><strong><em>Positional embeddings</em></strong>: to indicate the token position in the sentence.</li>
</ul>
<p>Then, to predict if the second sentence is indeed connected to the first, the following steps are performed:</p>
<ul>
<li>The entire input sequence goes through the <em>Transformer model</em>.</li>
<li>The output of the <code>[CLS]</code> token is transformed into a <code>2×1</code> shaped vector, using a simple <em>classification layer</em> (learned matrices of weights and biases).</li>
<li>Calculating the probability of <code>IsNextSequence</code> with <em>softmax</em>.</li>
</ul>
<p>When training the <strong>BERT</strong> model, <strong><em>Masked LM</em></strong> and <strong><em>Next Sentence Prediction</em></strong> are trained together, with the goal of minimizing the combined loss function of the two strategies.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:780/1*vUuxeQKHA5RHaaZ8-oVQ_A.png" alt="https://miro.medium.com/v2/resize:fit:780/1*vUuxeQKHA5RHaaZ8-oVQ_A.png" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-the-transformer-network-better-than-cnns-and-rnns">How is the <em>Transformer Network</em> better than <em>CNNs</em> and <em>RNNs</em>?<a href="#how-is-the-transformer-network-better-than-cnns-and-rnns" class="hash-link" aria-label="Direct link to how-is-the-transformer-network-better-than-cnns-and-rnns" title="Direct link to how-is-the-transformer-network-better-than-cnns-and-rnns">​</a></h2>
<ul>
<li>With <strong>RNN</strong>, you have to go <em>word by word</em> to access to the cell of the last word. If the network is formed with a long reach, it may take several steps to remember, each masked state (output vector in a word) depends on the previous masked state. This becomes a major problem for GPUs. This sequentiality is an obstacle to the parallelization of the process. In addition, in cases where such sequences are too long, the model tends to forget the contents of the distant positions one after the other or to mix with the contents of the following positions. In general, whenever <strong><em>long-term dependencies</em></strong> are involved, we know that <strong>RNN</strong> suffers from the <strong><em>Vanishing Gradient Problem</em></strong>.</li>
<li>Early efforts were trying to solve the dependency problem with <strong><em>sequential convolutions</em></strong> for a solution to the <strong>RNN</strong>. A long sequence is taken and the convolutions are applied. The disadvantage is that <strong>CNN</strong> approaches require many layers to capture long-term dependencies in the sequential data structure, without ever succeeding or making the network so large that it would eventually become impractical.</li>
<li>The <strong>Transformer</strong> presents a new approach, it proposes to <em>encode</em> each word and apply the <strong><em>mechanism of attention</em></strong> in order to connect two distant words, then the <em>decoder</em> predicts the sentences according to all the words preceding the current word. This workflow can be parallelized, accelerating learning and solving the <strong><em>long-term dependencies</em></strong> problem.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="is-there-a-way-to-train-a-large-language-model-llm-to-store-a-specific-context">Is there a way to train a Large Language Model (LLM) to store a specific context?<a href="#is-there-a-way-to-train-a-large-language-model-llm-to-store-a-specific-context" class="hash-link" aria-label="Direct link to Is there a way to train a Large Language Model (LLM) to store a specific context?" title="Direct link to Is there a way to train a Large Language Model (LLM) to store a specific context?">​</a></h2>
<p>The only way at the moment to &quot;memorize&quot; past conversations is to <strong>include past conversations in the prompt</strong>.</p>
<p>Consider:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">You are a friendly support person. The customer will ask you questions, and you will provide polite responses</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Q: My phone won&#x27;t start. What do I do? &amp;lt;-- This is a past question</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: Try plugging your phone into the charger for an hour and then turn it on. The most common cause for a phone not starting is that the battery is dead.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Q: I&#x27;ve tried that. What else can I try? &amp;lt;-- This is a past question</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: Hold the button for 15 seconds. It may need a reset.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Q: I did that. It worked, but the screen is blank. &amp;lt;-- This is a current question</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>You will hit a token limit at some point (if you chat long enough). Each GPT-3 model has a <a href="https://platform.openai.com/docs/models/gpt-3" target="_blank" rel="noopener noreferrer">maximum number of tokens</a> you can pass to it. In the case of <code>text-davinci-003</code>, it is <code>4096</code> tokens. When you hit this limit, the OpenAI API will throw an error.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-transfer-learning-techniques-can-you-use-in-llms">What <em>Transfer learning Techniques</em> can you use in LLMs?<a href="#what-transfer-learning-techniques-can-you-use-in-llms" class="hash-link" aria-label="Direct link to what-transfer-learning-techniques-can-you-use-in-llms" title="Direct link to what-transfer-learning-techniques-can-you-use-in-llms">​</a></h2>
<p>There are several <strong>Transfer Learning</strong> techniques that are commonly used in <strong>LLMs</strong>. Here are three of the most popular:</p>
<ul>
<li><strong>Feature-based transfer learning</strong>: This technique involves using a pre-trained language model as a <em>feature extractor</em>, and then training a separate model on top of the extracted features for the target task.</li>
<li><strong>Fine-tuning</strong>: involves taking a pre-trained language model and training it on a <em>specific task</em>. Sometimes when fine-tuning, you can keep the model weights fixed and just add a new layer that you will train. Other times you can slowly unfreeze the layers one at a time. You can also use unlabelled data when pre-training, by masking words and trying to predict which word was masked.</li>
<li><strong>Multi-task learning</strong>: involves training a single model on multiple related tasks simultaneously. The idea is that the model will learn to share information across tasks and improve performance on each individual task as a result.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-transfer-learning-and-why-is-it-important">What is <em>Transfer Learning</em> and why is it important?<a href="#what-is-transfer-learning-and-why-is-it-important" class="hash-link" aria-label="Direct link to what-is-transfer-learning-and-why-is-it-important" title="Direct link to what-is-transfer-learning-and-why-is-it-important">​</a></h2>
<p>A pre-trained model, such as GPT-3, essentially takes care of massive amounts of hard work for the developers: It teaches the model to do a basic understanding of the problem and provides solutions in a <em>generic</em> format.</p>
<p>With <strong>transfer learning</strong>, given that the pre-trained models can generate basic solutions, we can <em>transfer the learning to another context</em>. As a result, we will be able to customize the model to our requirements using fine-tuning without the need to retrain the entire model.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-difference-between-encoder-vs-decoder-models">What&#x27;s the difference between <em>Encoder</em> vs <em>Decoder</em> models?<a href="#whats-the-difference-between-encoder-vs-decoder-models" class="hash-link" aria-label="Direct link to whats-the-difference-between-encoder-vs-decoder-models" title="Direct link to whats-the-difference-between-encoder-vs-decoder-models">​</a></h2>
<p><strong>Encoder models</strong>:</p>
<ul>
<li>They use only the <em>encoder</em> of a <strong>Transformer model</strong>. At each stage, the attention layers can access <strong>all the words</strong> in the <strong>initial sentence</strong>.</li>
<li>The pretraining of these models usually revolves around somehow <em>corrupting</em> a given sentence (for instance, by <em>masking</em> random words in it) and tasking the model with <strong>finding or reconstructing</strong> the initial sentence.</li>
<li>They are best suited for tasks requiring an <strong>understanding of the full sentence</strong>, such as sentence classification, named entity recognition (and more general word classification), and extractive question answering.</li>
</ul>
<p><strong>Decoder models</strong>:</p>
<ul>
<li>They use only the <em>decoder</em> of a <strong>Transformer model</strong>. At each stage, for a given word the attention layers <strong>can only access the words positioned before it in the sentence</strong>.</li>
<li>The pretraining of decoder models usually revolves around <strong>predicting</strong> the next word in the sentence.</li>
<li>They are best suited for tasks involving <strong>text generation</strong>.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*cfNpm7aDO4lD3e-Wkwgc1g.png" alt="https://miro.medium.com/v2/resize:fit:1400/1*cfNpm7aDO4lD3e-Wkwgc1g.png" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-difference-between-wordpiece-vs-bpe">What&#x27;s the difference between <em>Wordpiece</em> vs <em>BPE</em>?<a href="#whats-the-difference-between-wordpiece-vs-bpe" class="hash-link" aria-label="Direct link to whats-the-difference-between-wordpiece-vs-bpe" title="Direct link to whats-the-difference-between-wordpiece-vs-bpe">​</a></h2>
<p><strong>WordPiece</strong> and <strong>BPE</strong> are both <em>subword tokenization algorithms</em>. They work by breaking down words into smaller units, called subwords. We then define a desired <strong><em>vocabulary size</em></strong> and keep adding subwords till the limit is reached.</p>
<ul>
<li><strong>BPE</strong> starts with a vocabulary of all the characters in the training data. It then <strong><em>iteratively merges</em></strong> the <strong><em>most frequent pairs</em></strong> of characters until the desired <em>vocabulary size</em> is reached. The merging is done <em>greedily</em>, meaning that the most frequent pair of characters is always merged first.</li>
<li><strong>WordPiece</strong> also starts with a vocabulary of all the characters in the training data. It then uses a <strong><em>statistical model</em></strong> to choose the pair of characters that is most likely to improve the likelihood of the training data until the <em>vocab size</em> is reached.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-difference-between-global-and-local-attention-in-llms">What&#x27;s the difference between <em>Global</em> and <em>Local Attention</em> in LLMs?<a href="#whats-the-difference-between-global-and-local-attention-in-llms" class="hash-link" aria-label="Direct link to whats-the-difference-between-global-and-local-attention-in-llms" title="Direct link to whats-the-difference-between-global-and-local-attention-in-llms">​</a></h2>
<p>Consider the example sentence &quot;<em>Where is Wally</em>&quot; which should be translated to its Italian counterpart &quot;<em>Dove è Wally</em>&quot;. In the transformer architecture, the <strong><em>encoder</em></strong> processes the input word by word, producing three different <strong><em>hidden states</em></strong>.</p>
<p>Then, the <strong>attention layer</strong> produces a single fixed-size <strong><em>context vector</em></strong> from all the encoder hidden states (often with a weighted sum) and it represents the &quot;<em>attention</em>&quot; that must be given to that context when processing such input word. Here is when <strong><em>global</em></strong> and <strong><em>local</em></strong> attention comes into play.</p>
<p><strong>Global attention</strong> considers <em>all the hidden</em> states in creating the <strong><em>context vector</em></strong>. When is applied, a lot of computation occurs. This is because all the hidden states must be taken into consideration, concatenated into a matrix, and processed by a <strong>NN</strong> to compute their weights.</p>
<p>On the other hand, <strong>local attention</strong> considers <em>only a subset</em> of all the hidden states in creating the <strong><em>context vector</em></strong>. The subset can be obtained in many different ways, such as with <strong>Monotonic Alignment</strong> and <strong>Predictive Alignment</strong>.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1200/0*eOBSEK6zMxz-S5kK.jpeg" alt="https://miro.medium.com/v2/resize:fit:1200/0*eOBSEK6zMxz-S5kK.jpeg" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-difference-between-next-token-prediction-vs-masked-language-modeling-in-llm">What&#x27;s the difference between <em>next-token-prediction</em> vs <em>masked-language-modeling</em> in LLM?<a href="#whats-the-difference-between-next-token-prediction-vs-masked-language-modeling-in-llm" class="hash-link" aria-label="Direct link to whats-the-difference-between-next-token-prediction-vs-masked-language-modeling-in-llm" title="Direct link to whats-the-difference-between-next-token-prediction-vs-masked-language-modeling-in-llm">​</a></h2>
<p>Both are techniques for training large language models and involve predicting a word in a sequence of words.</p>
<ul>
<li>
<p><strong>Next token prediction</strong>: the model is given a sequence of words with the goal of predicting the next word. For example, given the phrase <em>Hannah is a ____</em>, the model would try to predict:</p>
</li>
<li>
<p><em>Hannah is a sister</em></p>
</li>
<li>
<p><em>Hannah is a friend</em></p>
</li>
<li>
<p><em>Hannah is a marketer</em></p>
</li>
<li>
<p><em>Hannah is a comedian</em></p>
</li>
<li>
<p><strong>Masked-language-modeling</strong>: the model is given a sequence of words with the goal of predicting a <em>masked</em> word in the middle. For example, given the phrase, <em>Jako mask reading</em>, the model would try to fill the gap as,</p>
</li>
<li>
<p><em>Jacob fears reading</em></p>
</li>
<li>
<p><em>Jacob loves reading</em></p>
</li>
<li>
<p><em>Jacob enjoys reading</em></p>
</li>
<li>
<p><em>Jacon hates reading</em></p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-a-multi-head-attention-mechanism-is-needed-in-a-transformer-based-architecture">Why a <em>Multi-Head Attention</em> mechanism is needed in a <em>Transformer-based Architecture</em>?<a href="#why-a-multi-head-attention-mechanism-is-needed-in-a-transformer-based-architecture" class="hash-link" aria-label="Direct link to why-a-multi-head-attention-mechanism-is-needed-in-a-transformer-based-architecture" title="Direct link to why-a-multi-head-attention-mechanism-is-needed-in-a-transformer-based-architecture">​</a></h2>
<p>Take for example the sentence:</p>
<blockquote>
<p>Bark is very cute and he is a dog.</p>
</blockquote>
<p>Here, if we take the word ‘<code>dog</code>’, grammatically we understand that the words ‘<code>Bark</code>’, ‘<code>cute</code>’, and ‘<code>he</code>’ should have some significance or relevance with the word ‘<code>dog</code>’. These words say that the dog’s name is Bark, it is a male dog, and that he is a cute dog.</p>
<p>In simple terms, just one attention mechanism may not be able to correctly identify these three words as relevant to ‘<code>dog</code>’, and we can sense that three attentions are better here to signify the three words with the word ‘<code>dog</code>’.</p>
<p>Therefore, to overcome some of the pitfalls of using <strong>single attention</strong>, <strong>multi-head attention</strong> is used. This reduces the load on one attention to find all significant words and also increases the chances of finding more relevant words easily.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-would-you-use-encoder-decoder-rnns-vs-plain-sequence-to-sequence-rnns-for-automatic-translation">Why would you use <em>Encoder-Decoder RNNs</em> vs <em>plain sequence-to-sequence RNNs</em> for <em>automatic translation</em>?<a href="#why-would-you-use-encoder-decoder-rnns-vs-plain-sequence-to-sequence-rnns-for-automatic-translation" class="hash-link" aria-label="Direct link to why-would-you-use-encoder-decoder-rnns-vs-plain-sequence-to-sequence-rnns-for-automatic-translation" title="Direct link to why-would-you-use-encoder-decoder-rnns-vs-plain-sequence-to-sequence-rnns-for-automatic-translation">​</a></h2>
<p>A <strong>plain sequence-to-sequence RNN</strong> would start translating a sentence <em>immediately</em> after reading the first word of a sentence, while an <strong>Encoder-Decoder RNN</strong> will first <em>read the whole sentence</em> and then translate it.</p>
<p>In general, if you translate a sentence one word at a time, the result will be terrible. For example, the french sentence &quot;<em>Je vous en prie</em>&quot; means &quot;<em>You are welcome</em>&quot; but if you translate it one word at a time using <em>plain sequence-to-sequence RNN</em>, you get &quot;<em>I you in pray</em>&quot; which it does not have sense. So in automatic translation cases is much better to use <em>Encoder-Decoder RNNs</em> to read the whole sentence first and then translate it.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-self-attention-mechanism-in-the-transformer-architecture">Explain what is <em>Self-Attention</em> mechanism in the Transformer architecture?<a href="#explain-what-is-self-attention-mechanism-in-the-transformer-architecture" class="hash-link" aria-label="Direct link to explain-what-is-self-attention-mechanism-in-the-transformer-architecture" title="Direct link to explain-what-is-self-attention-mechanism-in-the-transformer-architecture">​</a></h2>
<p>We can think of <strong>self-attention</strong> as a mechanism that enhances the information content of an input embedding by <em>including information about the input’s context</em>. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output.</p>
<p>Imagine that we have a text <em>x</em>, which we convert from raw text using an embedding algorithm. To then apply the attention, we map a query (<em>q</em>) as well as a set of key-value pairs (<em>k</em>, <em>v</em>) to our output <em>x</em>. Both <em>q</em>, <em>k</em>, as well as v, are vectors. The result <em>z</em> is called the attention-head and is then sent along a simple feed-forward neural network.</p>
<p><img decoding="async" loading="lazy" src="https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/transformer.png" alt="https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/transformer.png" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-transfer-learning-work-in-llms">How does <em>Transfer Learning</em> work in LLMs?<a href="#how-does-transfer-learning-work-in-llms" class="hash-link" aria-label="Direct link to how-does-transfer-learning-work-in-llms" title="Direct link to how-does-transfer-learning-work-in-llms">​</a></h2>
<p><strong>Transfer learning</strong> in <strong>LLMs</strong> involves training a language model on a <em>large corpus of text</em>, and then <strong>fine-tuning</strong> the model on a specific task. The process involves several steps:</p>
<ol>
<li>
<p><strong>Pre-training</strong>: In this step, the language model is trained on a large corpus of text to learn the patterns and relationships within the language. The goal of pre-training is to generate a set of <strong>general-purpose</strong> language representations that can be used for a wide range of NLP tasks.</p>
</li>
<li>
<p><strong>Fine-tuning</strong>: Once the language model is pre-trained, it can be fine-tuned on a <strong>specific NLP task</strong>, such as sentiment analysis or text classification. Fine-tuning involves taking the pre-trained model and training it on a smaller dataset specific to the target task. The goal of fine-tuning is to adapt the general-purpose language representations to the specific task at hand.</p>
</li>
<li>
<p><strong>Inference</strong>: Once the model is fine-tuned, it can be used to make predictions on new data. During inference, the model takes in the input text and generates an output that represents the model’s prediction for the task at hand.</p>
</li>
</ol>
<p><img decoding="async" loading="lazy" src="https://www.researchgate.net/publication/342547232/figure/fig4/AS:907957101404162@1593485250772/The-concept-of-the-transfer-learning.png" alt="https://www.researchgate.net/publication/342547232/figure/fig4/AS:907957101404162@1593485250772/The-concept-of-the-transfer-learning.png" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network">How does an LLM <em>parameter</em> relate to a <em>weight</em> in a Neural Network?<a href="#how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network" class="hash-link" aria-label="Direct link to how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network" title="Direct link to how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network">​</a></h2>
<p>Yes, the parameters in a large language model (LLM) are similar to the weights in a standard neural network. In both LLMs and neural networks, these parameters are numerical values that start as random coefficients and are adjusted during training to minimize loss. These parameters include not only the weights that determine the strength of connections between neurons but also the biases, which affect the output of neurons. In a large language model (LLM) like GPT-4 or other transformer-based models, the term &quot;parameters&quot; refers to the numerical values that determine the behavior of the model. These parameters include weights and biases, which together define the connections and activations of neurons within the model. Here&#x27;s a more detailed explanation:</p>
<ul>
<li><strong>Weights</strong>: Weights are numerical values that define the strength of connections between neurons across different layers in the model. In the context of LLMs, weights are primarily used in the attention mechanism and the feedforward neural networks that make up the model&#x27;s architecture. They are adjusted during the training process to optimize the model&#x27;s ability to generate relevant and coherent text.</li>
<li><strong>Biases</strong>: Biases are additional numerical values that are added to the weighted sum of inputs before being passed through an activation function. They help to control the output of neurons and provide flexibility in the model&#x27;s learning process. Biases can be thought of as a way to shift the activation function to the left or right, allowing the model to learn more complex patterns and relationships in the input data.</li>
</ul>
<p>The training process involves adjusting these parameters (weights and biases) iteratively to minimize the loss function. This is typically done using gradient descent or a variant thereof, such as stochastic gradient descent or Adam optimizer. The loss function measures the difference between the model&#x27;s predictions and the true values (e.g., the correct next word in a sentence). By minimizing the loss, the model learns to generate text that closely resembles the patterns in its training data.</p>
<p>Researchers often use the term &quot;parameters&quot; instead of &quot;weights&quot; to emphasize that both weights and biases play a crucial role in the model&#x27;s learning process.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-some-downsides-of-fine-tuning-llms">What are some downsides of <em>fine-tuning</em> LLMs?<a href="#what-are-some-downsides-of-fine-tuning-llms" class="hash-link" aria-label="Direct link to what-are-some-downsides-of-fine-tuning-llms" title="Direct link to what-are-some-downsides-of-fine-tuning-llms">​</a></h2>
<ul>
<li><strong>Fine-tuning</strong> requires manually creating tons of data, and the model may memorize portions of the data you provide.</li>
<li>Every time you want to add a new capability, instead of adding a few lines to a prompt, you need to create a bunch of fake data and then run the finetune process and then use the newly fine-tuned model.</li>
<li>It is more expensive.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-word-embedding-position-embedding-and-positional-encoding-in-bert">What is the difference between <em>Word Embedding</em>, <em>Position Embedding</em> and <em>Positional Encoding</em> in BERT?<a href="#what-is-the-difference-between-word-embedding-position-embedding-and-positional-encoding-in-bert" class="hash-link" aria-label="Direct link to what-is-the-difference-between-word-embedding-position-embedding-and-positional-encoding-in-bert" title="Direct link to what-is-the-difference-between-word-embedding-position-embedding-and-positional-encoding-in-bert">​</a></h2>
<ul>
<li>A <strong><em>Word embedding</em></strong> is a learned lookup map i.e. <em>every word</em> is given a <em>one hot encoding</em> which then functions as an <em>index</em>, and the corresponding to this index is a <code>n</code> <em>dimensional vector</em> where the coefficients are learned when training the model.</li>
<li>A <strong>Positional embedding</strong> is similar to a <strong><em>word embedding</em></strong>. Except it is the <em>position</em> in the <em>sentence</em> is used as the index, rather than the one hot encoding. Those positions then correspond to a <code>n</code>-vector whose coefficients come from the trained model.</li>
<li>On the other hand, the <strong>Positional encoding</strong> is a <strong><em>static function</em></strong> that maps an integer input to real-valued vectors in a way that captures the inherent relationships among the positions. That is, it captures the fact that position <code>4</code> in input is more closely related to position <code>5</code> than it is to position <code>17</code>. In other words, <strong>positional encoding</strong> is not learned but a chosen mathematical function, <code>N -&gt; R^n</code>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-difference-between-feature-based-transfer-learning-vs-fine-tuning-in-llms">What’s the difference between <em>Feature-based Transfer Learning</em> vs. <em>Fine Tuning</em> in LLMs?<a href="#whats-the-difference-between-feature-based-transfer-learning-vs-fine-tuning-in-llms" class="hash-link" aria-label="Direct link to whats-the-difference-between-feature-based-transfer-learning-vs-fine-tuning-in-llms" title="Direct link to whats-the-difference-between-feature-based-transfer-learning-vs-fine-tuning-in-llms">​</a></h2>
<ul>
<li>In <strong>Feature-based Transfer Learning</strong>, you can train <em>word embeddings</em> by running a <strong>model <code>A</code></strong> and then using those features from model <code>A</code> (i.e. <em>word vectors</em>) on a <strong>different task</strong>, or model <code>B</code>.</li>
<li>When <strong>Fine Tuning</strong>, you can use the exact <strong>same model <code>A</code></strong> and just run it on a <strong>different task</strong>. Sometimes when fine-tuning, you can keep the model weights fixed and just add a new layer that you will train. Other times you can slowly unfreeze the layers one at a time. You can also use unlabelled data when pre-training, by masking words and trying to predict which word was masked.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*n49fDyrzIEeiUXqG.jpeg" alt="image" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-do-transformers-need-positional-encodings">Why do transformers need <em>Positional Encodings</em>?<a href="#why-do-transformers-need-positional-encodings" class="hash-link" aria-label="Direct link to why-do-transformers-need-positional-encodings" title="Direct link to why-do-transformers-need-positional-encodings">​</a></h2>
<p>Consider the input sentence - &quot;<em>I am good</em>&quot;.</p>
<p>In <strong>RNNs</strong>, we feed the sentence to the network <em>word by word</em>. That is, first the word &quot;<code>I</code>&quot; is passed as input, next the word &quot;<code>am</code>&quot; is passed, and so on. We feed the sentence word by word so that our network understands the sentence completely.</p>
<p>But with the <strong>transformer network</strong>, we don&#x27;t follow the recurrence mechanism. So, instead of feeding the sentence word by word, we feed all the words in the sentence <strong>parallel</strong> to the network. Feeding the words in parallel helps in decreasing the training time and also helps in learning the long-term dependency.</p>
<p>When we feed the words parallel to the transformer, the word order (position of the words in the sentence) is important. So, we should give some information about the word order to the transformer so that it can understand the sentence.</p>
<p>If we pass the input matrix directly to the transformer, it cannot understand the word order. So, instead of feeding the input matrix directly to the transformer, we need to add some information indicating the word order (position of the word) so that our network can understand the meaning of the sentence. To do this, we introduce a technique called <strong>positional encoding</strong>. Positional encoding, as the name suggests, is an <strong>encoding</strong> indicating the position of the word in a sentence (word order).</p>
<p><img decoding="async" loading="lazy" src="https://machinelearningmastery.com/wp-content/uploads/2022/01/PE6.png" alt="https://machinelearningmastery.com/wp-content/uploads/2022/01/PE6.png" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-difference-between-monotonic-alignment-and-predictive-alignment-in-transformers">What&#x27;s the difference between <em>Monotonic alignment</em> and <em>Predictive alignment</em> in transformers?<a href="#whats-the-difference-between-monotonic-alignment-and-predictive-alignment-in-transformers" class="hash-link" aria-label="Direct link to whats-the-difference-between-monotonic-alignment-and-predictive-alignment-in-transformers" title="Direct link to whats-the-difference-between-monotonic-alignment-and-predictive-alignment-in-transformers">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-openai-gpt-model-temperature">What is OpenAI GPT model temperature?<a href="#what-is-openai-gpt-model-temperature" class="hash-link" aria-label="Direct link to What is OpenAI GPT model temperature?" title="Direct link to What is OpenAI GPT model temperature?">​</a></h2>
<p>Temperature is a parameter of OpenAI ChatGPT, GPT-3 and GPT-4 models that governs the randomness and thus the creativity of the responses.</p>
<p>It is always a number between 0 and 1. A temperature of 0 means the responses will be very straightforward, almost deterministic (meaning you almost always get the same response to a given prompt) A temperature of 1 means the responses can vary wildly.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-exactly-does-the-temperature-work">How exactly does the temperature work?<a href="#how-exactly-does-the-temperature-work" class="hash-link" aria-label="Direct link to How exactly does the temperature work?" title="Direct link to How exactly does the temperature work?">​</a></h3>
<p>Under the hood, large language models try to predict the next best word given a prompt. One word at a time. They assign a probability to each word in their vocabulary, and then picks a word among those.</p>
<p>A temperature of 0 means roughly that the model will always select the highest probability word. A higher temperature means that the model might select a word with slightly lower probability, leading to more variation, randomness and creativity. A very high temperature therefore increases the risk of &quot;hallucination&quot;, meaning that the AI starts selecting words that will make no sense or be offtopic.</p>
<p>Since all characters count, the ratio of words to tokens is language dependent.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rules-of-thumb-for-temperature-choice">Rules of thumb for temperature choice<a href="#rules-of-thumb-for-temperature-choice" class="hash-link" aria-label="Direct link to Rules of thumb for temperature choice" title="Direct link to Rules of thumb for temperature choice">​</a></h3>
<p>Your choice of temperature should depend on the task you are giving GPT.</p>
<p>For transformation tasks (extraction, standardization, format conversion, grammar fixes) prefer a temperature of 0 or up to 0.3. For writing tasks, you should juice the temperature higher, closer to 0.5. If you want GPT to be highly creative (for marketing or advertising copy for instance), consider values between 0.7 and 1.</p>
<p>If you want to experiment and create many variations quickly, a high temperature is better.</p>
<p><a href="https://gptforwork.com/guides/openai-gpt3-temperature" target="_blank" rel="noopener noreferrer">How to use OpenAI GPT-3 temperature • Gptforwork.com</a></p>
<p><a href="https://platform.openai.com/docs/guides/flex-processing" target="_blank" rel="noopener noreferrer">Flex processing - OpenAI Platform</a></p>
<ul>
<li>Flex processing provides significantly lower costs for <a href="https://platform.openai.com/docs/api-reference/chat" target="_blank" rel="noopener noreferrer">Chat Completions</a> or <a href="https://platform.openai.com/docs/api-reference/responses" target="_blank" rel="noopener noreferrer">Responses</a> requests in exchange for slower response times and occasional resource unavailability. It is ideal for non-production or lower-priority tasks such as model evaluations, data enrichment, or asynchronous workloads.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="topics">Topics<a href="#topics" class="hash-link" aria-label="Direct link to Topics" title="Direct link to Topics">​</a></h2>
<ul>
<li>token embeddings</li>
<li>positional embeddings</li>
<li>self-attention</li>
<li>transformers</li>
<li>intuitive understanding of Q, K, V</li>
<li>causal and multi-head attention</li>
<li>temperature, top-k, top-p</li>
<li>classification &amp; instruction fine-tuning</li>
<li>rotary positional encoding (RoPE)</li>
<li>KV Cache</li>
<li>infini-attention (long context windows)</li>
<li>mixture of experts (MoE)<!-- -->
<ul>
<li><a href="https://youtu.be/pctR3sdCJgQ" target="_blank" rel="noopener noreferrer">Mixture of Memory Experts (MoME) | Data Brew | Episode 36</a></li>
</ul>
</li>
<li>grouped query attention</li>
<li>basic understanding of llama-2 architecture and techniques which is actually a recap of all the previous subjects</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="links">Links<a href="#links" class="hash-link" aria-label="Direct link to Links" title="Direct link to Links">​</a></h2>
<p><a href="https://www.mlstack.cafe/blog/large-language-models-llms-interview-questions" target="_blank" rel="noopener noreferrer">MLStack.Cafe - Kill Your Next Machine Learning, Data Science &amp; Python Interview. Find your next ML Job.</a></p>
<p><a href="https://medium.com/machine-learning-mindset/top-20-interview-questions-answers-on-llms-generative-ai-42a2e16cb276" target="_blank" rel="noopener noreferrer">Top 20 Interview Questions &amp; Answers on LLMs &amp; Generative AI | by amirsina torfi | Machine Learning Mindset | Nov, 2023 | Medium</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/llm/interview-questions.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-05-03T17:16:39.000Z" itemprop="dateModified">May 3, 2025</time></b> by <b>Deepak</b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai/llm/interview-questions-top-50"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Top 50 Large Language Model (LLM) Interview</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai/llm/intro"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Intro</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#can-you-provide-a-high-level-overview-of-transformers-architecture" class="table-of-contents__link toc-highlight">Can you provide a high-level overview of Transformers&#39; architecture?</a></li><li><a href="#how-next-sentence-prediction-nsp-is-used-in-language-modeling" class="table-of-contents__link toc-highlight">How <em>next sentence prediction (NSP)</em> is used in language modeling?</a></li><li><a href="#how-can-you-evaluate-the-performance-of-language-models" class="table-of-contents__link toc-highlight">How can you <em>evaluate the performance</em> of Language Models?</a></li><li><a href="#how-do-generative-language-models-work" class="table-of-contents__link toc-highlight">How do <em>generative language models</em> work?</a></li><li><a href="#what-is-a-token-in-the-large-language-models-context" class="table-of-contents__link toc-highlight">What is a <em>token</em> in the Large Language Models context?</a></li><li><a href="#whats-the-advantage-of-using-transformer-based-vs-lstm-based-architectures-in-nlp" class="table-of-contents__link toc-highlight">What&#39;s the advantage of using transformer-based vs LSTM-based architectures in NLP?</a></li><li><a href="#can-you-provide-some-examples-of-alignment-problems-in-large-language-models" class="table-of-contents__link toc-highlight">Can you provide some examples of <em>alignment problems</em> in Large Language Models?</a></li><li><a href="#how-adaptative-softmax-is-useful-in-large-language-models" class="table-of-contents__link toc-highlight">How <em>Adaptative Softmax</em> is useful in Large Language Models?</a></li><li><a href="#how-does-bert-training-work" class="table-of-contents__link toc-highlight">How does BERT <em>training</em> work?</a></li><li><a href="#how-is-the-transformer-network-better-than-cnns-and-rnns" class="table-of-contents__link toc-highlight">How is the <em>Transformer Network</em> better than <em>CNNs</em> and <em>RNNs</em>?</a></li><li><a href="#is-there-a-way-to-train-a-large-language-model-llm-to-store-a-specific-context" class="table-of-contents__link toc-highlight">Is there a way to train a Large Language Model (LLM) to store a specific context?</a></li><li><a href="#what-transfer-learning-techniques-can-you-use-in-llms" class="table-of-contents__link toc-highlight">What <em>Transfer learning Techniques</em> can you use in LLMs?</a></li><li><a href="#what-is-transfer-learning-and-why-is-it-important" class="table-of-contents__link toc-highlight">What is <em>Transfer Learning</em> and why is it important?</a></li><li><a href="#whats-the-difference-between-encoder-vs-decoder-models" class="table-of-contents__link toc-highlight">What&#39;s the difference between <em>Encoder</em> vs <em>Decoder</em> models?</a></li><li><a href="#whats-the-difference-between-wordpiece-vs-bpe" class="table-of-contents__link toc-highlight">What&#39;s the difference between <em>Wordpiece</em> vs <em>BPE</em>?</a></li><li><a href="#whats-the-difference-between-global-and-local-attention-in-llms" class="table-of-contents__link toc-highlight">What&#39;s the difference between <em>Global</em> and <em>Local Attention</em> in LLMs?</a></li><li><a href="#whats-the-difference-between-next-token-prediction-vs-masked-language-modeling-in-llm" class="table-of-contents__link toc-highlight">What&#39;s the difference between <em>next-token-prediction</em> vs <em>masked-language-modeling</em> in LLM?</a></li><li><a href="#why-a-multi-head-attention-mechanism-is-needed-in-a-transformer-based-architecture" class="table-of-contents__link toc-highlight">Why a <em>Multi-Head Attention</em> mechanism is needed in a <em>Transformer-based Architecture</em>?</a></li><li><a href="#why-would-you-use-encoder-decoder-rnns-vs-plain-sequence-to-sequence-rnns-for-automatic-translation" class="table-of-contents__link toc-highlight">Why would you use <em>Encoder-Decoder RNNs</em> vs <em>plain sequence-to-sequence RNNs</em> for <em>automatic translation</em>?</a></li><li><a href="#explain-what-is-self-attention-mechanism-in-the-transformer-architecture" class="table-of-contents__link toc-highlight">Explain what is <em>Self-Attention</em> mechanism in the Transformer architecture?</a></li><li><a href="#how-does-transfer-learning-work-in-llms" class="table-of-contents__link toc-highlight">How does <em>Transfer Learning</em> work in LLMs?</a></li><li><a href="#how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network" class="table-of-contents__link toc-highlight">How does an LLM <em>parameter</em> relate to a <em>weight</em> in a Neural Network?</a></li><li><a href="#what-are-some-downsides-of-fine-tuning-llms" class="table-of-contents__link toc-highlight">What are some downsides of <em>fine-tuning</em> LLMs?</a></li><li><a href="#what-is-the-difference-between-word-embedding-position-embedding-and-positional-encoding-in-bert" class="table-of-contents__link toc-highlight">What is the difference between <em>Word Embedding</em>, <em>Position Embedding</em> and <em>Positional Encoding</em> in BERT?</a></li><li><a href="#whats-the-difference-between-feature-based-transfer-learning-vs-fine-tuning-in-llms" class="table-of-contents__link toc-highlight">What’s the difference between <em>Feature-based Transfer Learning</em> vs. <em>Fine Tuning</em> in LLMs?</a></li><li><a href="#why-do-transformers-need-positional-encodings" class="table-of-contents__link toc-highlight">Why do transformers need <em>Positional Encodings</em>?</a></li><li><a href="#whats-the-difference-between-monotonic-alignment-and-predictive-alignment-in-transformers" class="table-of-contents__link toc-highlight">What&#39;s the difference between <em>Monotonic alignment</em> and <em>Predictive alignment</em> in transformers?</a></li><li><a href="#what-is-openai-gpt-model-temperature" class="table-of-contents__link toc-highlight">What is OpenAI GPT model temperature?</a><ul><li><a href="#how-exactly-does-the-temperature-work" class="table-of-contents__link toc-highlight">How exactly does the temperature work?</a></li><li><a href="#rules-of-thumb-for-temperature-choice" class="table-of-contents__link toc-highlight">Rules of thumb for temperature choice</a></li></ul></li><li><a href="#topics" class="table-of-contents__link toc-highlight">Topics</a></li><li><a href="#links" class="table-of-contents__link toc-highlight">Links</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Deep Notes, Built with ❤️</div></div></div></footer></div>
</body>
</html>