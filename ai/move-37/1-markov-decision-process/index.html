<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ai/move-37/1-markov-decision-process" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">1. Markov Decision Process | Deep Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://deepaksood619.github.io/img/timeline_compressed.webp"><meta data-rh="true" name="twitter:image" content="https://deepaksood619.github.io/img/timeline_compressed.webp"><meta data-rh="true" property="og:url" content="https://deepaksood619.github.io/ai/move-37/1-markov-decision-process"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="1. Markov Decision Process | Deep Notes"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepaksood619.github.io/ai/move-37/1-markov-decision-process"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/move-37/1-markov-decision-process" hreflang="en"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/move-37/1-markov-decision-process" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://X3OY8NGHVH-dsn.algolia.net" crossorigin="anonymous"><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZSZMJXWSH3"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZSZMJXWSH3",{})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-TN3KBF4",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="search" type="application/opensearchdescription+xml" title="Deep Notes" href="/opensearch.xml">
<link rel="icon" href="/img/logo.webp">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(53, 120, 229)">

<script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5616865131274062" async crossorigin="anonymous"></script><link rel="stylesheet" href="/assets/css/styles.de817004.css">
<script src="/assets/js/runtime~main.4afceeeb.js" defer="defer"></script>
<script src="/assets/js/main.53197a82.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TN3KBF4" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Notes</b></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/deepaksood619" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://www.linkedin.com/in/deepaksood619/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/about-deepak-sood/">About Deepak Sood</a><button aria-label="Expand sidebar category &#x27;About Deepak Sood&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ai/">AI</a><button aria-label="Collapse sidebar category &#x27;AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/big-data/">Big Data</a><button aria-label="Expand sidebar category &#x27;Big Data&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/computer-vision-cv/">Computer Vision</a><button aria-label="Expand sidebar category &#x27;Computer Vision&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/content-moderation">Content Moderation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-science/">Data Science</a><button aria-label="Expand sidebar category &#x27;Data Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-visualization/">Data Visualization</a><button aria-label="Expand sidebar category &#x27;Data Visualization&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/deep-learning/">Deep Learning</a><button aria-label="Expand sidebar category &#x27;Deep Learning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathons">Hackathons</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/libraries/">Libraries</a><button aria-label="Expand sidebar category &#x27;Libraries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/llm/">ChatGPT / LLM</a><button aria-label="Expand sidebar category &#x27;ChatGPT / LLM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-algorithms/">ML Algorithms</a><button aria-label="Expand sidebar category &#x27;ML Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-fundamentals/">ML Fundamentals</a><button aria-label="Expand sidebar category &#x27;ML Fundamentals&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/model-evaluation/">Model Evaluation</a><button aria-label="Expand sidebar category &#x27;Model Evaluation&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai/move-37/">Move37</a><button aria-label="Collapse sidebar category &#x27;Move37&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai/move-37/1-markov-decision-process">1. Markov Decision Process</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/2-dynamic-programming">2. Dynamic Programming</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/3-monte-carlo-methods">3. Monte Carlo Methods</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/4-model-free-learning">4. Model Free Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/5-rl-in-continuous-space">5. RL in Continuous Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/algorithms">Algorithms</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/open-ai-gym">Open AI Gym</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/others">Others</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/q-learning-algorithms">Q-Learning Algorithms</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/quizzes">Quizzes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/reinforcement-learning">Reinforcement Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/syllabus">Syllabus</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/nlp/">NLP</a><button aria-label="Expand sidebar category &#x27;NLP&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/numpy/">Numpy</a><button aria-label="Expand sidebar category &#x27;Numpy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/others-resources-interview-learning-courses">Others / Resources / Interview / Learning / Courses</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/pandas/">Pandas</a><button aria-label="Expand sidebar category &#x27;Pandas&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/scikit-learn/">Scikit Learn / Scipy</a><button aria-label="Expand sidebar category &#x27;Scikit Learn / Scipy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/social-media-analytics-solution">Social Media Analytics Solution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/solutions">Solutions</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/algorithms/">Algorithms</a><button aria-label="Expand sidebar category &#x27;Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/book-summaries/">Book Summaries</a><button aria-label="Expand sidebar category &#x27;Book Summaries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/cloud/">Cloud</a><button aria-label="Expand sidebar category &#x27;Cloud&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/computer-science/">Computer Science</a><button aria-label="Expand sidebar category &#x27;Computer Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/courses/">Courses / Certifications</a><button aria-label="Expand sidebar category &#x27;Courses / Certifications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-structures/">Data Structures</a><button aria-label="Expand sidebar category &#x27;Data Structures&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases/">Databases</a><button aria-label="Expand sidebar category &#x27;Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/decentralized-applications/">Decentralized Applications</a><button aria-label="Expand sidebar category &#x27;Decentralized Applications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/devops/">DevOps</a><button aria-label="Expand sidebar category &#x27;DevOps&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/economics/">Economics</a><button aria-label="Expand sidebar category &#x27;Economics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/frontend/">Frontend</a><button aria-label="Expand sidebar category &#x27;Frontend&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/knowledge/">Knowledge</a><button aria-label="Expand sidebar category &#x27;Knowledge&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/languages/">Languages</a><button aria-label="Expand sidebar category &#x27;Languages&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/management/">Management</a><button aria-label="Expand sidebar category &#x27;Management&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/mathematics/">Mathematics</a><button aria-label="Expand sidebar category &#x27;Mathematics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/networking/">Networking</a><button aria-label="Expand sidebar category &#x27;Networking&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/psychology/">Psychology</a><button aria-label="Expand sidebar category &#x27;Psychology&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/python/">Python</a><button aria-label="Expand sidebar category &#x27;Python&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Deepak&#x27;s Wiki</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/technologies/">Technologies</a><button aria-label="Expand sidebar category &#x27;Technologies&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/ai/"><span itemprop="name">AI</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/ai/move-37/"><span itemprop="name">Move37</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">1. Markov Decision Process</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>1. Markov Decision Process</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<ul>
<li>
<p>In Reinforcement Learning, an AI learns how to optimally interact in a real-time environment using the time-delayed labels, called rewards as a signal.</p>
</li>
<li>
<p>The Markov Decision Process is a mathematical framework for defining the reinforcement learning problem using states, actions and rewards.</p>
</li>
<li>
<p>Through interaction with the environment, an AI will learn a policy which will return an action for a given state with the highest reward</p>
</li>
</ul>
<p>Markov Chain - has a set of states in a process that can move successively from one state to another, each move is a single step and is based on a transition model T that defines how to move from one state to the next</p>
<p>Markov Property - It states that given the present, the future is conditionally independent of the past, meaning the state in which the process is now, is dependent only on the state it was at one step ago</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bellman-equation">Bellman Equation<a href="#bellman-equation" class="hash-link" aria-label="Direct link to Bellman Equation" title="Direct link to Bellman Equation">​</a></h2>
<ul>
<li>State - a numeric representation of what the agent is observing at a particular point of time in the environment</li>
<li>Action - the input the agent provides to the environment, calculated by applying a policy to the current state</li>
<li>Reward - a feedback signal from the environment reflecting how well the agent is performing the goals of the game</li>
</ul>
<p>Goal of Reinforcement Learning</p>
<p>Given the current state we are in, choose the optimal action which will maximize the long-term expected reward provided by the environment.</p>
<p>Dynamic programming</p>
<ul>
<li>A class of algorithms, which seek to simplify complex problems, by breaking them up into sub-problems and solving the sub-problems recursively (by a function that calls itself)</li>
</ul>
<p>What question does the Bellman equation answer?</p>
<ul>
<li>Given the state I&#x27;m in, assuming I take the best possible action now and at subsequent step, what long-term reward can I expect?</li>
<li>What is the VALUE of the STATE?</li>
<li>Helps us evaluate the expected reward relative to the advantage or disadvantage of each state</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bellman-equation-for-deterministic-environments">Bellman Equation (For Deterministic Environments)<a href="#bellman-equation-for-deterministic-environments" class="hash-link" aria-label="Direct link to Bellman Equation (For Deterministic Environments)" title="Direct link to Bellman Equation (For Deterministic Environments)">​</a></h2>
<p>States that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.</p>
<p><img decoding="async" loading="lazy" alt="image" src="/assets/images/1.-Markov-Decision-Process-image1-a18d4d1b6d53489a02f20e22c3cb0151.jpg" width="993" height="120" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="y---gamma-is-the-discount-factor">Y - Gamma is the discount factor<a href="#y---gamma-is-the-discount-factor" class="hash-link" aria-label="Direct link to Y - Gamma is the discount factor" title="Direct link to Y - Gamma is the discount factor">​</a></h2>
<p>The discount variable allows us to decide how important the possible future rewards are compared to the present reward.</p>
<p>Gamma Tips</p>
<ul>
<li>It is important to tune this hyperparameter to get optimum results</li>
<li>Successful values range between 0.9 to 0.99</li>
<li>A lower value encourages short-term thinking</li>
<li>A higher value emphasizes long-term rewards</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="markov-decision-processes">Markov Decision Processes<a href="#markov-decision-processes" class="hash-link" aria-label="Direct link to Markov Decision Processes" title="Direct link to Markov Decision Processes">​</a></h2>
<p>MDP is an approach in achieving reinforcement learning to take decisions in a matrix. The MDP tries to capture a world in the form of a grid by dividing it into states, actions, transition matrix, and rewards. The solution to an MDP is called a policy and the objective is to find the optimal policy for a task that MDP is imposed.</p>
<ul>
<li><strong>State</strong></li>
</ul>
<p>AStateis a set of tokens that represent every condition that the agent can be in.</p>
<ul>
<li><strong>Model</strong></li>
</ul>
<p>AModel(sometimes called Transition Model) gives an action&#x27;s effect in a state. In particular, T(S, a, S&#x27;) defines a transition T where being in state S and taking an action &#x27;a&#x27; takes us to state S&#x27; (S and S&#x27; may be same). For stochastic actions (noisy, non-deterministic) we also define a probability P(S&#x27;|S,a) which represents the probability of reaching a state S&#x27; if action &#x27;a&#x27; is taken in state S.</p>
<ul>
<li><strong>Actions</strong></li>
</ul>
<p>An Action &#x27;a&#x27; is set of all possible decisions. a(s) defines the set of actions that can be taken being in state S.</p>
<ul>
<li><strong>Reward</strong></li>
</ul>
<p>A Reward is a real-valued response to an action. R(s) indicates the reward for simply being in the state S. R(S,a) indicates the reward for being in a state S and taking an action &#x27;a&#x27;. R(S, a, S&#x27;) indicates the reward for being in a state S, taking an action &#x27;a&#x27; and ending up in a state S&#x27;.</p>
<ul>
<li><strong>Policy</strong></li>
</ul>
<p>A policy is a solution to the Markov Decision Process. A policy is a set of actions that are taken by the agent to reach a goal. It indicates the action &#x27;a&#x27; to be taken while in state S. A policy is denoted as <code>&#x27;Pi&#x27; π(s) --&gt;∞</code></p>
<p>π*is called the optimal policy, which maximizes the expected reward. Among all the policies taken, the optimal policy is the one that optimizes to maximize the amount of reward received or expected to receive over a lifetime. For an MDP, there&#x27;s no end of the lifetime and you have to decide the end time</p>
<p>Thus, the policy is nothing but a guide telling which action to take for a given state. It is not a plan but uncovers the underlying plan of the environment by returning the actions to take for each state.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="markov-decision-process-mdp-is-a-tuplesatr">Markov Decision Process (MDP) is a tuple(S,A,T,r,?)<a href="#markov-decision-process-mdp-is-a-tuplesatr" class="hash-link" aria-label="Direct link to Markov Decision Process (MDP) is a tuple(S,A,T,r,?)" title="Direct link to Markov Decision Process (MDP) is a tuple(S,A,T,r,?)">​</a></h2>
<p><img decoding="async" loading="lazy" alt="image" src="data:image/jpeg;base64,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" width="300" height="111" class="img_ev3q"></p>
<ul>
<li><strong>&#x27;S&#x27; Set of observations.</strong> The agent observes the environment state as one item of this set.</li>
<li><strong>&#x27;A&#x27; Set of actions.</strong> The set of actions the agent can choose one from to interact with the environment.</li>
<li><strong>&#x27;T&#x27; --P(s&#x27; | s, a)transition probability matrix.</strong> This models what next states&#x27;will be after the agent makes the action a while being in the current state &#x27;s&#x27;.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image" src="data:image/jpeg;base64,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" width="300" height="29" class="img_ev3q"></p>
<ul>
<li><strong>&#x27;r&#x27; --P(r | s, a)reward model</strong> that models what reward the agent will receive when it performs actionawhen it is in state &#x27;s&#x27;.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image" src="data:image/jpeg;base64,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" width="300" height="29" class="img_ev3q"></p>
<ul>
<li><strong>&#x27;?&#x27;: discount factor.</strong> This factor is a numerical value between0and1that represents the relative importance between immediate and future rewards. I.e, If the agent has to select between two actions one of them will give it a high immediate reward immediately after performing the action but will lead into going to state from which the agents expect to get less future rewards than another state that can be reached after doing an action with less immediate reward?</li>
</ul>
<p>In addition to the state value-function, for convenience RL algorithms introduce another function which is the state-action pair<strong>Q function</strong>. Q is a function of a state-action pair and returns a real value.</p>
<p><img decoding="async" loading="lazy" alt="image" src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/wgALCACEA5ABAREA/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/2gAIAQEAAAABnwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAf//EABQQAQAAAAAAAAAAAAAAAAAAAKD/2gAIAQEAAQUCcl//xAAUEAEAAAAAAAAAAAAAAAAAAACg/9oACAEBAAY/AnJf/8QAFBABAAAAAAAAAAAAAAAAAAAAoP/aAAgBAQABPyFyX//aAAgBAQAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD//xAAUEAEAAAAAAAAAAAAAAAAAAACg/9oACAEBAAE/EHJf/9k=" width="912" height="132" class="img_ev3q"></p>
<p>The optimal Q-function**Q*(s, a)**means the expected total reward recieved by an agent starting in <strong>s</strong> and picks action <strong>a</strong>, then will behave optimally afterwards.There,*<em>Q</em>(s, a)**is an indication for how good it is for an agent to pick action a while being in state s.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="value-iteration-vs-policy-iteration">Value-Iteration vs Policy-Iteration<a href="#value-iteration-vs-policy-iteration" class="hash-link" aria-label="Direct link to Value-Iteration vs Policy-Iteration" title="Direct link to Value-Iteration vs Policy-Iteration">​</a></h2>
<p>Both value-iteration and policy-iteration algorithms can be used foroffline planningwhere the agent is assumed to have prior knowledge about the effects of its actions on the environment (they assume the MDP model is known). Comparing to each other, policy-iteration is computationally efficient as it often takes considerably fewer number of iterations to converge although each iteration is more computationally expensive.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="extensions">Extensions<a href="#extensions" class="hash-link" aria-label="Direct link to Extensions" title="Direct link to Extensions">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pomdp---a-partially-observable-markov-decision-process-is-an-mdp-with">POMDP - A Partially Observable Markov Decision Process is an MDP with<a href="#pomdp---a-partially-observable-markov-decision-process-is-an-mdp-with" class="hash-link" aria-label="Direct link to POMDP - A Partially Observable Markov Decision Process is an MDP with" title="Direct link to POMDP - A Partially Observable Markov Decision Process is an MDP with">​</a></h2>
<p>hidden states. It is a hidden Markov model with actions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="sensor-networks">Sensor Networks<a href="#sensor-networks" class="hash-link" aria-label="Direct link to Sensor Networks" title="Direct link to Sensor Networks">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deterministic-policy---action-taken-entirely-depend-on-the-state">Deterministic Policy - Action taken entirely depend on the state<a href="#deterministic-policy---action-taken-entirely-depend-on-the-state" class="hash-link" aria-label="Direct link to Deterministic Policy - Action taken entirely depend on the state" title="Direct link to Deterministic Policy - Action taken entirely depend on the state">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stochastic-policy---action-is-choosen-using-some-random-factor">Stochastic Policy - Action is choosen using some random factor<a href="#stochastic-policy---action-is-choosen-using-some-random-factor" class="hash-link" aria-label="Direct link to Stochastic Policy - Action is choosen using some random factor" title="Direct link to Stochastic Policy - Action is choosen using some random factor">​</a></h2>
<p><img decoding="async" loading="lazy" alt="image" src="/assets/images/1.-Markov-Decision-Process-image6-b1a535380602888df78c41db945bc2ef.jpg" width="998" height="364" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="image" src="/assets/images/1.-Markov-Decision-Process-image7-bd1ce55e8bc796e5f1c2d7a15125eae5.jpg" width="999" height="464" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="image" src="/assets/images/1.-Markov-Decision-Process-image8-effd8e319f80afc175f0b6705bb17896.jpg" width="999" height="454" class="img_ev3q"></p>
<ul>
<li><strong>Optimal Policy - to learn an optimal policy we have to learn an optimal value function, of which there are two kinds, state-action and action-value</strong></li>
<li><strong>We can compute the value function using Bellman equation which expresses the value of any state as the sum of the immediate reward plus the value of the state that follows</strong></li>
<li>Cumulative Future Reward</li>
<li>Discounted Factor - Describes the preference of the agent for the current reward over future reward</li>
<li>State-value function - for each state, state-value function yields the expected return. Are the function of environment state</li>
<li>Action-value function - Are the function of the environment state and the agent&#x27;s action. In action-value function we have 4 values for each state, corresponding to 4 actions (up, down, left, right).</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image" src="/assets/images/1.-Markov-Decision-Process-image9-04161a7d558aabb3f887670e565ad97b.jpg" width="999" height="469" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="google-dopamine">Google Dopamine<a href="#google-dopamine" class="hash-link" aria-label="Direct link to Google Dopamine" title="Direct link to Google Dopamine">​</a></h2>
<p>Dopamine is a research framework for fast prototyping of reinforcement learning algorithms.</p>
<p><a href="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa" target="_blank" rel="noopener noreferrer">https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/1-markov-decision-process.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-02-05T13:06:14.000Z" itemprop="dateModified">Feb 5, 2024</time></b> by <b>Deepak</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai/move-37/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Move37</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai/move-37/2-dynamic-programming"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">2. Dynamic Programming</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#bellman-equation" class="table-of-contents__link toc-highlight">Bellman Equation</a></li><li><a href="#bellman-equation-for-deterministic-environments" class="table-of-contents__link toc-highlight">Bellman Equation (For Deterministic Environments)</a></li><li><a href="#y---gamma-is-the-discount-factor" class="table-of-contents__link toc-highlight">Y - Gamma is the discount factor</a></li><li><a href="#markov-decision-processes" class="table-of-contents__link toc-highlight">Markov Decision Processes</a></li><li><a href="#markov-decision-process-mdp-is-a-tuplesatr" class="table-of-contents__link toc-highlight">Markov Decision Process (MDP) is a tuple(S,A,T,r,?)</a></li><li><a href="#value-iteration-vs-policy-iteration" class="table-of-contents__link toc-highlight">Value-Iteration vs Policy-Iteration</a></li><li><a href="#extensions" class="table-of-contents__link toc-highlight">Extensions</a></li><li><a href="#pomdp---a-partially-observable-markov-decision-process-is-an-mdp-with" class="table-of-contents__link toc-highlight">POMDP - A Partially Observable Markov Decision Process is an MDP with</a></li><li><a href="#sensor-networks" class="table-of-contents__link toc-highlight">Sensor Networks</a></li><li><a href="#deterministic-policy---action-taken-entirely-depend-on-the-state" class="table-of-contents__link toc-highlight">Deterministic Policy - Action taken entirely depend on the state</a></li><li><a href="#stochastic-policy---action-is-choosen-using-some-random-factor" class="table-of-contents__link toc-highlight">Stochastic Policy - Action is choosen using some random factor</a></li><li><a href="#google-dopamine" class="table-of-contents__link toc-highlight">Google Dopamine</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Deep Notes, Built with ❤️</div></div></div></footer></div>
</body>
</html>