<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ai/move-37/3-monte-carlo-methods" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.0">
<title data-rh="true">3. Monte Carlo Methods | Deep Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://deepaksood619.github.io/img/timeline_blurred.webp"><meta data-rh="true" name="twitter:image" content="https://deepaksood619.github.io/img/timeline_blurred.webp"><meta data-rh="true" property="og:url" content="https://deepaksood619.github.io/ai/move-37/3-monte-carlo-methods"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="3. Monte Carlo Methods | Deep Notes"><meta data-rh="true" name="description" content="Internet of Things Optimization"><meta data-rh="true" property="og:description" content="Internet of Things Optimization"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepaksood619.github.io/ai/move-37/3-monte-carlo-methods"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/move-37/3-monte-carlo-methods" hreflang="en"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/move-37/3-monte-carlo-methods" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://X3OY8NGHVH-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AI","item":"https://deepaksood619.github.io/ai/"},{"@type":"ListItem","position":2,"name":"Move37","item":"https://deepaksood619.github.io/ai/move-37/"},{"@type":"ListItem","position":3,"name":"3. Monte Carlo Methods","item":"https://deepaksood619.github.io/ai/move-37/3-monte-carlo-methods"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZSZMJXWSH3"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZSZMJXWSH3",{})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-TN3KBF4",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="search" type="application/opensearchdescription+xml" title="Deep Notes" href="/opensearch.xml">
<link rel="icon" href="/img/logo.webp">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(53, 120, 229)">


<script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5616865131274062" async crossorigin="anonymous"></script><link rel="stylesheet" href="/assets/css/styles.9f55d0bd.css">
<script src="/assets/js/runtime~main.7b3a7cc3.js" defer="defer"></script>
<script src="/assets/js/main.c96be3a1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TN3KBF4" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Notes</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/deepaksood619" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://www.linkedin.com/in/deepaksood619/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/about-deepak-sood/">About Deepak Sood</a><button aria-label="Expand sidebar category &#x27;About Deepak Sood&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ai/">AI</a><button aria-label="Collapse sidebar category &#x27;AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/big-data/">Big Data</a><button aria-label="Expand sidebar category &#x27;Big Data&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/computer-vision-cv/">Computer Vision</a><button aria-label="Expand sidebar category &#x27;Computer Vision&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/content-moderation">Content Moderation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-science/">Data Science</a><button aria-label="Expand sidebar category &#x27;Data Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-visualization/">Data Visualization</a><button aria-label="Expand sidebar category &#x27;Data Visualization&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/deep-learning/">Deep Learning</a><button aria-label="Expand sidebar category &#x27;Deep Learning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathon-hackhound">Hackathon HackHound</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathon-rabbittai">Hackathon Rabbitt AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathons">Hackathons</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/libraries/">Libraries</a><button aria-label="Expand sidebar category &#x27;Libraries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/llm/">LLM</a><button aria-label="Expand sidebar category &#x27;LLM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-algorithms/">ML Algorithms</a><button aria-label="Expand sidebar category &#x27;ML Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-fundamentals/">ML Fundamentals</a><button aria-label="Expand sidebar category &#x27;ML Fundamentals&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/model-evaluation/">Model Evaluation</a><button aria-label="Expand sidebar category &#x27;Model Evaluation&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai/move-37/">Move37</a><button aria-label="Collapse sidebar category &#x27;Move37&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/1-markov-decision-process">1. Markov Decision Process</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/2-dynamic-programming">2. Dynamic Programming</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai/move-37/3-monte-carlo-methods">3. Monte Carlo Methods</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/4-model-free-learning">4. Model Free Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/5-rl-in-continuous-space">5. RL in Continuous Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/algorithms">Algorithms</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/open-ai-gym">Open AI Gym</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/others">Others</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/q-learning-algorithms">Q-Learning Algorithms</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/quizzes">Quizzes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/reinforcement-learning">Reinforcement Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/move-37/syllabus">Syllabus</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/nlp/">NLP</a><button aria-label="Expand sidebar category &#x27;NLP&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/numpy/">Numpy</a><button aria-label="Expand sidebar category &#x27;Numpy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/others-resources-interview-learning-courses">Others / Resources / Interview / Learning / Courses</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/pandas/">Pandas</a><button aria-label="Expand sidebar category &#x27;Pandas&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/scikit-learn/">Scikit Learn / Scipy</a><button aria-label="Expand sidebar category &#x27;Scikit Learn / Scipy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/social-media-analytics-solution">Social Media Analytics Solution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/solutions">Solutions</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/algorithms/">Algorithms</a><button aria-label="Expand sidebar category &#x27;Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/book-summaries/">Book Summaries</a><button aria-label="Expand sidebar category &#x27;Book Summaries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/cloud/">Cloud</a><button aria-label="Expand sidebar category &#x27;Cloud&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/computer-science/">Computer Science</a><button aria-label="Expand sidebar category &#x27;Computer Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/courses/">Courses / Certifications</a><button aria-label="Expand sidebar category &#x27;Courses / Certifications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-structures/">Data Structures</a><button aria-label="Expand sidebar category &#x27;Data Structures&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-warehouses/">Data Warehouses</a><button aria-label="Expand sidebar category &#x27;Data Warehouses&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases/">Databases</a><button aria-label="Expand sidebar category &#x27;Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases-nosql/">NoSQL Databases</a><button aria-label="Expand sidebar category &#x27;NoSQL Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases-sql/">SQL Databases</a><button aria-label="Expand sidebar category &#x27;SQL Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/decentralized-applications/">Decentralized Applications</a><button aria-label="Expand sidebar category &#x27;Decentralized Applications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/devops/">DevOps</a><button aria-label="Expand sidebar category &#x27;DevOps&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/economics/">Economics</a><button aria-label="Expand sidebar category &#x27;Economics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/education/">Education</a><button aria-label="Expand sidebar category &#x27;Education&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/frontend/">Frontend</a><button aria-label="Expand sidebar category &#x27;Frontend&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/knowledge/">Knowledge</a><button aria-label="Expand sidebar category &#x27;Knowledge&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/languages/">Languages</a><button aria-label="Expand sidebar category &#x27;Languages&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/management/">Management</a><button aria-label="Expand sidebar category &#x27;Management&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/mathematics/">Mathematics</a><button aria-label="Expand sidebar category &#x27;Mathematics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/networking/">Networking</a><button aria-label="Expand sidebar category &#x27;Networking&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/psychology/">Psychology</a><button aria-label="Expand sidebar category &#x27;Psychology&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/python/">Python</a><button aria-label="Expand sidebar category &#x27;Python&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Deepak&#x27;s Wiki</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/technologies/">Technologies</a><button aria-label="Expand sidebar category &#x27;Technologies&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai/"><span>AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai/move-37/"><span>Move37</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">3. Monte Carlo Methods</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>3. Monte Carlo Methods</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="internet-of-things-optimization">Internet of Things Optimization<a href="#internet-of-things-optimization" class="hash-link" aria-label="Direct link to Internet of Things Optimization" title="Direct link to Internet of Things Optimization">​</a></h2>
<ul>
<li>Monte Carlo vs Dynamic Programming<!-- -->
<ul>
<li>No need for a complete Markov Decision Process</li>
<li>Computationally more efficient</li>
<li>Can be used with stochastic simulations</li>
</ul>
</li>
<li>In model-free reinforcement learning, as opposed to model based, we don&#x27;t know the reward function and the transition function beforehand we have to learn them though experience.</li>
<li>A model-free learning technique called monte carlo uses repeated random sampling to obtain numerical results</li>
<li>In first visite monte carlo the state value function is defined as the average of the returns following the agents first visit to S in a set of episodes</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="exploration-vs-exploitation">Exploration vs Exploitation<a href="#exploration-vs-exploitation" class="hash-link" aria-label="Direct link to Exploration vs Exploitation" title="Direct link to Exploration vs Exploitation">​</a></h2>
<ul>
<li>Example - In the context of a restaurant, we can order the same meal that we have already eaten, so <strong>exploit</strong> our previous knowledge to get already tested meal, vs we can <strong>explore</strong> new items in the hope of eating more tastier meal.</li>
<li>We can do this in RL by adjusting ϵ (epsilon), to balance between exploration vs exploitation to get the maximum reward possible.</li>
<li>Exploration vs Exploitation tradeoff</li>
<li>Epsilon Greedy</li>
</ul>
<p>epsilon is the fraction of times we sample a lever randomly and1- epsilonis the fraction of times we choose optimally.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="thompson-sampling">Thompson Sampling<a href="#thompson-sampling" class="hash-link" aria-label="Direct link to Thompson Sampling" title="Direct link to Thompson Sampling">​</a></h3>
<p>The basic idea is toassume a simple prior distributionon the underlying parameters of the reward distribution of every lever, and at every time step, play a lever according to itsposterior probabilityof being the best arm.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-armed-bandit-problem">Multi-armed bandit problem<a href="#multi-armed-bandit-problem" class="hash-link" aria-label="Direct link to Multi-armed bandit problem" title="Direct link to Multi-armed bandit problem">​</a></h3>
<p>In <a href="https://en.wikipedia.org/wiki/Probability_theory" target="_blank" rel="noopener noreferrer">probability theory</a>, the <strong>multi-armed bandit problem</strong>(sometimes called the <strong><em>K</em>- or <em>N</em>-armed bandit problem</strong>) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice&#x27;s properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.</p>
<p>The name comes from imagining a <a href="https://en.wikipedia.org/wiki/Gambler" target="_blank" rel="noopener noreferrer">gambler</a> at a row of <a href="https://en.wikipedia.org/wiki/Slot_machines" target="_blank" rel="noopener noreferrer">slot machines</a>(sometimes known as &quot;one-armed bandits&quot;), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.The multi-armed bandit problem also falls into the broad category of <a href="https://en.wikipedia.org/wiki/Stochastic_scheduling" target="_blank" rel="noopener noreferrer">stochastic scheduling</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Multi-armed_bandit" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Multi-armed_bandit</a></p>
<ul>
<li>
<p>We must strike a balance between explore/exploit</p>
</li>
<li>
<p>Epsilon Greedy - epsilon (a hyperparameter) is the probability that our agent will choose a random action instead of following policy</p>
</li>
<li>
<p>Algorithm</p>
<ol>
<li>
<p>generate a random number p, between 0 and 1</p>
</li>
<li>
<p>if p &lt; (1-ε) take the action dictated by policy</p>
</li>
<li>
<p>Otherwise take a random action</p>
</li>
</ol>
</li>
<li>
<p>First visit optimization</p>
<ul>
<li>What happens if we visit the same state more than once</li>
<li>It&#x27;s been proven the subsequent visits doesn&#x27;t change the answer</li>
<li>All we need is the first visit</li>
<li>We throw rest of the data away</li>
</ul>
</li>
<li>
<p>Monte Carlo Q Learning Algorithm</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image" src="/assets/images/3.-Monte-Carlo-Methods-image1-92f4f6d1dc56000c37f54c5b82808541.jpg" width="999" height="626" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mc-control-and-mc-prediction">MC Control and MC Prediction<a href="#mc-control-and-mc-prediction" class="hash-link" aria-label="Direct link to MC Control and MC Prediction" title="Direct link to MC Control and MC Prediction">​</a></h2>
<p>There are two types of tasks in reinforcement learning -- Prediction and Control.</p>
<ul>
<li><strong>Prediction</strong></li>
</ul>
<p>A task that can predict expected total reward from any given state assuming the functionπ(a|s)is given. Prediction calculates the value functionVπ</p>
<p>e.g.<!-- -->:Policyevaluation<!-- --> (Estimate).</p>
<ul>
<li><strong>Control</strong></li>
</ul>
<p>A task that can findpolicyπ(a|s)that maximizes the expected total reward from any given state. Simply if given a policyπcontrol finds an optimal policyπ*.</p>
<p>e.g.<!-- -->:Policy<!-- --> Improvement (Optimize).</p>
<p>Policy Iteration is a combination of prediction and control to find optimal policy.</p>
<p>There are two types of policy learning methods -</p>
<ul>
<li><strong>On policy learning</strong>
<ul>
<li>This methodlearns on the job, it evaluates or improves the policy that used to make the decisions.</li>
<li>We must act based on our current policy</li>
</ul>
</li>
<li><strong>Off policy learning</strong>
<ul>
<li>This method evaluates one policy while following another policy. The earlier is called target policy which may be deterministic and thelatterbehaviorpolicy is stochastic.</li>
<li>Any action is okay</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-free-learning">Model Free Learning<a href="#model-free-learning" class="hash-link" aria-label="Direct link to Model Free Learning" title="Direct link to Model Free Learning">​</a></h3>
<p>Learn a problem when not all the components are available. In model free learning we just focus on calculating the value functions directly from the interactions with the environment. Our aim here is to figureout Vfor unknown MDP assuming that we have a policy.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mc-method---monte-carlo-method-for-model-free-learning">MC Method - Monte Carlo Method for model free learning<a href="#mc-method---monte-carlo-method-for-model-free-learning" class="hash-link" aria-label="Direct link to MC Method - Monte Carlo Method for model free learning" title="Direct link to MC Method - Monte Carlo Method for model free learning">​</a></h2>
<p>There are two different types of MC.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="first-visit-mc-average-returns-only-for-first-time-s-is-visited-in-an-episode">First-visit MC: average returns only for first time s is visited in an episode<a href="#first-visit-mc-average-returns-only-for-first-time-s-is-visited-in-an-episode" class="hash-link" aria-label="Direct link to First-visit MC: average returns only for first time s is visited in an episode" title="Direct link to First-visit MC: average returns only for first time s is visited in an episode">​</a></h3>
<ul>
<li>Incremental Mean</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="every-visit-mc-average-returns-for-every-time-s-is-visited-in-an-episode">Every-Visit MC: average returns for every time s is visited in an episode<a href="#every-visit-mc-average-returns-for-every-time-s-is-visited-in-an-episode" class="hash-link" aria-label="Direct link to Every-Visit MC: average returns for every time s is visited in an episode" title="Direct link to Every-Visit MC: average returns for every time s is visited in an episode">​</a></h3>
<p>Both converge asymptotically.</p>
<p>Monte Carlo methods for control task (Policy iteration is the base of control task)</p>
<p>In model-free RL, we need to interact with the environment to find out the best strategy so we need to explore the entire the state space while figuring out best actions.</p>
<ul>
<li>Exploration<!-- -->:is<!-- --> about finding more information about the environment. In other words exploring a lot of states and actions in the environment.</li>
<li>Exploitation<!-- -->:is<!-- --> about exploiting the known information to maximize the reward.</li>
</ul>
<p>Example for exploration vs exploitation (in context of Roomba, floor cleaning robot) - When the state is in charged up mode It needs to cover maximum area in a grid world for cleaning which falls under exploration. When the state of the machine changes to low battery it needs to find the charging dock as soon as possible to avoid getting stuck here the robot needs to exploit rather than explore to maximize the reward.So due to the exploration problem, we cannot expect Roomba to act greedily in MC to improve policy instead we use the epsilon-greedy policy.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="epsilon-greedy-policy">Epsilon-greedy policy<a href="#epsilon-greedy-policy" class="hash-link" aria-label="Direct link to Epsilon-greedy policy" title="Direct link to Epsilon-greedy policy">​</a></h2>
<p>The best-known action based on our experience is selected with (1-epsilon) probability and the rest of the time i.e. with epsilon probability any action is selected randomly. Initially, epsilon is 1 so we can explore more but as we do many iterations we slowly decrease the epsilon to 0( which is exploitation → choosing the best-known action) this get us to have the value of epsilon between 0 and 1.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="glie-monte-carlo-method-greedy-in-the-limit-of-infinite-exploration">GLIE Monte Carlo Method (Greedy in the Limit of Infinite Exploration)<a href="#glie-monte-carlo-method-greedy-in-the-limit-of-infinite-exploration" class="hash-link" aria-label="Direct link to GLIE Monte Carlo Method (Greedy in the Limit of Infinite Exploration)" title="Direct link to GLIE Monte Carlo Method (Greedy in the Limit of Infinite Exploration)">​</a></h2>
<p>GLIE Monte-Carlo is an on-policy learning method that learns from complete episodes. For each state-action pair, we keep track of how many times the pair has been visited with a simple counter function.</p>
<p>N(St,At) ← N(St,At) + 1</p>
<p>For each episode, we can update our estimated value function using an incremental mean.</p>
<p>Q(St,At) ← Q(St,At) + (1 / N(St,At)) (Gt--Q(St,At))</p>
<p>Here, Gt either represents the return from time t when the agent first visited the state-action pair, or the sum of returns from each time t that the agent visited the state-action pair, depending on whether you are using first-visit or every-visit Monte Carlo.</p>
<p>We&#x27;ll adopt a ϵ-greedy policy with ϵ=1/k where k represents the number of episodes our agent has learned from.</p>
<p><img decoding="async" loading="lazy" alt="image" src="/assets/images/3.-Monte-Carlo-Methods-image2-5844575c4be2f2d8372a969cf468f4b0.jpg" width="999" height="617" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="temporal-difference-learning">Temporal Difference Learning<a href="#temporal-difference-learning" class="hash-link" aria-label="Direct link to Temporal Difference Learning" title="Direct link to Temporal Difference Learning">​</a></h2>
<p>The temporal difference approach approximates the value of a state-action pair by comparing estimates at two points in time (thus the name, temporal difference). The intuition behind this method is that we can formulate a better estimate for the value of a state-action pair after having observed some of the reward that our agent accumulates after having visited a state and performing a given action.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-learning-for-trading">Q-Learning for Trading<a href="#q-learning-for-trading" class="hash-link" aria-label="Direct link to Q-Learning for Trading" title="Direct link to Q-Learning for Trading">​</a></h2>
<ul>
<li>A partially observable markov decision process is one where we don&#x27;t know what the true state looks like, but we can observe a part of it.</li>
<li>A Q table is one where the states and rows and actions are columns, it helps us find the best action to take for each state</li>
<li>Q learning is the process of learning what this Q table is directly, without needing to learn either the transition probability or reward function</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/move-37/3-monte-carlo-methods.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-02-05T13:06:14.000Z" itemprop="dateModified">Feb 5, 2024</time></b> by <b>Deepak</b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai/move-37/2-dynamic-programming"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">2. Dynamic Programming</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai/move-37/4-model-free-learning"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">4. Model Free Learning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#internet-of-things-optimization" class="table-of-contents__link toc-highlight">Internet of Things Optimization</a></li><li><a href="#exploration-vs-exploitation" class="table-of-contents__link toc-highlight">Exploration vs Exploitation</a><ul><li><a href="#thompson-sampling" class="table-of-contents__link toc-highlight">Thompson Sampling</a></li><li><a href="#multi-armed-bandit-problem" class="table-of-contents__link toc-highlight">Multi-armed bandit problem</a></li></ul></li><li><a href="#mc-control-and-mc-prediction" class="table-of-contents__link toc-highlight">MC Control and MC Prediction</a><ul><li><a href="#model-free-learning" class="table-of-contents__link toc-highlight">Model Free Learning</a></li></ul></li><li><a href="#mc-method---monte-carlo-method-for-model-free-learning" class="table-of-contents__link toc-highlight">MC Method - Monte Carlo Method for model free learning</a><ul><li><a href="#first-visit-mc-average-returns-only-for-first-time-s-is-visited-in-an-episode" class="table-of-contents__link toc-highlight">First-visit MC: average returns only for first time s is visited in an episode</a></li><li><a href="#every-visit-mc-average-returns-for-every-time-s-is-visited-in-an-episode" class="table-of-contents__link toc-highlight">Every-Visit MC: average returns for every time s is visited in an episode</a></li></ul></li><li><a href="#epsilon-greedy-policy" class="table-of-contents__link toc-highlight">Epsilon-greedy policy</a></li><li><a href="#glie-monte-carlo-method-greedy-in-the-limit-of-infinite-exploration" class="table-of-contents__link toc-highlight">GLIE Monte Carlo Method (Greedy in the Limit of Infinite Exploration)</a></li><li><a href="#temporal-difference-learning" class="table-of-contents__link toc-highlight">Temporal Difference Learning</a></li><li><a href="#q-learning-for-trading" class="table-of-contents__link toc-highlight">Q-Learning for Trading</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Deep Notes, Built with ❤️</div></div></div></footer></div>
</body>
</html>