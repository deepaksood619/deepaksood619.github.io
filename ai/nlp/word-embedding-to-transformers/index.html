<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ai/nlp/word-embedding-to-transformers" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Word Embedding to Transformers | Deep Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://deepaksood619.github.io/img/timeline_compressed.webp"><meta data-rh="true" name="twitter:image" content="https://deepaksood619.github.io/img/timeline_compressed.webp"><meta data-rh="true" property="og:url" content="https://deepaksood619.github.io/ai/nlp/word-embedding-to-transformers"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Word Embedding to Transformers | Deep Notes"><meta data-rh="true" name="description" content="1. Introduction"><meta data-rh="true" property="og:description" content="1. Introduction"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepaksood619.github.io/ai/nlp/word-embedding-to-transformers"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/nlp/word-embedding-to-transformers" hreflang="en"><link data-rh="true" rel="alternate" href="https://deepaksood619.github.io/ai/nlp/word-embedding-to-transformers" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://X3OY8NGHVH-dsn.algolia.net" crossorigin="anonymous"><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZSZMJXWSH3"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZSZMJXWSH3",{})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-TN3KBF4",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="search" type="application/opensearchdescription+xml" title="Deep Notes" href="/opensearch.xml">
<link rel="icon" href="/img/logo.webp">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(53, 120, 229)">

<script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5616865131274062" async crossorigin="anonymous"></script><link rel="stylesheet" href="/assets/css/styles.de817004.css">
<script src="/assets/js/runtime~main.3ac108ad.js" defer="defer"></script>
<script src="/assets/js/main.1e5cb5d7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TN3KBF4" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.webp" alt="Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Notes</b></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/deepaksood619" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://www.linkedin.com/in/deepaksood619/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/about-deepak-sood/">About Deepak Sood</a><button aria-label="Expand sidebar category &#x27;About Deepak Sood&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ai/">AI</a><button aria-label="Collapse sidebar category &#x27;AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/big-data/">Big Data</a><button aria-label="Expand sidebar category &#x27;Big Data&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/computer-vision-cv/">Computer Vision</a><button aria-label="Expand sidebar category &#x27;Computer Vision&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/content-moderation">Content Moderation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-science/">Data Science</a><button aria-label="Expand sidebar category &#x27;Data Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/data-visualization/">Data Visualization</a><button aria-label="Expand sidebar category &#x27;Data Visualization&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/deep-learning/">Deep Learning</a><button aria-label="Expand sidebar category &#x27;Deep Learning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/hackathons">Hackathons</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/libraries/">Libraries</a><button aria-label="Expand sidebar category &#x27;Libraries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/llm/">ChatGPT / LLM</a><button aria-label="Expand sidebar category &#x27;ChatGPT / LLM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-algorithms/">ML Algorithms</a><button aria-label="Expand sidebar category &#x27;ML Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/ml-fundamentals/">ML Fundamentals</a><button aria-label="Expand sidebar category &#x27;ML Fundamentals&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/model-evaluation/">Model Evaluation</a><button aria-label="Expand sidebar category &#x27;Model Evaluation&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/move-37/">Move37</a><button aria-label="Expand sidebar category &#x27;Move37&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai/nlp/">NLP</a><button aria-label="Collapse sidebar category &#x27;NLP&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/nlp/chatbot-chatops">Chatbot / chatops</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/nlp/chatbot-saas">Chatbot SAAS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/nlp/intro">NLP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/nlp/nlp-concepts">NLP Concepts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/nlp/nltk">NLTK</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai/nlp/word-embedding-to-transformers">Word Embedding to Transformers</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/numpy/">Numpy</a><button aria-label="Expand sidebar category &#x27;Numpy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/others-resources-interview-learning-courses">Others / Resources / Interview / Learning / Courses</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/pandas/">Pandas</a><button aria-label="Expand sidebar category &#x27;Pandas&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai/scikit-learn/">Scikit Learn / Scipy</a><button aria-label="Expand sidebar category &#x27;Scikit Learn / Scipy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/social-media-analytics-solution">Social Media Analytics Solution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai/solutions">Solutions</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/algorithms/">Algorithms</a><button aria-label="Expand sidebar category &#x27;Algorithms&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/book-summaries/">Book Summaries</a><button aria-label="Expand sidebar category &#x27;Book Summaries&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/cloud/">Cloud</a><button aria-label="Expand sidebar category &#x27;Cloud&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/computer-science/">Computer Science</a><button aria-label="Expand sidebar category &#x27;Computer Science&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/courses/">Courses / Certifications</a><button aria-label="Expand sidebar category &#x27;Courses / Certifications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-structures/">Data Structures</a><button aria-label="Expand sidebar category &#x27;Data Structures&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/databases/">Databases</a><button aria-label="Expand sidebar category &#x27;Databases&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/decentralized-applications/">Decentralized Applications</a><button aria-label="Expand sidebar category &#x27;Decentralized Applications&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/devops/">DevOps</a><button aria-label="Expand sidebar category &#x27;DevOps&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/economics/">Economics</a><button aria-label="Expand sidebar category &#x27;Economics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/frontend/">Frontend</a><button aria-label="Expand sidebar category &#x27;Frontend&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/knowledge/">Knowledge</a><button aria-label="Expand sidebar category &#x27;Knowledge&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/languages/">Languages</a><button aria-label="Expand sidebar category &#x27;Languages&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/management/">Management</a><button aria-label="Expand sidebar category &#x27;Management&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/mathematics/">Mathematics</a><button aria-label="Expand sidebar category &#x27;Mathematics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/networking/">Networking</a><button aria-label="Expand sidebar category &#x27;Networking&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/psychology/">Psychology</a><button aria-label="Expand sidebar category &#x27;Psychology&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/python/">Python</a><button aria-label="Expand sidebar category &#x27;Python&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Deepak&#x27;s Wiki</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/technologies/">Technologies</a><button aria-label="Expand sidebar category &#x27;Technologies&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/ai/"><span itemprop="name">AI</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/ai/nlp/"><span itemprop="name">NLP</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Word Embedding to Transformers</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Word Embedding to Transformers</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-introduction">1. Introduction<a href="#1-introduction" class="hash-link" aria-label="Direct link to 1. Introduction" title="Direct link to 1. Introduction">​</a></h2>
<p>Natural language processing (NLP) is an active research field of linguistic, computer science and artificial intelligence. The main goal of NLP is the capability of a computer to understand content in texts or documents. There are many different challenging tasks to solve in the field of NLP:</p>
<ul>
<li><strong>Machine Translation:</strong> is the task of translating a sentence x from one language to a sentence y in another language. Therefore one of the most popular example is: <a href="http://www.deepl.com/" target="_blank" rel="noopener noreferrer">www.deepl.com</a></li>
<li><strong>Text classification:</strong> Text classification is the process of understanding the meaning of unstructured text and organizing it into predefined categories (tags). One of the most popular text classification task is sentiment analysis, which aims to categorize unstructured data by sentiments. A basic task in text classification / sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level-whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral.</li>
<li><strong>Semantic analysis:</strong> Semantic tasks analyze the structure of sentences, word interactions, and related concepts, in an attempt to discover the meaning of words, as well as understand the topic of a text.</li>
<li><strong>Part-of-speech tagging:</strong> Part-of-speech (PoS) tagging is a popular natural language processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context. PoS tagging is useful for identifying relationships between words and, therefore, understand the meaning of sentences.</li>
<li><strong>Text summarization:</strong> Text summarization in NLP is the process of summarizing the most important informations in large texts for quicker consumption.</li>
<li><strong>Generative models for NLP:</strong> Generative models are normally trained on a large amount of data, with the ability to create afterwards new texts.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-nltk---natural-language-toolkit">2. NLTK - natural language toolkit<a href="#2-nltk---natural-language-toolkit" class="hash-link" aria-label="Direct link to 2. NLTK - natural language toolkit" title="Direct link to 2. NLTK - natural language toolkit">​</a></h2>
<p>NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-tokenizing">2.1 Tokenizing<a href="#21-tokenizing" class="hash-link" aria-label="Direct link to 2.1 Tokenizing" title="Direct link to 2.1 Tokenizing">​</a></h3>
<p>A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. Normally, it is the first step in each NLP project. The two most common versions of tokenizers are the &quot;word based tokenizing&quot; and the &quot;sentence based tokenizing&quot;.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="tokenizing-by-words">Tokenizing by words<a href="#tokenizing-by-words" class="hash-link" aria-label="Direct link to Tokenizing by words" title="Direct link to Tokenizing by words">​</a></h4>
<p>Words are like the atoms of natural language. They are the smallest unit of meaning that still makes sense on its own. Tokenizing a text by word allows to identify words that come up particularly often. For example, if you were analyzing a group of job postings, then you might find that the word &quot;Python&quot; comes up often. That could suggest high demand for Python knowledge, but you would need to look deeper to know more.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="tokenizing-by-sentence">Tokenizing by sentence<a href="#tokenizing-by-sentence" class="hash-link" aria-label="Direct link to Tokenizing by sentence" title="Direct link to Tokenizing by sentence">​</a></h4>
<p>When you tokenize by sentence, you can analyze how those words relate to one another and see more context.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-stemming">2.2 Stemming<a href="#22-stemming" class="hash-link" aria-label="Direct link to 2.2 Stemming" title="Direct link to 2.2 Stemming">​</a></h3>
<p>Stemming and Lemmatization are text normalization (or sometimes called word normalization) techniques in the field of natural language processing that are used to prepare text, words, and documents for further processing, because the mathematical models needs a vector description of a word or a sentence, and can not work with &quot;strings&quot; directly. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960’s.</p>
<p>Stemming is a text processing task in which reduces words to their root, which is the core part of a word. For example, the words &quot;helping&quot; and &quot;helper&quot; share the root &quot;help&quot;. Stemming allows to zero in on the basic meaning of a word rather than all the details of how it is being used. Stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word. The natural language toolkit (NLTK) contains more than only one stemming algorithm. For the English language, you can choose between PorterStammer or LancasterStammer, PorterStemmer being the oldest one originally developed in 1979. LancasterStemmer was developed in 1990 and uses a more aggressive approach than Porter Stemming Algorithm.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="porter-stemmer">Porter Stemmer<a href="#porter-stemmer" class="hash-link" aria-label="Direct link to Porter Stemmer" title="Direct link to Porter Stemmer">​</a></h4>
<p>Porter Stemmer uses suffix stripping to produce stems. Notice how the Porter Stemmer is giving mthe root (stem) of the word &quot;cats&quot; by simply removing the &quot;s&quot; after cat. This is a suffix added to cat to make it plural. But if you look at &quot;trouble&quot;, &quot;troubling&quot; and &quot;troubled&quot; they are stemmed to &quot;trouble&quot; because Porter Stemmer algorithm does not follow linguistics rather a set of rules for different cases that are applied in phases (step by step) to generate stems. This is the reason why Porter Stemmer does not often generate stems that are actual English words.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="lancaster-stemmer">Lancaster Stemmer<a href="#lancaster-stemmer" class="hash-link" aria-label="Direct link to Lancaster Stemmer" title="Direct link to Lancaster Stemmer">​</a></h4>
<p>The Lancaster Stemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. At each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="snowball-stemmer">Snowball Stemmer<a href="#snowball-stemmer" class="hash-link" aria-label="Direct link to Snowball Stemmer" title="Direct link to Snowball Stemmer">​</a></h4>
<p>The Snowball Stemmer is a non-English stemmer. With the Snowball Stemmer it is possible to create a own language stemmer.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="23-lemmatizing">2.3 Lemmatizing<a href="#23-lemmatizing" class="hash-link" aria-label="Direct link to 2.3 Lemmatizing" title="Direct link to 2.3 Lemmatizing">​</a></h3>
<p>Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words. For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words. The Python NLTK library provide the WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of the words.</p>
<p><strong>Now the question may arise, when do I use which method (Stemming or Lemmatizing)?</strong></p>
<p>The Stemmer method is much faster than Lemmatizing, but it produces in general worse results. Using Stemming might end up that the generated stem is not an actual word, because it uses no corpus. It depends a bit on the current problem which you are working on. If you are building a language application in which language and a grammatic is important, then the lemmatizing approach will be better than the stemming approach.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-word-embeddings">3. Word embeddings<a href="#3-word-embeddings" class="hash-link" aria-label="Direct link to 3. Word embeddings" title="Direct link to 3. Word embeddings">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-overview-of-word-embeddings">3.1 Overview of word embeddings<a href="#31-overview-of-word-embeddings" class="hash-link" aria-label="Direct link to 3.1 Overview of word embeddings" title="Direct link to 3.1 Overview of word embeddings">​</a></h3>
<p>In NLP, word embedding is a projection of a word, consisting of characters into meaningful vectors of real numbers. Conceptually it involves a mathematical embedding from a dimension N (all words in a given corpus)-often a simple one-hot encoding is used- to a continuous vector space with a much lower dimensionality, typically 128 or 256 dimensions are used. Word embedding is a crucial preprocessing step for training a neural network. There extists many different approaches for word embedding, which will be explained in the following sections of this blog.</p>
<p><strong>There are two main properties for a useful projection:</strong></p>
<ul>
<li>Distributed representation of concepts and continuity of words sharing similar properties</li>
<li>Allow to learn this projection for the given task</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*sROrntT_wJOI6-DoBViyjw.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.1 - Illustration of a word embedding</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-one-hot-encoding">3.2 One-hot encoding<a href="#32-one-hot-encoding" class="hash-link" aria-label="Direct link to 3.2 One-hot encoding" title="Direct link to 3.2 One-hot encoding">​</a></h3>
<p>One-hot encoded words are not useful to solve a NLP task successfully. Assume that an arbitrary dictionary consists out of 5000 different words. This means, when using one-hot encoding, each word will be represented by a vector of length 5000, but 4999 of these entries are zero. It can be concluded, that the dimensionality gets very high and the featurespace is very sparse. Additionally there is not any connection of words with a similar meaning, as visible in figure 1 above.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1348/1*9ZuDXoc2ek-GfHE2esty5A.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.2 - Example of a one-hot encoding for words</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-embedding-layer">3.3 Embedding layer<a href="#33-embedding-layer" class="hash-link" aria-label="Direct link to 3.3 Embedding layer" title="Direct link to 3.3 Embedding layer">​</a></h3>
<p>A standard approach is, to feed the one-hot encoded tokens (mostly words, or sentence) into a embedding layer. During training the model tries to find a suitable embedding (lower dimensionality as the input layer). The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. In some cases it could be useful to use a pretrained embedding, which was trained on a hugh corpus of words. Figure 3 shows a schematic architecture of a word based embedding layer.</p>
<ul>
<li><strong>Input:</strong> one-hot encoding of the word in a vocabulary</li>
<li><strong>Output:</strong> one vector of N dimensions (given by the user, probably tuned with hyperparameter tuning)</li>
</ul>
<p>There exists many different approaches of word embedding, some of the most popular ones we will look in more detail during the following sections.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*pktqRMGoYo9_fRCI8RpKLQ.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.3 - Visualization of a embedding layer</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="331-training-of-an-embedding-layer"><strong>3.3.1 Training of an embedding layer</strong><a href="#331-training-of-an-embedding-layer" class="hash-link" aria-label="Direct link to 331-training-of-an-embedding-layer" title="Direct link to 331-training-of-an-embedding-layer">​</a></h4>
<p>There exists three main options to train a embedding layer, and it depends on the natural language processing problem which approach should be chosen, to solve the problem.</p>
<p><strong>Three main options for training a embedding layer:</strong></p>
<ul>
<li>Train the weights of the embedding layer from scratch for a given task.</li>
<li>Use a pretrained embedding such as word2vec, GloVe, FastText, ELMo, (these approaches will be explained in more detail later on).</li>
<li>Similar approach as for transfer learning: take a pretrained embedding and fine tune it through training (with a very small learningrate, e.g: 1e-5) for the given task.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="332-keras-embedding-layer">3.3.2 Keras embedding layer<a href="#332-keras-embedding-layer" class="hash-link" aria-label="Direct link to 3.3.2 Keras embedding layer" title="Direct link to 3.3.2 Keras embedding layer">​</a></h3>
<p>Keras offers an embedding layer that can be used for neural networks, such as RNN’s (recurrent neural networks) for text data. This layer is defined as the first layer of a more complex architecture. The embedding layer needs at least three input values:</p>
<ul>
<li><strong>input_dim:</strong> Integer. Size of the vocabulary, i.e. maximum integer index+1.</li>
<li><strong>output_dim:</strong> Integer. Dimension of the dense embedding.</li>
<li><strong>input_length:</strong> Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li><code>e = Embedding(200, 32, input_length=50)</code> → vocabulary with 200 words, output dimension should be 50 and each document will have 50 words</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="example-of-a-keras-embedding-layer">Example of a keras embedding layer<a href="#example-of-a-keras-embedding-layer" class="hash-link" aria-label="Direct link to Example of a keras embedding layer" title="Direct link to Example of a keras embedding layer">​</a></h4>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># define the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Sequential</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#vocabulary size = 30, inputlegth = 4, output size = 8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">add</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">layers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">vocabsize</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> inputlength</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">maxlength</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">add</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">layers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Flatten</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#binary classification problem</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">add</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">layers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Dense</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> activation</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;sigmoid&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token comment" style="color:#999988;font-style:italic"># compile the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">compile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">optimizer</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;adam&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loss</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;binarycrossentropy&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">metrics</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&#x27;accuracy&#x27;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token comment" style="color:#999988;font-style:italic"># summarize the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">summary</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Model summary</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;sequential_2&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Layer </span><span class="token punctuation" style="color:#393A34">(</span><span class="token builtin">type</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> Output Shape </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> Param</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token operator" style="color:#393A34">==</span><span class="token operator" style="color:#393A34">=</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">embedding_1 </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Embedding</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">400</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">flatten_1 </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Flatten</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dense_1 </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Dense</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">33</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token operator" style="color:#393A34">==</span><span class="token operator" style="color:#393A34">==</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Total params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">433</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Trainable params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">433</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Non</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">trainable params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token boolean" style="color:#36acaa">None</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Results</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># fit the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">fit</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">padded_docs</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> labels</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> epochs</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">50</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> verbose</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># evaluate the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loss</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> accuracy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">evaluate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">padded_docs</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> labels</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> verbose</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;Accuracy: %f&#x27;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">accuracy</span><span class="token operator" style="color:#393A34">*</span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#100.00</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="34-word2vec">3.4 Word2Vec<a href="#34-word2vec" class="hash-link" aria-label="Direct link to 3.4 Word2Vec" title="Direct link to 3.4 Word2Vec">​</a></h3>
<p>Word2Vec relates to a type of models, producing word embedding into space with contextual similarity, i.e. words that share common contexts are located in close proximity. We will explain the three main architectures of the word2Vec approach in the next sections. For more details we refer to the original papers.</p>
<p>One hypothesis of the word2vec approach is, that a word is represented by its &quot;context&quot;, or in other words, a window around the target word is created and words falling in the window are added to the &quot;bag&quot; disregarding the order. The word2vec mapping is not only about clustering related words, it is also able to preserve some &quot;path meaning&quot;.</p>
<p>For the word2vec approach, there exist many different architecture approaches:</p>
<ul>
<li><strong>One word context:</strong> uses only one single word as context (this strategy is not often used, mainly a educational approach).</li>
<li><strong>Skip-gram model:</strong> An extension of one word context to multi word context at the output.</li>
<li><strong>Continuous bag-of-word model:</strong> An extension of one word context to multi word context at the input.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="341-one-word-context-model">3.4.1 One word context model<a href="#341-one-word-context-model" class="hash-link" aria-label="Direct link to 3.4.1 One word context model" title="Direct link to 3.4.1 One word context model">​</a></h4>
<p>The one word approach takes only one word as input and tries, based on this input to predtict the output word. This approach is not used widely in practise, it is more an educational approach, because the model can not learn any context in a given document.</p>
<p><strong>Lossfunction of the one word context model</strong></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:532/1*y8Tst8vqxQoaiY-75AqEqg.png" alt="image" class="img_ev3q"></p>
<p>The model simply tries to maximise the negative log of the probability of the output word given the input word.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*IsK_X7YdgmOVFGhIpuKEJA.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.4 - one word context model</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="342-skip-gram-model">3.4.2 Skip-gram model<a href="#342-skip-gram-model" class="hash-link" aria-label="Direct link to 3.4.2 Skip-gram model" title="Direct link to 3.4.2 Skip-gram model">​</a></h4>
<p>A more suitable approach than the one word context model is the skip-gram model. This model takes one target word as input to predict the context (neighbouring words).</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1128/1*ieqh2zbnW7K0cmXk9BusXA.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.5 - skip-gram model</em></p>
<p><strong>Lossfunction of the skip-gram model</strong></p>
<p>The lossfunction is not very different than the lossfunction of the word context model.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1018/1*n_ot-YRLGznWvFGfc8TfqQ.png" alt="image" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="343-continuous-bag-of-word">3.4.3 Continuous bag-of-word<a href="#343-continuous-bag-of-word" class="hash-link" aria-label="Direct link to 3.4.3 Continuous bag-of-word" title="Direct link to 3.4.3 Continuous bag-of-word">​</a></h4>
<p>In the continuous bag-of-word approach (CBOW) the model tries to predict the actual word based from some surrounding words, in this sense it is the reversed approach of the skip gram model. The order of the surrounding words does not influence the prediction (therefore: bag-of-words).</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1100/1*kCgUJvDMw4bgULaddAMgYA.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.6 - Continuous bag-of-word model</em></p>
<p><strong>Lossfunction of the continuous bag-of-word model</strong></p>
<p>The lossfunction is similar to the lossfunction of the one word model. The only difference is that more than one words are given.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:938/1*UyLbzvpdpARj1TUtL4naWg.png" alt="image" class="img_ev3q"></p>
<p>More about the mathematical concept behind the model can be found in original paper.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="35-glove---global-vectors-for-word-representation">3.5 GloVe - Global vectors for word representation<a href="#35-glove---global-vectors-for-word-representation" class="hash-link" aria-label="Direct link to 3.5 GloVe - Global vectors for word representation" title="Direct link to 3.5 GloVe - Global vectors for word representation">​</a></h3>
<p>Another, more advanced approach for word embedding is the GloVe model. GloVe means global vectors for word representation. This approach is not based on a pure ANN (artificial neural network) approach, but also on statistical approaches. The GloVe-approach combines basically the advantages of the word2vec approach and the LSA approach. LSA mean Latent Semantic Analysis and was one of the first approaches for vector embedding of words.</p>
<p>GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="351-basic-principle-of-glove">3.5.1 Basic principle of GloVe<a href="#351-basic-principle-of-glove" class="hash-link" aria-label="Direct link to 3.5.1 Basic principle of GloVe" title="Direct link to 3.5.1 Basic principle of GloVe">​</a></h4>
<p>Some basic notation first:</p>
<ul>
<li><strong><em>X</em></strong> : is the matrix of word-word co-occurrence. The matrix is quadratic and has the shape of the number of words in the corpus.</li>
<li><strong><em>X_i=∑(X_ik)</em></strong> : The number of times any word appears in the context of word i.</li>
<li><strong><em>P_ij=P(i|j)=X_ij/X_i</em></strong> : the probability that word j appear in the context of word i.</li>
</ul>
<p>Given a corpus having <strong><em>V</em></strong> words, the co-occurence matrix <strong><em>X</em></strong> will have the shape of <strong><em>V*V</em></strong> where the i-th row and j-th column of <strong><em>X</em></strong>,<strong><em>X_ij</em></strong> denotes, how many times word <strong><em>i</em></strong> has co-occurred with word <strong><em>j</em></strong>.</p>
<p>As an easy example, we are using the following sentence:</p>
<p><strong>&quot;the cat sat on the mat&quot;.</strong></p>
<p>We use a windowsize of &quot;one&quot; for this example, but it is also possible / useful to use a larger window size. Figure 7, shows the resulting co-occurrence matrix of the example sentence above:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:630/1*mdFqxZKedPy8-Bhhuth-Eg.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.7 - Example of a simple Co-occurence matrix</em></p>
<p>To measure the similarity between words, we need three words at a time. The following table shows such an example:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1196/1*wGQzP1BWYgkfajyjZfj8CA.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.8 - GloVe probability table</em></p>
<p>The table above shows the co-occurrence probabilities for target words ice and steam with selected context words from a six billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large values (much greater than one) correlate well with properties specific to ice, and small values (much less than one) correlate well with properties specific of steam.</p>
<p>You can see that given two words, i.e. ice and steam, if the third word k (also called the &quot;probe word&quot;):</p>
<ul>
<li>is very similar to ice but irrelevant to steam (e.g. k=solid), <strong><em>P_ik/P_jk</em></strong> will be very high (<code>&gt;1</code>),</li>
<li>is very similar to steam but irrelevant to ice (e.g. k=gas), <strong><em>P_ik/P_jk</em></strong> will be very small (<code>&lt;1</code>),</li>
<li>is related or unrelated to either words, then <strong><em>P_ik/P_jk</em></strong> will be close to 1</li>
</ul>
<p>So, if we can find a way to incorporate <strong><em>P_ik/P_jk</em></strong> to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="352-mathematical-background-of-the-glove-approach">3.5.2 Mathematical background of the GloVe approach<a href="#352-mathematical-background-of-the-glove-approach" class="hash-link" aria-label="Direct link to 3.5.2 Mathematical background of the GloVe approach" title="Direct link to 3.5.2 Mathematical background of the GloVe approach">​</a></h4>
<p>It can be shown, that an appropriate starting point for word vector learning could be working with ratios of co-occurrence probabilities rather then the probabilities themselves. We take the following equation as a starting point:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:490/1*Ehxi0Kd1FbZivISBNJrxcw.png" alt="image" class="img_ev3q"></p>
<p>In the formula from above, <strong><em>F()</em></strong> can be taken to be a complicated function parameterized by a neural network. The ratio <strong><em>P_ik/P_jk</em></strong> depends on three words <strong><em>i</em></strong>,<strong><em>j</em></strong> and <strong><em>k</em></strong>. In the formula above are <strong>w</strong> the word vectors and <strong><em>w_tilde</em></strong> are the separate context vectors.</p>
<p>After some recalculation steps we get to following formula:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:734/1*3NZVpENLBeAxdIX7KxTKZQ.png" alt="image" class="img_ev3q"></p>
<p>A main drawback to this model is that it weights all co-occurrences evenly, even those that happen rarely or never. Such rare co-occurrences are noisy and carry less information than the more frequent ones - yet even just the zero entries account for 75-95% of the data in <strong><em>X</em></strong>, depending on the vocabulary size and corpus. The authors from the GloVe-paper proposed a new weighted least squares regression model that addresses these problems. The weight function <strong><em>f(Xij)</em></strong> is shown in figure 9.</p>
<p><strong>Lossfunction of the GloVe model</strong></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1162/1*RqSbaez98jyc6jj49qZ2CA.png" alt="image" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1216/1*TXWYDO4f8pGipKk2hRWzfw.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.9 - Example of the weightfunction f(Xij)</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="353-training-details-of-glove">3.5.3 Training details of GloVe<a href="#353-training-details-of-glove" class="hash-link" aria-label="Direct link to 3.5.3 Training details of GloVe" title="Direct link to 3.5.3 Training details of GloVe">​</a></h4>
<p>Glove is based on matrix factorization techniques on the word-context matrix, also known as the co-occurrence matrix. It first constructs a large matrix of (words - context) co-occurrence information (the violett matrix in figure 10 below), i.e. for each &quot;word&quot; (the rows), we count how frequently we see this word in some &quot;context&quot; (the columns) in a large corpus. The number of &quot;contexts&quot; is of course large, since it is essentially combinatorial in size. The idea then is to apply matrix factorization to approximate this matrix as depicted in the following figure 10.</p>
<p>So then we factorize this matrix to yield a lower-dimensional (word - features) matrix (the orange matrix in figure 10 below), where now each row yields a vector representation for the corresponding word. In general, this is done by minimizing a &quot;reconstruction loss&quot;. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.</p>
<p>Mostly the word-feature matrix and the feature-context matrix are initialized randomly and attempt to multiply them to get a word-context co-occurrence matrix, which is as similar as possible to the original matrix. After training, the word-featrue matrix gives the learned word embedding for each word where the number of columns (features) can be present to a specific number of dimension, given by the user as a hyperparameter.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*8UEaG6Bm9krv8N7IDnjvxg.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.10 - Conceptual model for the GloVe model</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="36-fasttext-enriching-word-vectors-with-subword-information">3.6 FastText (enriching word vectors with subword information)<a href="#36-fasttext-enriching-word-vectors-with-subword-information" class="hash-link" aria-label="Direct link to 3.6 FastText (enriching word vectors with subword information)" title="Direct link to 3.6 FastText (enriching word vectors with subword information)">​</a></h3>
<p>FastText is a extension of the traditional Word2Vec approach, especially the continuous skip-gram model and it was proposed by Facebook. Instead of feeding different individual words into the neural network, FastText breaks down the words into several n-grams, or sub-words. The word embedding vector for a word will be the sum of all of its n-grams.</p>
<p>FastText has one main advantage in contrast to the previous explained methods. This approach can not only represent the words in the vocabulary, but also out-of-vocabulary words can be represented since their n-grams probably appears in other words of the trainingset. This can be very useful for compound words.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="361-keypoints-of-fasttext">3.6.1 Keypoints of FastText<a href="#361-keypoints-of-fasttext" class="hash-link" aria-label="Direct link to 3.6.1 Keypoints of FastText" title="Direct link to 3.6.1 Keypoints of FastText">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="hierarchical-softmax">hierarchical softmax<a href="#hierarchical-softmax" class="hash-link" aria-label="Direct link to hierarchical softmax" title="Direct link to hierarchical softmax">​</a></h5>
<p>A softmax function is very common used as an activation function to output the probability of a given input to belong to <strong><em>n</em></strong> different classes in multi-class classification problems. Hierarchical softmax proves to be very efficient when there are a large number of categories and there is a class imbalance present in the data. Here, the classes are arranged in a tree distribution instead of a flat, list-like structure. The construction of the hierarchical softmax layer is based on the Huffman coding tree, which uses shorter trees to represent more frequently occurring classes and longer trees for rarer, more infrequent classes. The probability that a given text belongs to a class is explored via a depth-first search along the nodes across the different branches. Therefore, branches (or equivalently, classes) with low probabilities can be discarded away. For data where there are a huge number of classes, this will result in a highly reduced order of complexity, thereby speeding up the classification process significantly compared to traditional models.</p>
<p>A more detailed explanation, also with the mathematics behind can be found in this post.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="word-n-grams">Word n-grams<a href="#word-n-grams" class="hash-link" aria-label="Direct link to Word n-grams" title="Direct link to Word n-grams">​</a></h5>
<p>Using only a bag of words representation of the text leaves out crucial sequential information. Taking word order into account will end up being computationally expensive for large datasets. So as a happy medium, FastText incorporates a bag of n-grams representation along with word vectors to preserve some informations about the surrounding words appearing near each word. This representation is very useful for classification applications, as the contextual meaning of a couple of different words strung together also results in a particular sentiment echoed by that piece of text.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="n-gram-hashing">n-gram hashing<a href="#n-gram-hashing" class="hash-link" aria-label="Direct link to n-gram hashing" title="Direct link to n-gram hashing">​</a></h5>
<p>Working with n-grams lead to a very huge number of different n-grams. The FastText approach uses a hashing function to convert each character n-gram to a hashed integer value between 1 and B. This method can dramatically decrease the the size of the bucket.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:990/1*zDPmG8lQgjh41JbRygEQcQ.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.11 - Visualization of the dictionary reduction</em></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:850/1*rG_CB4twhOg3vD5AgR1t0w.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.12 - Visualization of hashing process</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="362-mathematical-background-of-the-fasttext-approach">3.6.2 Mathematical background of the FastText approach<a href="#362-mathematical-background-of-the-fasttext-approach" class="hash-link" aria-label="Direct link to 3.6.2 Mathematical background of the FastText approach" title="Direct link to 3.6.2 Mathematical background of the FastText approach">​</a></h4>
<p><strong>log-likelihood of the skipgram model</strong> (given is the target word, and the model is trained to predict well words that appear in its context):</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:650/1*bV89nzlGh4GBTKKxgky6mA.png" alt="image" class="img_ev3q"></p>
<p>The context <strong><em>C_t</em></strong> is the set of indices of words surrounding word <strong><em>w_t</em></strong>. The skipgram model uses a scoring function, and calcualte the probability <strong><em>p(wc|wt)</em></strong> with a softmax function. The FastText model uses a very similar scoring function but in a slightly different way. In the FastText approach, each word ww will be represented as a bag of n-grams.</p>
<p>Here a simple example:</p>
<p>input word: where → split in n-grams (where n=3): wh, whe, her, ere, re</p>
<p>The main difference between skipgram and FastText is only that the scoring function is the sum over all n-gram vectors.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:604/1*ZtY01Q5ajXLMc1ByTwEkcA.png" alt="image" class="img_ev3q"></p>
<p>A word will be represented by the sum of the vector representation of its different n-grams. This scoring function is then used for the softmax function, same as in the skipgram approach.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="363-architecture-of-fasttext">3.6.3 Architecture of FastText<a href="#363-architecture-of-fasttext" class="hash-link" aria-label="Direct link to 3.6.3 Architecture of FastText" title="Direct link to 3.6.3 Architecture of FastText">​</a></h4>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1206/1*Y8526ukD0Dk9IpAssFsCxw.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.13 - Architecture overview of the FastText model</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="37-elmo">3.7 ELMo<a href="#37-elmo" class="hash-link" aria-label="Direct link to 3.7 ELMo" title="Direct link to 3.7 ELMo">​</a></h3>
<p>All previous presented word embedding approaches have one thing together. They all have exactly one embedding for one word or a n-gram of a word. They do not look at the contextual relationship of a specific word, or only in a very small range and in a limited way (skip gram model).</p>
<p><strong>These approaches have two main problems:</strong></p>
<ul>
<li>Always the same representation for a word type, regardless of the context in which a word token occurs.</li>
<li>We just have one representation for a word, but words have different aspects, including semantics, syntatic behaviour, and register/connotations</li>
</ul>
<p>In contrast to Word2Vec, ELMo (Embeddings from language models) looks at the entire sentence before assigning each word in its embedding. The ELMo approach makes the step over from simple word embedding, like the approaches shown before to language model.</p>
<p><strong>Here one example where ELMo can help for creating a better embedding:</strong></p>
<ul>
<li>&quot;The <strong>plane</strong> took of at exactly nine o’clock&quot;</li>
<li>&quot;The <strong>plane</strong> surface is a must for any cricket pitch&quot;</li>
<li><strong>&quot;Plane</strong> geometry is fun to study&quot;</li>
</ul>
<p>The word &quot;plane&quot; has in each of this three sentences a completely different meaning.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="371-keypoints-of-elmo">3.7.1 Keypoints of ELMo<a href="#371-keypoints-of-elmo" class="hash-link" aria-label="Direct link to 3.7.1 Keypoints of ELMo" title="Direct link to 3.7.1 Keypoints of ELMo">​</a></h4>
<p>Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a two layer bi-directional LSTM, trained on a specific task to be able to create those embeddings. As for other word embeddings, it is also possible to use a pretrained version of a ELMo model.</p>
<p>ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="372-elmo-architecture">3.7.2 ELMo architecture<a href="#372-elmo-architecture" class="hash-link" aria-label="Direct link to 3.7.2 ELMo architecture" title="Direct link to 3.7.2 ELMo architecture">​</a></h4>
<p>ELMo basically consists out of two bi-directional LSTM layers - so that the language model does not only have a sense of the next word, but also the previous word. Given a sequence of N tokens (this could be a whole sentence, or at least a part of a sentence) the bidirectional language model computes the forward and also the backward probability. In more mathematical sense, the ELMo model tries to maximize the log likelihood of the forward and backward probability.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*eBcOSrMrXReLwKvTedlpxA.png" alt="image" class="img_ev3q"></p>
<p><strong>ELMo consists mainly of the following points:</strong></p>
<ul>
<li>Two biLSTM layers</li>
<li>Character CNN to build initial word representation</li>
<li>The character CNN use residual connections</li>
<li>Tie parameters of token input and output an tie these between forward and backward Language models</li>
</ul>
<p>The architecture showed in figure 14, uses a character-level convolutional neural network (CNN) to represent words of a text string into raw word vectors. These raw word vectors act as inputs to the first layer of the biLSTM. The forward pass contains information about a certain word and the context (other words) before that word. The backward pass contains information about the word and the context after it. This pair of information, from the forward and backward pass, forms the intermediate word vectors. These intermediate word vectors are fed into the next layer of the biLSTM. The final representation (ELMo) is the weighted sum of the raw word vectors and the two intermediate word vectors.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*AS2KJAwx18XbuyAefz9fLQ.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.14 - Architecture of the ELMo language model</em></p>
<p>The main part of ELMo, the two bidirectional LSTMs are more or less straight forward. A more interesting part is the embedding part <strong><em>(E1,E2,…,En)</em></strong>. This will be explained in more detail in the upcoming sections.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="373-elmo-embedding">3.7.3 ELMo embedding<a href="#373-elmo-embedding" class="hash-link" aria-label="Direct link to 3.7.3 ELMo embedding" title="Direct link to 3.7.3 ELMo embedding">​</a></h4>
<p>In the paper &quot;Character-Aware neural language models&quot;, they propose a language model that leverages subword informations through a character-level convolutional neural network (CNN), whose output is used as an input to a recurrent neural network language model (RNN-LM). If we study the architecture in figure 15 in more detail, we can see that the ELMo is quite similar to the character-aware neural language model approach from 2015. They changed the simple vanilla LSTM to a bidirectional LSTM model, but the embedding part is in both approaches the same.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1020/1*Z4TSBi8Mh_jGc1dltZ_ZpA.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.15 - Architecture of the word embedding as a preprocessing step for the ELMo model</em></p>
<p>The first layer performs a lookup of character embeddings (in the example above it is four dimensional) and stack them to the matrix <strong><em>C_k</em></strong>. On the matrix <strong><em>C_k</em></strong> are applied multiple filters (convolutions). In the example above, there are in total twelve filters - three filters with width of two (blue) four filters with width of three, and five filters with the width of four. On the resulting matrices of the filters is applied a max-over-time pooling operation, to obtain a fixed-dimensional representation of each word. This vector is used as input for the highway network. The output of the highway network will be used as input for the first bidirectional LSTM layer in the ELMo architecture.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="374-character-level-convolutional-neural-network">3.7.4 Character-level convolutional neural network<a href="#374-character-level-convolutional-neural-network" class="hash-link" aria-label="Direct link to 3.7.4 Character-level convolutional neural network" title="Direct link to 3.7.4 Character-level convolutional neural network">​</a></h4>
<p>Let assuming that <strong><em>C</em></strong> is the (one-hot encoded) vocabulary of characters. We can assume that the dimensionality will be 26 (every character in the alphabet). With this two informations we can build up the matrix for the character embeddings <strong><em>Q</em></strong>. The chracter-level representation of a word <strong><em>k</em></strong> with length <strong><em>l</em></strong> is given by the matrix <strong><em>C_k</em></strong>, where the j-th column in <strong><em>C_k</em></strong> corresponds to the character embedding for <strong><em>c_j</em></strong> in <strong><em>Q</em></strong>. In a next step several filters are applied on the matrix <strong><em>C_k</em></strong>. Finally they using a max-over-time pooling, to make sure, that each word have the same length as input of the highway network.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="375-highway-network">3.7.5 Highway network<a href="#375-highway-network" class="hash-link" aria-label="Direct link to 3.7.5 Highway network" title="Direct link to 3.7.5 Highway network">​</a></h4>
<p>The next step after the character-level CNN is the highway network. The authors of the paper refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers of information highways. The main idea of the introduction of highway networks was, that many recent empirical breakthroughs in supervised machine learning have been achieved through the usage of deep neural networks. However, the training process of deep neural network is not as straight forward as simply adding additional layers to the architecture. Highway networks were a novel approach for the optimization of networks with a virtually arbitrary depth. The approach uses a learned gating mechanism, similar to the LSTM, or RNN gates, with this gating mechanism, the network can have paths along, which some informations can be processed through several layers without attentuation. This is a similar approach as the residual skip connections, which will be shortly explain in a later stage of this blog.</p>
<p>This model uses as input the max-over-time pooled vector, at least in the ELMo approach, created from the character-level CNN. The authors of the paper showed, that it is also possible in principle to feed the output of the charactet-level CNN <strong><em>yk</em></strong> directly to the LSTM layer. But the authors observed, that feed <strong><em>yk</em></strong> through a highway network before the LSTM will lead to some improvements of the accuracy. Highway networks in general can also be used in many other machine learning and deep learning problems. The word embedding in the ELMo approach is just one specific application of the highway network approach.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1120/1*xI2buV-uemPFE5MvCc2HsA.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.16 - Overview of a highway layer</em></p>
<p>A plain feedforward neural network consists mainly of <strong><em>L</em></strong> layers and each layer applies a non-linear transformation <strong><em>H</em></strong>, where <strong><em>H</em></strong> is normally a affine transformation function, followed by a non-linear activation function (sigmoid, ReLU, etc.).</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:396/1*KCnbf6kign76VQXmMXKHCA.png" alt="image" class="img_ev3q"></p>
<p>For the highway network, the authors introduced two additional non-linear transformations, <strong><em>T(x,W_T)</em></strong> and <strong><em>C(x,W_C)</em></strong>. This yields to the following formula.</p>
<p><strong>A layer of the highway network does the following:</strong></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1006/1*GObc-kv9tmy0XXBeWp3Pew.png" alt="image" class="img_ev3q"></p>
<p>whereas <strong><em>C = 1-T</em></strong> and therefore:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1138/1*1t0vsGv-ujRn0IKGYTAFOA.png" alt="image" class="img_ev3q"></p>
<p>where <strong><em>H</em></strong> is the non-linearity from a plain feedforward layer and <strong><em>T</em></strong> and <strong><em>C</em></strong> are the two additional introduced non-linearities. <strong><em>T</em></strong> is called the transform gate and <strong><em>(1−T)</em></strong> or <strong><em>C</em></strong> is called the carry gate. Similar to the memory cell in LSTM networks, highway layers allow for training of deep networks by adaptively carrying some dimensions of the input directly to the output. A more detailed explanation of the highway network can be found in the paper &quot;Highway Networks&quot;</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="376-elmo-summary">3.7.6 ELMo summary<a href="#376-elmo-summary" class="hash-link" aria-label="Direct link to 3.7.6 ELMo summary" title="Direct link to 3.7.6 ELMo summary">​</a></h4>
<p><strong>Sum up the most important features</strong></p>
<ul>
<li><strong>Contextual:</strong> The representation for each word depends on the entire context in which it is used.</li>
<li><strong>Deep:</strong> The word representations combine all layers of a deep pre-trained neural network.</li>
<li><strong>Character based:</strong> ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-sequence-to-sequence-models">4. Sequence to sequence Models<a href="#4-sequence-to-sequence-models" class="hash-link" aria-label="Direct link to 4. Sequence to sequence Models" title="Direct link to 4. Sequence to sequence Models">​</a></h2>
<p>The approach of squence to sequence models, or short seq2seq models, was introduced in 2014 by Google. The aim is to map a input vector with a fixed length to a output vector also with a fixed length, but maybe the length of the input and output is different. The model consists basically of a encoder part (one RNN or LSTM layer) and of a decoder part (another RNN or LSTM layer).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="41-encoder---decoder-principle">4.1 Encoder - Decoder principle<a href="#41-encoder---decoder-principle" class="hash-link" aria-label="Direct link to 4.1 Encoder - Decoder principle" title="Direct link to 4.1 Encoder - Decoder principle">​</a></h3>
<p>An encoder processes the input sequence and compresses the information into a context vector <strong><em>h_t</em></strong> of a fixed length. The decoder is initialized with the context vector <strong><em>h_t</em></strong> to emit the transformed output. The encoder consists of embedding layer followed by a RNN layer or a LSTM layer. The power of this model lies in the fact that it can map seqeunces of different lengths (input and output vector).</p>
<p>A basic Seq2Seq model can work quite well for short text sequences, but it has difficulties with long sequences, because the context vector has a fixed length and has to encode a lot of infomation. One possible solution to overcome the problem of long sequences, is to plug in the attention mechanism.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*eAM9Dl-lPk0nxN0Zbbynew.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.17 - Sequence to sequence architecture</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-attention-mechanism">5. Attention mechanism<a href="#5-attention-mechanism" class="hash-link" aria-label="Direct link to 5. Attention mechanism" title="Direct link to 5. Attention mechanism">​</a></h2>
<p>In the previous architecture (sequence-to-sequence models), the encoder compressed the whole source sentence into a single vector, the so called context vector <strong><em>h_t</em></strong>. This can be very hard - the number of possible meanings of source is infinite. When the encoder is forced to put all information into a single vector, it is likely to forget something. Not only it is hard for the encoder to put all information into a single vector - this is also hard for the decoder to extract all important information from the context vector <strong><em>h_t</em></strong>. The decoder sees only one representation of source. However, at each generation step, different parts of source can be more useful than others. But in the current setting, the decoder has to extract relevant information from the same fixed representation.</p>
<p>An attention mechanism is a part of a neural network. At each decoder step, it decides which source parts are important for the decoder. In this setting, the encoder does not have to compress the whole source into a single vector - it gives representations for all source tokens (for example, all RNN states instead of the last one).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="51-mathematical-background-of-attention-mechanism">5.1 Mathematical background of attention mechanism<a href="#51-mathematical-background-of-attention-mechanism" class="hash-link" aria-label="Direct link to 5.1 Mathematical background of attention mechanism" title="Direct link to 5.1 Mathematical background of attention mechanism">​</a></h3>
<p>There are mainly three different ways to compute attention scores:</p>
<ul>
<li><strong>dot-product - the simplest method:</strong></li>
</ul>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:506/1*Ek-iT96cha-0a6ROPmBEwQ.png" alt="image" class="img_ev3q"></p>
<ul>
<li><strong>bilinear function (aka &quot;Luong attention&quot;)</strong> more details could be find in the original paper</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:556/1*6Qe4WGnwqecWbbeLwzhgog.png" alt="image" class="img_ev3q"></p>
<ul>
<li><strong>multi-layer perceptron (aka &quot;Bahdanau attention&quot;)</strong> more details could be fint in the original paper</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:890/1*zfFRVgD8VFbRT6PHTfBDvQ.png" alt="image" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="511-bahdanau-model">5.1.1 Bahdanau model<a href="#511-bahdanau-model" class="hash-link" aria-label="Direct link to 5.1.1 Bahdanau model" title="Direct link to 5.1.1 Bahdanau model">​</a></h4>
<p>The encoder of the bahdanau model has two RNNs layers, one forward and one backward (bidirectional) layer, which reads input in the both directions. For each token, states of the two RNNs are concatenated. To get an attention score, apply a multi-layer perceptron (MLP) to an encoder state and a decoder state. Attention is used between decoder steps: state <strong><em>h</em>(t−1)_</strong> is used to compute attention and its output, and both <strong><em>h</em>(t−1)_</strong> and its outputs are passed to the decoder at step <strong><em>t</em></strong>.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="512-luong-model">5.1.2 Luong model<a href="#512-luong-model" class="hash-link" aria-label="Direct link to 5.1.2 Luong model" title="Direct link to 5.1.2 Luong model">​</a></h4>
<p>The encoder of the luong model uses only one unidirectional RNN layer. The attention mechanism is applied same as in the bahdanau model, after the RNN decoder step <strong><em>t</em></strong> before making a prediction. State <strong><em>h_t</em></strong> used to compute attention and its output. The luong model uses a simpler scoring function (bilinear) than the bahdanau model.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="52-self-attention-mechanism">5.2 Self-attention mechanism<a href="#52-self-attention-mechanism" class="hash-link" aria-label="Direct link to 5.2 Self-attention mechanism" title="Direct link to 5.2 Self-attention mechanism">​</a></h3>
<p>The self-attention mechanism was introduced in the paper &quot;Attention is all you need&quot;. This approach is similar to the basic attention mechanism which was shown before. Self-attention is one of the key components of the transformer architecture, which we will explain in section 6 in more detail. The difference between attention and self-attention is that self-attention operates between representations of the same nature: e.g., all encoder states in some layer. Self-attention is the part of the model where tokens interact with each other. Each token &quot;looks&quot; at other tokens in the sentence with an attention mechanism, gathers context, and updates the previous representation of &quot;self&quot;.</p>
<p>Formally, this intuition of self-attention is implemented with a query-key-value attention. Each input token of a self-attention layer receives three representations (matrices) corresponding to the roles it can play:</p>
<ul>
<li><strong>query</strong> - asking for information</li>
<li><strong>key</strong> - saying that it has some information</li>
<li><strong>value</strong> - giving the information</li>
</ul>
<p>The self-attention function can be described as mapping a query (<strong><em>W_Q</em></strong>) and a set of key-value (<strong><em>W_K</em></strong>, <strong><em>W_V</em></strong>) pairs to an output. The input consists of a query vector and key-value vectors (dimension of values: <strong><em>d_v</em></strong>, dimension of keys: <strong><em>d_k</em></strong>). Based on these three vectors the self-attention layers calculates the three matrices <strong><em>W_Q</em></strong>, <strong><em>W_K</em></strong> and <strong><em>W_V</em></strong>. With these three matrices we can calulate the attention weigths (the output matrix of the attention layer).</p>
<p><strong>Formula to calculate the self-attention output:</strong></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:896/1*aOe5BibzwSQYLjYqPrMU_w.png" alt="image" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1368/1*glkmu3PCc8oISSO43-lEdg.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.18 - Overview of the self-attention mechanism</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="53-multi-head-attention-mechanism">5.3 Multi-head attention mechanism<a href="#53-multi-head-attention-mechanism" class="hash-link" aria-label="Direct link to 5.3 Multi-head attention mechanism" title="Direct link to 5.3 Multi-head attention mechanism">​</a></h3>
<p>The multi-head attention mechanism is an extension of the self-attention mechanism and is also implemented in the transformer architecture. The main idea of the authors of the paper was, instead performing a single attention function with <strong><em>d_model</em></strong>-dimensional keys, values and queries, it would be beneficial to linearly project the queries, keys and values <strong><em>h</em></strong> times with different, learned linear projections (the <strong><em>W_K</em></strong>, <strong><em>W_Q</em></strong> and <strong><em>W_V</em></strong> matrices).</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1296/1*XtPNn8-XUXEFHsptvgZRVQ.png" alt="image" class="img_ev3q"></p>
<p>where:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1000/1*x7tvbTs6hite9V25k0nidg.png" alt="image" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:874/1*mHWENrcV7kAF-81OCWqhNw.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.19 - Overview of the multi-head attention mechanism</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-transformers">6. Transformers<a href="#6-transformers" class="hash-link" aria-label="Direct link to 6. Transformers" title="Direct link to 6. Transformers">​</a></h2>
<p>Transformers are introduced in 2017, in the paper &quot;Attention is all you need&quot;. It is based solely on attention mechanism: i.e., without recurrence (RNN’s) or convolutions (CNN’s). On top of higher translation quality, the model is faster to train by up to an order of magnitude. Currently, Transformers (with variations) are de-facto standard models not only in sequence to sequence tasks but also for language modeling and in pretraining settings.</p>
<p>Transformer introduced a new modeling paradigm. In contrast to previous models where processing within encoder and decoder was done with recurrence or convolutions, transformer operates using only attention. The original transformer model uses the outputs of the decoder as input for the next prediction step, as shown in figure 20. We will see in the next sections, that some newer versions of the transformer only uses the encoder part or only the decoder part.</p>
<p>The transformer architecture is a stackwise architecture, it is simply possible and also useful to stack several encoder- and decoder layer together. The number of stacks could be a hyperparameter, which should be tuned during training, as well the number of attention head and the dimensions of the <strong><em>Q</em></strong>,<strong><em>K</em></strong> and <strong><em>V</em></strong> matrices of the self-attention layer.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="61-transformer-architecture">6.1 Transformer architecture<a href="#61-transformer-architecture" class="hash-link" aria-label="Direct link to 6.1 Transformer architecture" title="Direct link to 6.1 Transformer architecture">​</a></h3>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*GDpDsoqIKFpg4L0jIYEdMg.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.20 - Overview of the transformer model</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="611-positional-encoding">6.1.1 Positional encoding<a href="#611-positional-encoding" class="hash-link" aria-label="Direct link to 6.1.1 Positional encoding" title="Direct link to 6.1.1 Positional encoding">​</a></h4>
<p>Similiar to the previous shown NLP models, the transformer approach also uses learned embeddings to convert the input tokens and output tokens to vectors of dimension <strong><em>d_model</em></strong>. It is possible to use the Word2Vec embedding, to convert the input words to a continuous vector with a predefined dimension.</p>
<p>The transfomer model contains no recurrent (RNN’s or LSTM’s) an no convolutions, in order to make use of the order of a sequence, it is necessary to insert informations about the relative and absolute position of the input tokens of the model. The authors introduced as an additional embedding step the so called positional embedding, which is applied after the classical word embedding.</p>
<p><strong>Mathematical formulation of the positional encoding approach:</strong></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1278/1*lL4i6gZ8noFD1X8rseqPdA.png" alt="image" class="img_ev3q"></p>
<p>The final positional encoding which is then used as input of the transformer is simple a sum of the embedding vector and the positional vector. For example if we consider the word &quot;black&quot;, the positional vector is calculated as follow:</p>
<p><strong><em>pc(black)=embedding(black)+pe(black)</em></strong>, therefore it is necessary that the embedding vector and the positional vector have the same dimensions, mostly 512 dimensions.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="612-feed-forward-blocks">6.1.2 Feed forward blocks<a href="#612-feed-forward-blocks" class="hash-link" aria-label="Direct link to 6.1.2 Feed forward blocks" title="Direct link to 6.1.2 Feed forward blocks">​</a></h4>
<p>Additionally to the attention layers, each stack of the encoder and decoder contains one feed forward network. This layer consists of two linear layers with ReLU non-linearity in between. After looking at other tokens via an attention mechanism, a model uses an feed forward network block to process this new information.</p>
<p><strong>Mathematical formulation of the feed forward layer:</strong></p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1030/1*Xkuep1tFpDxa7h2Ul5AQbQ.png" alt="image" class="img_ev3q"></p>
<p>A feed forward layer could be build out of convolutional layer with kernelsize = 1. The network used in the paper &quot;Attention is all you need&quot; has a input and output dimensionality of <strong><em>d_model = 512</em></strong> and the inner layer has a dimensionality of <strong><em>d_ff = 2048</em></strong>.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="613-residual-connections">6.1.3 Residual connections<a href="#613-residual-connections" class="hash-link" aria-label="Direct link to 6.1.3 Residual connections" title="Direct link to 6.1.3 Residual connections">​</a></h4>
<p>Residual connections are very simple (add a block’s input to its output), but at the same time are very useful. They ease the gradient flow through a network and allow stacking a lot of layers. In traditional, basic neural network architectures each output of a layer is feed into the next layer. The authors of the paper &quot;Deep Residual Learning for Image Recognition&quot; showed that deeper network not always ends up with smaller training and test errors. Therefore the residual connections can be very useful for deep networks, for skipping some layers that do not help to improve the model accuracy.</p>
<p>In the transformer, residual connections are used after each multi-head attention and feed forward network block.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1140/1*6WlIo8W1_Qc01hjWdZy-1Q.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.21 - Residual block</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="614-layer-normalization">6.1.4 Layer normalization<a href="#614-layer-normalization" class="hash-link" aria-label="Direct link to 6.1.4 Layer normalization" title="Direct link to 6.1.4 Layer normalization">​</a></h4>
<p>The layer normalization, which is applied after the multi-head attention layer contains a add and normalization function. The add function is used to sum up the output of the multi-head attention and the residual connection, which comes directly from the input of the attention layer. The layer can be described as follows:</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:992/1*nGItSLFnvAvTfB86_aCB1w.png" alt="image" class="img_ev3q"></p>
<p>Many different methods exists for the layer normalization, but we will not go in detail in this blog.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="62-gpt-language-model">6.2 GPT language model<a href="#62-gpt-language-model" class="hash-link" aria-label="Direct link to 6.2 GPT language model" title="Direct link to 6.2 GPT language model">​</a></h3>
<p>The GPT-1 GPT-2, or GPT-3 are highly advanced models for several NLP taks. These models were introduced by OpenAI. The licence of the actual GPT-3 model was sold to Microsoft and is no longer accessible for the community. OpenAI showed with the GPT-2 and 3 in a very impressive way how fast the reasearch and developement is still going on. The transformers went from training from scratch over fine tuning on a specific task to finally zero shot, or few shots models in less than three years. The newest zero-shot / few-shot GPT-3 model requires no fine tuning for each downstream task and showed quite impressive results in some NLP taks, more details about the GPT-3 paper.</p>
<p>OpenAI was reaching the goal of using a GPT model on a downstream task without fine tuning, with the following four steps:</p>
<ul>
<li><strong>Fine tuning</strong>: A transformer was trained on a large corpus of data (this could be done in a unsupervised way). Afterwards the trained model was fine tuned specifically on a downstream task.</li>
<li><strong>Few shot</strong>: This step represents a hugh step forward. As startingpoint there is still a pretrained model. When the model needs to make inferences, it is presented with some examples of the task to perform as conditioning. Conditioning replaces weight updating.</li>
<li><strong>One shot</strong>: This step takes the process yet further. The pretrained GPT model is presented with only one demonstration of the downstream taks. There will be no weight updating</li>
<li><strong>Zero shot</strong>: The pretrained GPT model is presented with NO example of the downstream task. Therefore the GPT model must be able to generalize very good on mostly all NLP problems.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*9p0wMj_k8Gdmxaknhm1H6w.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.22 - Symbolic example of the different learning approaches in the GPT models</em></p>
<p>The main goal of OpenAI is to generalize the concept of understanding language to any downstream taks. Therefore the training is highly intensive and requires a hugh amount of data, because the GPT model rapidly envolved from 117M parameters (GPT-2 small) to 345M parameters (GPT-2 medium) to 762M parameters (GPT-2 large) and 1,542M parameters (GPT-2 extra large).. The newst GPT-3 model has incredible 1.5 billion parameters.</p>
<p>All GPT model uses only the decoder part of the original transformer architecture, shown in figure 23 below. In comparison to the BERT model, GPT do not use a bidirectional approach. More about a step-by-step implementation of a GPT model could be found in the book &quot;Transformers for natural language processing&quot; written by Daniel Rothman.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:382/1*Ei9bP_ZaAUJ5iTsC8mFnsw.png" alt="image" class="img_ev3q"></p>
<p><em>Fig.23 - Overview of the GPT model archtiecture</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="63-bert-bidirectional-encoder-representations-from-transformers">6.3 BERT (Bidirectional encoder representations from transformers)<a href="#63-bert-bidirectional-encoder-representations-from-transformers" class="hash-link" aria-label="Direct link to 6.3 BERT (Bidirectional encoder representations from transformers)" title="Direct link to 6.3 BERT (Bidirectional encoder representations from transformers)">​</a></h3>
<p>The BERT model, a new language representation model from Google AI, uses pre-training and fine-tuning to create state-of-the-art models for a wide range of tasks. These tasks include question answering systems, sentiment analysis, and language inference. BERT uses a multi-layer bidirectional transformer encoder. The self-attention layer performs self-attention in both directions. Google has released two variants of the model:</p>
<ul>
<li><strong>BERT Base:</strong> Number of transformers layers = 12, total parameters = 110M</li>
<li><strong>BERT Large:</strong> Number of transformers layers = 24, total parameters = 340M</li>
</ul>
<p>The BERT approach uses a semi-supervised training approach. The model was pretrained on a large text corpus without any labels (unsupervised learning). Google AI used the the WordPiece word embedding, with 30&#x27;000 tokens vocabulary. The BERT model was trained on the BookCorpus with 800 million words and the English wikipedia with 2500 million words. The pretraining contains two different NLP taks. First, &quot;masked language modeling (MLM)&quot; and &quot;next senctence prediction (NSP)&quot;.</p>
<p><strong>Masked language modeling</strong></p>
<p>Masked language modeling does not require training a model with a sequence of visible words followed by a masked sequence to predict. Here a short example from the book &quot;Transformers for natural language processing&quot;:</p>
<ul>
<li><em>&quot;The cat sat on it because it was a nice rug&quot;</em></li>
</ul>
<p>The decoder would mask the attention sequence after the model reached the word &quot;it&quot;:</p>
<ul>
<li><em>&quot;The cat sat on it masked sequence&quot;</em></li>
</ul>
<p>But the BERT encoder masks a random token to make a prediction:</p>
<ul>
<li><em>&quot;The cat sat on it MASK it was a nice rug&quot;</em></li>
</ul>
<p>The multi-head attention sub-layer can now see the whole sequence, run the self-attention process, and predict the masked token.</p>
<p><strong>next sentence prediction</strong></p>
<p>In this approach the input contains always two sentences. Therefore two new tokens were added:</p>
<ul>
<li>CLS this is a binary classification token, added to the beginning of the first sentence to predict, if the second sentence follows the first one. A positive sample is a pair of two consecutive senctences taken from a dataset. A negative sample is created using sentence from different documents.</li>
<li>SEP is a separation token that signals the end of a sequence.</li>
</ul>
<p>The aim is now to learn, if the two sentence are two consecutive sequences from one document or not.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="631-bert-finetuning">6.3.1 BERT finetuning<a href="#631-bert-finetuning" class="hash-link" aria-label="Direct link to 6.3.1 BERT finetuning" title="Direct link to 6.3.1 BERT finetuning">​</a></h4>
<p>The usage of a BERT model for a specific downstream taks is relatively straightforward. There are many different versions of pretrained models available. A very good pasge is <a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">https://huggingface.co/models</a>. After loading a pretrained version of the BERT model, it is necessary to fine tune the parameters of the model. It is also necessary to add one addtitional output layer and a input layer if needed. For a Q &amp; A Bot we have three inputs, the context a answer an a corresponding answer which should be find in the context. Figure 24 shows symbolic the idea behind the pretraining and fine tuning process.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*-LA8h0eXRZKlzFvrxtsm2w.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.24 - Two steps of using a BERT Transformer</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="632-comparison-of-the-different-models">6.3.2 Comparison of the different models<a href="#632-comparison-of-the-different-models" class="hash-link" aria-label="Direct link to 6.3.2 Comparison of the different models" title="Direct link to 6.3.2 Comparison of the different models">​</a></h4>
<p>Figure 25 below, shows the main differences in pre-training model architectures. The BERT model uses a bidirectional Transformer. The GPT model from OpenAI uses a simple left-to-right transformer. The ELMo model does not use the transformer approach, as we have seen earlier in this blog. Therefore the BERT and GPT model performs better than previous approaches including ELMo. Until today, various versions, with some small changes or adaptions have been introduced, but all of these models still using the transformer model at the core.</p>
<p><img decoding="async" loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*MtFDXtosIeq4tuBdEG9pQA.jpeg" alt="image" class="img_ev3q"></p>
<p><em>Fig.25 - Two steps of using a BERT Transformer</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7-practical-example">7. Practical Example<a href="#7-practical-example" class="hash-link" aria-label="Direct link to 7. Practical Example" title="Direct link to 7. Practical Example">​</a></h2>
<p>Check out the following Github repository, this blog can also be found there as a HTML file, with additional practial python examples.</p>
<p><a href="https://github.com/robinvetsch/Transformer_SQUAD" target="_blank" rel="noopener noreferrer">https://github.com/robinvetsch/Transformer_SQUAD</a>.</p>
<p><a href="https://medium.com/@RobinVetsch/nlp-from-word-embedding-to-transformers-76ae124e6281" target="_blank" rel="noopener noreferrer">NLP - From Word Embedding to Transformers | by Robin | Medium</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="others">Others<a href="#others" class="hash-link" aria-label="Direct link to Others" title="Direct link to Others">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mamba">Mamba<a href="#mamba" class="hash-link" aria-label="Direct link to Mamba" title="Direct link to Mamba">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="shortcomings-of-transformers">Shortcomings of Transformers<a href="#shortcomings-of-transformers" class="hash-link" aria-label="Direct link to Shortcomings of Transformers" title="Direct link to Shortcomings of Transformers">​</a></h4>
<ul>
<li>Transformer models are widely used for language modeling tasks.</li>
<li>They suffer from a quadratic attention mechanism, which makes them slow for long sequences.</li>
<li><strong>The slowness of Transformers for long sequences limits their usefulness.</strong></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-mamba-is-solving-these-problems">How Mamba is solving these problems<a href="#how-mamba-is-solving-these-problems" class="hash-link" aria-label="Direct link to How Mamba is solving these problems" title="Direct link to How Mamba is solving these problems">​</a></h4>
<ul>
<li>Mamba is a new model that uses an SSM architecture for linear-time scaling.</li>
<li>It is based on S4, a state space model, which can be discretized for use with text data.</li>
<li>Mamba can be used in two modes: RNN mode and CNN mode. RNN mode is better for inference while CNN mode is better for training.</li>
<li>Mamba introduces selectivity to the model by making the model parameters dependent on the input.</li>
</ul>
<p>Mamba is a linear-time language model that outperforms Transformers on various tasks. It achieves this speedup by using a combination of techniques including an SSM architecture and recurrent updates. Overall, the post explains how Mamba solves mostly long sequence limitations especially Transformer being computationally expensive to understand and model long sequences of user interactions.</p>
<p><a href="https://jackcook.com/2024/02/23/mamba.html" target="_blank" rel="noopener noreferrer">Mamba: The Easy Way</a></p>
<p><a href="https://thegradient.pub/mamba-explained/" target="_blank" rel="noopener noreferrer">Mamba Explained</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="links">Links<a href="#links" class="hash-link" aria-label="Direct link to Links" title="Direct link to Links">​</a></h2>
<ul>
<li><a href="https://mlops.substack.com/p/state-space-sequence-models-over" target="_blank" rel="noopener noreferrer">State Space Sequence Models over Transformers?</a></li>
<li><a href="https://huggingface.co/learn/nlp-course/chapter1/1" target="_blank" rel="noopener noreferrer">Introduction - Hugging Face NLP Course</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/deepaksood619/deepaksood619.github.io/tree/master/docs/ai/nlp/word-embedding-to-transformers.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-10-18T19:05:58.000Z" itemprop="dateModified">Oct 18, 2024</time></b> by <b>Deepak Sood</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai/nlp/nltk"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">NLTK</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai/numpy/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Numpy</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-introduction" class="table-of-contents__link toc-highlight">1. Introduction</a></li><li><a href="#2-nltk---natural-language-toolkit" class="table-of-contents__link toc-highlight">2. NLTK - natural language toolkit</a><ul><li><a href="#21-tokenizing" class="table-of-contents__link toc-highlight">2.1 Tokenizing</a></li><li><a href="#22-stemming" class="table-of-contents__link toc-highlight">2.2 Stemming</a></li><li><a href="#23-lemmatizing" class="table-of-contents__link toc-highlight">2.3 Lemmatizing</a></li></ul></li><li><a href="#3-word-embeddings" class="table-of-contents__link toc-highlight">3. Word embeddings</a><ul><li><a href="#31-overview-of-word-embeddings" class="table-of-contents__link toc-highlight">3.1 Overview of word embeddings</a></li><li><a href="#32-one-hot-encoding" class="table-of-contents__link toc-highlight">3.2 One-hot encoding</a></li><li><a href="#33-embedding-layer" class="table-of-contents__link toc-highlight">3.3 Embedding layer</a></li><li><a href="#332-keras-embedding-layer" class="table-of-contents__link toc-highlight">3.3.2 Keras embedding layer</a></li><li><a href="#34-word2vec" class="table-of-contents__link toc-highlight">3.4 Word2Vec</a></li><li><a href="#35-glove---global-vectors-for-word-representation" class="table-of-contents__link toc-highlight">3.5 GloVe - Global vectors for word representation</a></li><li><a href="#36-fasttext-enriching-word-vectors-with-subword-information" class="table-of-contents__link toc-highlight">3.6 FastText (enriching word vectors with subword information)</a></li><li><a href="#37-elmo" class="table-of-contents__link toc-highlight">3.7 ELMo</a></li></ul></li><li><a href="#4-sequence-to-sequence-models" class="table-of-contents__link toc-highlight">4. Sequence to sequence Models</a><ul><li><a href="#41-encoder---decoder-principle" class="table-of-contents__link toc-highlight">4.1 Encoder - Decoder principle</a></li></ul></li><li><a href="#5-attention-mechanism" class="table-of-contents__link toc-highlight">5. Attention mechanism</a><ul><li><a href="#51-mathematical-background-of-attention-mechanism" class="table-of-contents__link toc-highlight">5.1 Mathematical background of attention mechanism</a></li><li><a href="#52-self-attention-mechanism" class="table-of-contents__link toc-highlight">5.2 Self-attention mechanism</a></li><li><a href="#53-multi-head-attention-mechanism" class="table-of-contents__link toc-highlight">5.3 Multi-head attention mechanism</a></li></ul></li><li><a href="#6-transformers" class="table-of-contents__link toc-highlight">6. Transformers</a><ul><li><a href="#61-transformer-architecture" class="table-of-contents__link toc-highlight">6.1 Transformer architecture</a></li><li><a href="#62-gpt-language-model" class="table-of-contents__link toc-highlight">6.2 GPT language model</a></li><li><a href="#63-bert-bidirectional-encoder-representations-from-transformers" class="table-of-contents__link toc-highlight">6.3 BERT (Bidirectional encoder representations from transformers)</a></li></ul></li><li><a href="#7-practical-example" class="table-of-contents__link toc-highlight">7. Practical Example</a></li><li><a href="#others" class="table-of-contents__link toc-highlight">Others</a><ul><li><a href="#mamba" class="table-of-contents__link toc-highlight">Mamba</a></li></ul></li><li><a href="#links" class="table-of-contents__link toc-highlight">Links</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Deep Notes, Built with ❤️</div></div></div></footer></div>
</body>
</html>